#!/usr/bin/env python3
"""
MCAP åˆ° LeRobot v2.1 æ ‡å‡†æ ¼å¼è½¬æ¢å™¨

ğŸ“¹ æ”¯æŒ360på’Œ720påˆ†è¾¨ç‡ - å¯é€šè¿‡ --resolution å‚æ•°é€‰æ‹©ï¼ˆé»˜è®¤720p: 1280x720ï¼‰
åœ¨æµå¼å†™å…¥è§†é¢‘æ—¶å®æ—¶ç¼©æ”¾åˆ°ç›®æ ‡åˆ†è¾¨ç‡ï¼Œå‡å°‘å­˜å‚¨ç©ºé—´å’Œè®­ç»ƒåŠ è½½æ—¶é—´

åŠŸèƒ½ç‰¹æ€§:
1. 30Hz åŸºå‡†æ—¶é—´æˆ³åŒæ­¥ï¼ˆåŸºäºROS header.stampï¼‰
2. Linux æœåŠ¡å™¨ä¼˜åŒ– - é’ˆå¯¹å¤šæ ¸CPUå’Œé«˜æ•ˆå†…å­˜ç®¡ç†
3. å†…å­˜ä¼˜åŒ–å¤„ç†ï¼Œæ”¯æŒå¤§å‹MCAPæ–‡ä»¶
4. åˆ†æ‰¹å¤„ç†æ•°æ®ï¼Œé¿å…å†…å­˜æº¢å‡º
5. åŸºäº mcap_topic.xlsx å…·èº«åŒè‡‚topic é…ç½®
6. LeRobot v2.1 æ ‡å‡†æ ¼å¼è¾“å‡ºï¼ˆMP4è§†é¢‘ + åˆ†å±‚ç›®å½•ï¼‰
7. æ•°æ®è´¨é‡è¯„ä¼°æŠ¥å‘Š
8. ä¸»è‡‚ã€ä»è‡‚å’Œå¤¹çˆªæ›²çº¿å›¾ç»˜åˆ¶
9. è§†é¢‘å®æ—¶ç¼©æ”¾åˆ°ç›®æ ‡åˆ†è¾¨ç‡ï¼ˆæ”¯æŒ360p/720pï¼Œé€šè¿‡--resolutionå‚æ•°é€‰æ‹©ï¼‰

LeRobot v2.1 æ ‡å‡†ç‰¹æ€§:
- MP4 è§†é¢‘æ ¼å¼ï¼ˆæ›¿ä»£JPGå›¾åƒï¼‰
- åˆ†å±‚ç›®å½•ç»“æ„ï¼ˆmeta/data/videosï¼‰
- æ¯é›†ä¸€ä¸ªParquetæ–‡ä»¶ï¼ˆepisode_XXXXXX.parquetï¼‰
- æ¯é›†æ¯ç›¸æœºä¸€ä¸ªMP4æ–‡ä»¶
- å®Œæ•´çš„å…ƒæ•°æ®ï¼ˆinfo.json, episodes.jsonl, tasks.jsonl, episodes_stats.jsonlï¼‰

ä½¿ç”¨æ–¹æ³•:
python mcap_to_lerobot.py \
  --input /mnt/nas/synnas/docker2/å¤–éƒ¨æ•°æ®/å¤–æ¥ç¿å°”æ›¼2000æ¡/GroceryStore_Restrocking_Fallen \
  --output /home/kemove/mcap_to_lerobot/test1 \
  --ffmpeg-threads 32 \
  --ffmpeg-cpu-used 8 \
  --resolution 720p \
  --no-plot
"""

import sys
import json
import argparse
import numpy as np
import pandas as pd
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import cv2
from datetime import datetime
from collections import defaultdict
from dataclasses import dataclass, asdict
import matplotlib.pyplot as plt
from matplotlib.gridspec import GridSpec
import gc
import os
import shutil
import subprocess
import multiprocessing as mp
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import psutil
import time
from tqdm import tqdm

# MCAP ç›¸å…³å¯¼å…¥
try:
    from mcap.reader import make_reader
    from mcap_ros2.decoder import DecoderFactory as mcap_ros2_decoder
except ImportError:
    print("è¯·å®‰è£… mcap ç›¸å…³ä¾èµ–:")
    print("pip install mcap mcap-ros2-support")
    sys.exit(1)

# Linux æœåŠ¡å™¨ä¼˜åŒ–é…ç½®
def optimize_for_linux_server():
    """é’ˆå¯¹ Linux æœåŠ¡å™¨è¿›è¡Œä¼˜åŒ–é…ç½®"""
    # è®¾ç½® NumPy ä½¿ç”¨ä¼˜åŒ–çš„ BLASï¼ˆæ ¹æ®CPUæ ¸å¿ƒæ•°åŠ¨æ€è°ƒæ•´ï¼‰
    cpu_count = psutil.cpu_count()
    # å¯¹äºå¤šæ ¸ç³»ç»Ÿï¼Œå¯ä»¥é€‚å½“å¢åŠ çº¿ç¨‹æ•°ä»¥æé«˜æ€§èƒ½
    # ä½†ä¸ºäº†é¿å…è¿‡åº¦ç«äº‰ï¼Œé™åˆ¶ä¸ºCPUæ ¸å¿ƒæ•°çš„ä¸€åŠ
    optimal_threads = max(1, cpu_count // 2)
    
    os.environ['OPENBLAS_NUM_THREADS'] = str(optimal_threads)
    os.environ['MKL_NUM_THREADS'] = str(optimal_threads)
    os.environ['NUMEXPR_NUM_THREADS'] = str(optimal_threads)
    os.environ['OMP_NUM_THREADS'] = str(optimal_threads)
    
    # è®¾ç½® pandas ä½¿ç”¨æ›´é«˜æ•ˆçš„å†…å­˜æ¨¡å¼
    pd.set_option('mode.chained_assignment', None)
    pd.set_option('compute.use_bottleneck', True)
    pd.set_option('compute.use_numexpr', True)
    
    # è®¾ç½® matplotlib åç«¯ï¼ˆLinux æœåŠ¡å™¨é€šå¸¸æ²¡æœ‰æ˜¾ç¤ºï¼‰
    plt.switch_backend('Agg')  # ä½¿ç”¨éäº¤äº’å¼åç«¯
    
    # è®¾ç½® OpenCV ä¼˜åŒ–ï¼ˆæ ¹æ®CPUæ ¸å¿ƒæ•°è°ƒæ•´ï¼‰
    cv2.setNumThreads(optimal_threads)
    
    # è®¾ç½®åƒåœ¾å›æ”¶ä¼˜åŒ–
    gc.set_threshold(700, 10, 10)  # æ›´ç§¯æçš„åƒåœ¾å›æ”¶
    
    print("âœ… Linux æœåŠ¡å™¨ä¼˜åŒ–é…ç½®å·²å¯ç”¨")
    print(f"   ç³»ç»Ÿå†…å­˜: {psutil.virtual_memory().total / (1024**3):.1f} GB")
    print(f"   CPU æ ¸å¿ƒæ•°: {cpu_count}")
    print(f"   ä½¿ç”¨çº¿ç¨‹æ•°: {optimal_threads}")
    print(f"   å¯ç”¨å†…å­˜: {psutil.virtual_memory().available / (1024**3):.1f} GB")

# åˆå§‹åŒ– Linux æœåŠ¡å™¨ä¼˜åŒ–
optimize_for_linux_server()

# è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# ä¸»è¦é…ç½®å¸¸é‡
TARGET_FREQUENCY = 30  # Hz - ç»Ÿä¸€ç›®æ ‡é‡‡æ ·é¢‘ç‡
MAX_PROCESSING_DURATION_SECONDS = 300  # æœ€å¤§å¤„ç†æ—¶é•¿ï¼ˆ5åˆ†é’Ÿï¼‰
BATCH_SIZE = 500  # æ‰¹å¤„ç†å¤§å°ï¼ˆ100ç§’æ•°æ®ä¼˜åŒ–ï¼šå‡å°batch sizeï¼‰
CHUNK_SIZE = 500  # åˆ†å—å¤§å°ï¼ˆ100ç§’æ•°æ®ä¼˜åŒ–ï¼šå‡å°chunk sizeï¼‰

# é¢„å¤„ç†é™é‡‡æ ·é…ç½®ï¼ˆå‡å°‘æ’å€¼è¯¯å·®ï¼‰
USE_PREPROCESSING_DOWNSAMPLE = True  # å¯ç”¨é¢„å¤„ç†é™é‡‡æ ·
PREPROCESSING_CONFIG = {
    'leader_target_freq': 60,    # ä¸»è‡‚é¢„å¤„ç†ç›®æ ‡é¢‘ç‡ (62Hz â†’ 60Hz)
    'follower_target_freq': 180,  # ä»è‡‚é¢„å¤„ç†ç›®æ ‡é¢‘ç‡ (200Hz â†’ 180Hz)
    'use_antialiasing': True      # ä½¿ç”¨æŠ—æ··å æ»¤æ³¢å™¨
}

# åŒæ­¥å®¹å·®ï¼ˆç§’ï¼‰- åŸºäºå„æ¨¡æ€é¢‘ç‡ï¼Œæ§åˆ¶æ—¶é—´å¯¹é½ä¸¥æ ¼åº¦ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
# ç›¸æœº30Hzï¼šå¸§é—´éš”33.3msï¼Œé‡‡ç”¨åŠå¸§å®¹å·®ç¡®ä¿åŒæ­¥è´¨é‡
CAMERA_MAX_DRIFT_S = 1.0 / (2 * 30.0)  # ~16.7ms (åŠå¸§å®¹å·®)
# ä¸»è‡‚å…³èŠ‚62Hzï¼šå‘¨æœŸ16.13msï¼Œé‡‡ç”¨æ›´ä¸¥æ ¼é˜ˆå€¼
LEADER_JOINT_MAX_DRIFT_S = 0.005  # ~5ms (æ›´ä¸¥æ ¼)
# ä»è‡‚å…³èŠ‚200Hzï¼šå‘¨æœŸ5msï¼Œé‡‡ç”¨æ›´ä¸¥æ ¼é˜ˆå€¼
FOLLOWER_JOINT_MAX_DRIFT_S = 0.002  # ~2ms (æ›´ä¸¥æ ¼)
# é€šç”¨å®¹å·®ï¼ˆæ›´ä¸¥æ ¼ï¼‰
GENERIC_MAX_DRIFT_S = 0.015  # 15ms (æ›´ä¸¥æ ¼)

@dataclass
class TopicConfig:
    """Topicé…ç½®ä¿¡æ¯"""
    name: str
    datatype: str
    frequency: float
    description: str
    detection_dimension: str
    message_count: int = 0

@dataclass
class DataQualityMetrics:
    """æ•°æ®è´¨é‡æŒ‡æ ‡"""
    topic_name: str
    total_messages: int
    valid_messages: int
    missing_rate: float
    frequency_deviation: float
    data_continuity_score: float
    field_completeness: Dict[str, float]
    timestamp_gaps: List[float]
    quality_score: float

@dataclass
class ConversionReport:
    """è½¬æ¢æŠ¥å‘Š"""
    total_topics: int
    converted_frames: int
    processing_time: float
    quality_metrics: List[DataQualityMetrics]
    overall_quality_score: float
    conversion_issues: List[str]
    timestamp: str

class MCAPToLeRobotV21StandardConverter:
    """MCAP åˆ° LeRobot v2.1 æ ‡å‡†æ ¼å¼è½¬æ¢å™¨ - Linux æœåŠ¡å™¨ä¼˜åŒ–ç‰ˆæœ¬"""

    def __init__(self, mcap_file_path: str, output_dir: str, 
                 max_duration: int = 0, resolution: str = '720p'):
        """åˆå§‹åŒ–è½¬æ¢å™¨
        
        Args:
            resolution: ç›®æ ‡åˆ†è¾¨ç‡ï¼Œæ”¯æŒ '360p' (640x360) æˆ– '720p' (1280x720)
        """
        # å½’ä¸€åŒ–è·¯å¾„ï¼šå…¼å®¹ Windows é£æ ¼åæ–œæ ï¼Œç»Ÿä¸€ä¸º POSIX
        self.mcap_file_path = Path(str(mcap_file_path).replace('\\', '/')).resolve()
        # åŒæ—¶æä¾› self.input_path åˆ«åï¼Œä¾¿äºå¤–éƒ¨è°ƒç”¨ä¿æŒä¸€è‡´
        self.input_path = self.mcap_file_path
        self.output_dir = Path(str(output_dir).replace('\\', '/')).resolve()
        self.max_duration = max_duration
        # åˆå§‹åŒ–æ—¥å¿—è¾“å‡º
        # print(f"[Init] Input: {self.input_path.as_posix()}")
        # print(f"[Init] Output: {self.output_dir.as_posix()}")
        
        
        # è®¾ç½®ç›®æ ‡åˆ†è¾¨ç‡
        if resolution == '720p':
            self.target_width = 1280
            self.target_height = 720
            self.resolution_name = '720p'
        else:  # é»˜è®¤720pï¼Œä½†ä¿ç•™360pæ”¯æŒ
            self.target_width = 640
            self.target_height = 360
            self.resolution_name = '360p'
        
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # åˆ›å»º v2.1 æ ‡å‡†ç›®å½•ç»“æ„
        self.meta_dir = self.output_dir / "meta"
        self.data_dir = self.output_dir / "data"
        self.videos_dir = self.output_dir / "videos"
        
        # V2.1 ä¸éœ€è¦ episodes å­ç›®å½•ï¼Œç›´æ¥åœ¨ meta ä¸‹
        for dir_path in [self.meta_dir, self.data_dir, self.videos_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)

        # æ•°æ®å­˜å‚¨ - ä½¿ç”¨æ›´èŠ‚çœå†…å­˜çš„æ–¹å¼
        self.raw_data = defaultdict(list)
        self.timestamps = defaultdict(list)
        self.synchronized_data = {}
        self.target_timestamps = []
        self.topic_configs = {}
        self.data_chunks = []  # åˆ†æ‰¹å­˜å‚¨æ•°æ®

        # è´¨é‡è¯„ä¼°
        self.quality_metrics = []
        self.conversion_issues = []

        # M èŠ¯ç‰‡æ€§èƒ½ç›‘æ§
        self.performance_stats = {
            'memory_usage': [],
            'processing_times': [],
            'gc_counts': []
        }

        print(f"å¼€å§‹è½¬æ¢ [{self.resolution_name},{TARGET_FREQUENCY}Hz]: {self.mcap_file_path.name}")
        
        # åŠ è½½ Topic é…ç½®ï¼ˆä¸å†æ”¯æŒExcelï¼Œç›´æ¥ä»MCAPè‡ªåŠ¨æ¢æµ‹ï¼‰
        self.discover_topic_configs_from_mcap()

    # Excel åŠ è½½åŠŸèƒ½å·²ç§»é™¤

    def discover_topic_configs_from_mcap(self):
        """ä»MCAPè‡ªåŠ¨æ¢æµ‹Topicé…ç½®ï¼ˆä¸è§£ç æ¶ˆæ¯ï¼Œæ˜¾è‘—åŠ é€Ÿï¼Œå¹¶æä¾›è¿›åº¦æ¡ï¼‰"""
        topic_stats = {}

        try:
            file_size = self.mcap_file_path.stat().st_size
            with self.mcap_file_path.open("rb") as f:
                reader = make_reader(f)
                pbar = tqdm(total=file_size, unit="B", unit_scale=True, desc="æ‰«æMCAP", mininterval=0.5, leave=False)
                last_pos = f.tell()

                for schema, channel, message in reader.iter_messages():
                    topic = channel.topic
                    t_ns = message.log_time  # çº³ç§’æ—¶é—´æˆ³
                    dtype = schema.name if schema and hasattr(schema, "name") else "unknown"

                    s = topic_stats.get(topic)
                    if s is None:
                        topic_stats[topic] = {
                            "count": 1,
                            "first_ns": t_ns,
                            "last_ns": t_ns,
                            "datatype": dtype,
                        }
                    else:
                        s["count"] += 1
                        if t_ns < s["first_ns"]:
                            s["first_ns"] = t_ns
                        if t_ns > s["last_ns"]:
                            s["last_ns"] = t_ns

                    # ç”¨å·²è¯»å–çš„å­—èŠ‚ä½ç½®æ›´æ–°è¿›åº¦æ¡
                    pos = f.tell()
                    if pos > last_pos:
                        pbar.update(pos - last_pos)
                        last_pos = pos

                pbar.close()
        except KeyboardInterrupt:
            print("\nâš ï¸ æ‰«æè¢«ä¸­æ–­ï¼Œä½¿ç”¨å½“å‰ç»Ÿè®¡ç»“æœç»§ç»­ã€‚")
        except Exception as e:
            raise RuntimeError(f"ä»MCAPæ¢æµ‹Topicå¤±è´¥: {e}")

        # ç»„è£… TopicConfig å­—å…¸ï¼ˆä¸å…¨å±€ä½¿ç”¨ä¿æŒä¸€è‡´ï¼‰
        self.topic_configs = {}
        for name, s in topic_stats.items():
            duration_s = max((s["last_ns"] - s["first_ns"]) / 1e9, 1e-6)
            freq = (s["count"] / duration_s) if duration_s > 0 else 0.0
            self.topic_configs[name] = TopicConfig(
                name=name,
                datatype=s["datatype"],
                frequency=freq,
                description="auto from mcap",
                detection_dimension="auto",
                message_count=s["count"],
            )

        # print(f"æ£€æµ‹åˆ° {len(self.topic_configs)} ä¸ªtopic")
        # æ‰“å°éƒ¨åˆ†ç¤ºä¾‹
        for t in list(self.topic_configs.keys())[:10]:
            tc = self.topic_configs[t]
            # print(f"  - {tc.name}: {tc.frequency:.2f} Hz, count={tc.message_count}, type={tc.datatype}")

    def load_topic_configs_from_mcap(self):
        """ä» MCAP è‡ªåŠ¨æ¢æµ‹ Topicï¼Œä¼°ç®—é¢‘ç‡ä¸æ¶ˆæ¯ç»Ÿè®¡ï¼ˆå¿«é€Ÿæ‰«æï¼Œä¸è§£ç ï¼‰"""
        print("\næ‰«æMCAP(ç®€åŒ–)...")
        topic_stats = {}
        try:
            total_size = self.mcap_file_path.stat().st_size
            with self.mcap_file_path.open("rb") as f:
                reader = make_reader(f)
                pbar = tqdm(total=total_size, unit="B", unit_scale=True, desc="æ‰«æMCAP(ç®€åŒ–)", leave=False)
                last_pos = f.tell()
                for schema, channel, message in reader.iter_messages():
                    topic = channel.topic
                    t_ns = message.log_time  # çº³ç§’æ—¶é—´æˆ³
                    s = topic_stats.get(topic)
                    if s is None:
                        topic_stats[topic] = {
                            "count": 1,
                            "first_ns": t_ns,
                            "last_ns": t_ns,
                            "datatype": (schema.name if schema and hasattr(schema, "name") else "unknown"),
                        }
                    else:
                        s["count"] += 1
                        if t_ns < s["first_ns"]:
                            s["first_ns"] = t_ns
                        if t_ns > s["last_ns"]:
                            s["last_ns"] = t_ns
                    # æ›´æ–°è¿›åº¦æ¡
                    pos = f.tell()
                    if pos > last_pos:
                        pbar.update(pos - last_pos)
                        last_pos = pos
                pbar.close()
        except Exception as e:
            raise RuntimeError(f"ä»MCAPæ¢æµ‹Topicå¤±è´¥: {e}")

        # ç»„è£… TopicConfig å­—å…¸ï¼ˆä¸å…¨å±€ä½¿ç”¨ä¿æŒä¸€è‡´ï¼‰
        self.topic_configs = {}
        for name, s in topic_stats.items():
            duration_s = max((s["last_ns"] - s["first_ns"]) / 1e9, 1e-6)
            freq = (s["count"] / duration_s) if duration_s > 0 else 0.0
            self.topic_configs[name] = TopicConfig(
                name=name,
                datatype=s["datatype"],
                frequency=freq,
                description="auto from mcap",
                detection_dimension="auto",
                message_count=s["count"],
            )

        print(f"æ£€æµ‹åˆ° {len(self.topic_configs)} ä¸ªTopic")
        for t in list(self.topic_configs.keys())[:10]:
            tc = self.topic_configs[t]
            print(f"  - {tc.name}: {tc.frequency:.2f} Hz, count={tc.message_count}, type={tc.datatype}")

    def extract_timestamp_from_header(self, msg):
        """ä»ROSæ¶ˆæ¯headerä¸­æå–æ—¶é—´æˆ³ - ä¼˜å…ˆä½¿ç”¨header.stamp"""
        try:
            # ä¼˜å…ˆä½¿ç”¨header.stampï¼ˆä¼ æ„Ÿå™¨é‡‡é›†æ—¶é—´ï¼‰
            if hasattr(msg, 'header') and hasattr(msg.header, 'stamp'):
                stamp = msg.header.stamp
                timestamp = float(stamp.sec) + float(stamp.nanosec) * 1e-9
                # ç¡®ä¿æ—¶é—´æˆ³æœ‰æ•ˆï¼ˆå¤§äº0ï¼‰
                if timestamp > 0:
                    return timestamp
            # å›é€€åˆ°ç›´æ¥stampå­—æ®µ
            elif hasattr(msg, 'stamp'):
                stamp = msg.stamp
                timestamp = float(stamp.sec) + float(stamp.nanosec) * 1e-9
                if timestamp > 0:
                    return timestamp
            return None
        except Exception:
            return None

    def analyze_mcap_file_for_duration(self):
        """æ™ºèƒ½åˆ†æMCAPæ–‡ä»¶ï¼Œç¡®å®šå¤„ç†æ—¶é•¿å’Œç›¸å…³ä¿¡æ¯"""
        print("\næ™ºèƒ½åˆ†æMCAPæ–‡ä»¶...")
        
        start_time = None
        end_time = None
        topic_counts = defaultdict(int)
        topic_timestamps = {}  # è®°å½•æ¯ä¸ªtopicçš„æ—¶é—´æˆ³èŒƒå›´
        
        try:
            with open(self.mcap_file_path, "rb") as f:
                reader = make_reader(f, decoder_factories=[mcap_ros2_decoder()])
                
                message_count = 0
                for schema, channel, message, ros_msg in reader.iter_decoded_messages():
                    topic_name = channel.topic
                    message_count += 1
                    
                    if topic_name not in self.topic_configs:
                        continue
                    
                    # ä¼˜å…ˆä½¿ç”¨header.stampï¼ˆä¼ æ„Ÿå™¨é‡‡é›†æ—¶é—´ï¼‰ï¼Œå›é€€åˆ°log_time
                    timestamp = self.extract_timestamp_from_header(ros_msg)
                    if timestamp is None:
                        timestamp = message.log_time * 1e-9
                    
                    # è®°å½•æ—¶é—´èŒƒå›´
                    if start_time is None:
                        start_time = timestamp
                        # è®¾ç½®å…¨å±€èµ·å§‹æ—¶é—´æˆ³ï¼Œç¡®ä¿åç»­å¤„ç†ä½¿ç”¨ç›¸åŒçš„åŸºå‡†
                        self.start_timestamp = timestamp
                    end_time = timestamp
                    
                    topic_counts[topic_name] += 1
                    
                    # è®°å½•æ¯ä¸ªtopicçš„æ—¶é—´æˆ³èŒƒå›´
                    if topic_name not in topic_timestamps:
                        topic_timestamps[topic_name] = {'min': timestamp, 'max': timestamp}
                    else:
                        topic_timestamps[topic_name]['min'] = min(topic_timestamps[topic_name]['min'], timestamp)
                        topic_timestamps[topic_name]['max'] = max(topic_timestamps[topic_name]['max'], timestamp)
                    
                    # æ¯å¤„ç†10000æ¡æ¶ˆæ¯æ˜¾ç¤ºè¿›åº¦
                    if message_count % 10000 == 0:
                        print(f"  å·²å¤„ç† {message_count} æ¡æ¶ˆæ¯...")
                    
                    # å¦‚æœè¶…è¿‡æœ€å¤§å¤„ç†æ—¶é•¿ï¼Œåœæ­¢åˆ†æ
                    if self.max_duration > 0 and timestamp - start_time > self.max_duration:
                        break
                        
        except Exception as e:
            raise RuntimeError(f"åˆ†æMCAPæ–‡ä»¶å¤±è´¥: {e}")
        
        if start_time is None or end_time is None:
            raise ValueError("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ—¶é—´æˆ³æ•°æ®")
        
        # ç¡®ä¿æ—¶é—´èŒƒå›´æ­£ç¡®
        if end_time < start_time:
            start_time, end_time = end_time, start_time
            
        duration = end_time - start_time
        
        # åˆ†ætopicæ•°æ®è¦†ç›–æƒ…å†µ
        active_topics = 0
        for topic_name, ts_range in topic_timestamps.items():
            topic_duration = ts_range['max'] - ts_range['min']
            coverage = topic_duration / duration if duration > 0 else 0
            if coverage > 0.8:  # è¦†ç›–80%ä»¥ä¸Šæ—¶é—´
                active_topics += 1
        
        print(f"  æ–‡ä»¶æ—¶é•¿: {duration:.2f} ç§’")
        print(f"  æ—¶é—´èŒƒå›´: {start_time:.3f}s - {end_time:.3f}s")
        print(f"  ç›¸å¯¹æ—¶é—´: 0.000s - {duration:.3f}s")
        print(f"  æ€»æ¶ˆæ¯æ•°: {message_count}")
        print(f"  ç›¸å…³topics: {len(topic_counts)}")
        print(f"  æ´»è·ƒtopics: {active_topics} (è¦†ç›–>80%æ—¶é—´)")
        
        # å¦‚æœè®¾ç½®äº†æœ€å¤§å¤„ç†æ—¶é•¿ï¼Œé™åˆ¶å®é™…å¤„ç†æ—¶é•¿
        if self.max_duration > 0 and duration > self.max_duration:
            duration = self.max_duration
            end_time = start_time + duration
            print(f"  é™åˆ¶å¤„ç†æ—¶é•¿: {duration:.2f} ç§’")
        
        return start_time, end_time, duration, topic_counts

    def _determine_batch_strategy(self, duration):
        """æ ¹æ®æ•°æ®æ—¶é•¿æ™ºèƒ½é€‰æ‹©æ‰¹æ¬¡å¤„ç†ç­–ç•¥"""
        import psutil
        
        # è·å–ç³»ç»Ÿå†…å­˜ä¿¡æ¯
        total_memory_gb = psutil.virtual_memory().total / (1024**3)
        available_memory_gb = psutil.virtual_memory().available / (1024**3)
        
        # æ ¹æ®æ—¶é•¿é€‰æ‹©ç­–ç•¥
        if duration <= 30:  # 30ç§’ä»¥å†…
            strategy = {
                'strategy_name': 'å•æ‰¹æ¬¡å¤„ç†',
                'num_batches': 1,
                'batch_duration': duration,
                'memory_optimized': False,
                'description': 'çŸ­æ•°æ®ï¼Œç›´æ¥å¤„ç†'
            }
        elif duration <= 60:  # 1åˆ†é’Ÿä»¥å†…
            strategy = {
                'strategy_name': 'å°æ‰¹æ¬¡å¤„ç†',
                'num_batches': 2,
                'batch_duration': duration / 2,
                'memory_optimized': True,
                'description': 'ä¸­ç­‰æ•°æ®ï¼Œ2æ‰¹æ¬¡å¤„ç†'
            }
        elif duration <= 120:  # 2åˆ†é’Ÿä»¥å†…
            # 100ç§’æ•°æ®å†…å­˜ä¼˜åŒ–ç­–ç•¥
            if available_memory_gb > 6:
                strategy = {
                    'strategy_name': 'å†…å­˜ä¼˜åŒ–å¤§æ‰¹æ¬¡å¤„ç†',
                    'num_batches': 5,
                    'batch_duration': duration / 5,
                    'memory_optimized': True,
                    'description': '100ç§’æ•°æ®ï¼Œ5æ‰¹æ¬¡å¤„ç†ï¼ˆæ¯æ‰¹20ç§’ï¼‰- å†…å­˜ä¼˜åŒ–'
                }
            elif available_memory_gb > 3:
                strategy = {
                    'strategy_name': 'å†…å­˜ä¼˜åŒ–ä¸­æ‰¹æ¬¡å¤„ç†',
                    'num_batches': 8,
                    'batch_duration': duration / 8,
                    'memory_optimized': True,
                    'description': '100ç§’æ•°æ®ï¼Œ8æ‰¹æ¬¡å¤„ç†ï¼ˆæ¯æ‰¹12.5ç§’ï¼‰- å†…å­˜ä¼˜åŒ–'
                }
            else:
                strategy = {
                    'strategy_name': 'è¶…å†…å­˜ä¼˜åŒ–æ‰¹æ¬¡å¤„ç†',
                    'num_batches': 12,
                    'batch_duration': duration / 12,
                    'memory_optimized': True,
                    'description': '100ç§’æ•°æ®ï¼Œ12æ‰¹æ¬¡å¤„ç†ï¼ˆæ¯æ‰¹8.3ç§’ï¼‰- è¶…å†…å­˜ä¼˜åŒ–'
                }
        elif duration <= 300:  # 5åˆ†é’Ÿä»¥å†…
            # æ ¹æ®å¯ç”¨å†…å­˜åŠ¨æ€è°ƒæ•´
            if available_memory_gb > 8:
                strategy = {
                    'strategy_name': 'å¤§å†…å­˜æ‰¹æ¬¡å¤„ç†',
                    'num_batches': 6,
                    'batch_duration': duration / 6,
                    'memory_optimized': True,
                    'description': 'é•¿æ•°æ®ï¼Œå¤§å†…å­˜ï¼Œ6æ‰¹æ¬¡å¤„ç†'
                }
            else:
                strategy = {
                    'strategy_name': 'å†…å­˜ä¼˜åŒ–æ‰¹æ¬¡å¤„ç†',
                    'num_batches': 10,
                    'batch_duration': duration / 10,
                    'memory_optimized': True,
                    'description': 'é•¿æ•°æ®ï¼Œå†…å­˜å—é™ï¼Œ10æ‰¹æ¬¡å¤„ç†'
                }
        else:  # è¶…è¿‡5åˆ†é’Ÿ
            strategy = {
                'strategy_name': 'è¶…é•¿æ•°æ®æ‰¹æ¬¡å¤„ç†',
                'num_batches': max(15, int(duration / 20)),  # æ¯æ‰¹æœ€å¤š20ç§’
                'batch_duration': min(20.0, duration / max(15, int(duration / 20))),
                'memory_optimized': True,
                'description': 'è¶…é•¿æ•°æ®ï¼Œé«˜é¢‘ç‡æ‰¹æ¬¡å¤„ç†'
            }
        
        # ç¡®ä¿æ‰¹æ¬¡æ—¶é•¿åˆç†ï¼ˆ5-60ç§’ä¹‹é—´ï¼‰
        if strategy['batch_duration'] < 5:
            strategy['batch_duration'] = 5
            strategy['num_batches'] = max(1, int(duration / 5))
        elif strategy['batch_duration'] > 60:
            strategy['batch_duration'] = 60
            strategy['num_batches'] = max(1, int(duration / 60))
        
        # æ·»åŠ å†…å­˜ä½¿ç”¨é¢„ä¼°å’Œè‡ªé€‚åº”è°ƒæ•´
        # 100ç§’æ•°æ®å†…å­˜ä¼˜åŒ–ï¼šä¿å®ˆçš„å†…å­˜é¢„ä¼°
        base_memory_per_10s = 0.03  # æ¯10ç§’çº¦0.03GBï¼ˆå†…å­˜ä¼˜åŒ–æ¨¡å¼ï¼‰
        estimated_memory_per_batch = min(1.5, duration / strategy['num_batches'] * base_memory_per_10s)
        
        # æ ¹æ®å¯ç”¨å†…å­˜è°ƒæ•´æ‰¹æ¬¡å¤§å° - 100ç§’æ•°æ®å†…å­˜ä¼˜åŒ–
        if available_memory_gb < 2:  # å†…å­˜ä¸¥é‡ä¸è¶³æ—¶
            # å†…å­˜ä¸¥é‡ä¸è¶³æ—¶ï¼Œå¤§å¹…å¢åŠ æ‰¹æ¬¡æ•°
            strategy['num_batches'] = min(strategy['num_batches'] * 3, 20)  # æœ€å¤š20æ‰¹æ¬¡
            strategy['batch_duration'] = duration / strategy['num_batches']
            strategy['memory_optimized'] = True
            strategy['strategy_name'] += " (å†…å­˜ä¸¥é‡ä¸è¶³)"
        elif available_memory_gb < 4:  # å†…å­˜ä¸è¶³æ—¶
            # å†…å­˜ä¸è¶³æ—¶ï¼Œå¢åŠ æ‰¹æ¬¡æ•°
            strategy['num_batches'] = min(strategy['num_batches'] * 2, 15)  # æœ€å¤š15æ‰¹æ¬¡
            strategy['batch_duration'] = duration / strategy['num_batches']
            strategy['memory_optimized'] = True
            strategy['strategy_name'] += " (å†…å­˜ä¸è¶³ä¼˜åŒ–)"
        elif available_memory_gb < 6 and duration > 80:  # ä¸­ç­‰å†…å­˜ï¼Œé•¿æ•°æ®
            # ä¸­ç­‰å†…å­˜æ—¶ï¼Œé€‚åº¦å¢åŠ æ‰¹æ¬¡æ•°
            strategy['num_batches'] = min(int(strategy['num_batches'] * 1.5), 10)
            strategy['batch_duration'] = duration / strategy['num_batches']
            strategy['memory_optimized'] = True
            strategy['strategy_name'] += " (ä¸­ç­‰å†…å­˜ä¼˜åŒ–)"
        
        strategy['estimated_memory_per_batch'] = f"{estimated_memory_per_batch:.1f}GB"
        strategy['total_estimated_memory'] = f"{estimated_memory_per_batch * strategy['num_batches']:.1f}GB"
        strategy['available_memory'] = f"{available_memory_gb:.1f}GB"
        
        return strategy

    def _read_mcap_data_once(self, start_time, end_time):
        """ä¸€æ¬¡æ€§è¯»å–MCAPæ–‡ä»¶ä¸­çš„æ‰€æœ‰æ•°æ®ï¼ˆä¼˜åŒ–ï¼šé¿å…é‡å¤è¯»å–ï¼‰"""
        print(f"\nä¸€æ¬¡æ€§è¯»å–MCAPæ–‡ä»¶æ•°æ®...")
        
        all_messages = defaultdict(list)
        all_timestamps = defaultdict(list)
        
        # ä½¿ç”¨setè¿›è¡ŒO(1)æŸ¥æ‰¾
        valid_topics = set(self.topic_configs.keys())
        
        message_count = 0
        filtered_count = 0
        
        try:
            with open(self.mcap_file_path, "rb") as f:
                reader = make_reader(f, decoder_factories=[mcap_ros2_decoder()])
                
                for schema, channel, message, ros_msg in reader.iter_decoded_messages():
                    message_count += 1
                    topic_name = channel.topic
                    
                    # å¿«é€Ÿè¿‡æ»¤ï¼šO(1)æŸ¥æ‰¾
                    if topic_name not in valid_topics:
                        continue
                    
                    # ä¼˜å…ˆä½¿ç”¨header.stampï¼ˆä¼ æ„Ÿå™¨é‡‡é›†æ—¶é—´ï¼‰ï¼Œå›é€€åˆ°log_time
                    timestamp = self.extract_timestamp_from_header(ros_msg)
                    if timestamp is None:
                        timestamp = message.log_time * 1e-9
                    
                    # åˆå§‹åŒ–èµ·å§‹æ—¶é—´æˆ³ï¼ˆç¬¬ä¸€ä¸ªæœ‰æ•ˆæ¶ˆæ¯çš„æ—¶é—´æˆ³ï¼‰
                    if not hasattr(self, 'start_timestamp'):
                        self.start_timestamp = timestamp
                    
                    # å°†ç»å¯¹æ—¶é—´æˆ³è½¬æ¢ä¸ºç›¸å¯¹æ—¶é—´æˆ³
                    relative_timestamp = timestamp - self.start_timestamp
                    
                    # æ£€æŸ¥æ˜¯å¦åœ¨æ—¶é—´èŒƒå›´å†…
                    # start_timeå’Œend_timeæ˜¯ç»å¯¹æ—¶é—´æˆ³ï¼Œéœ€è¦è½¬æ¢ä¸ºç›¸å¯¹æ—¶é—´æˆ³
                    relative_start_time = start_time - self.start_timestamp
                    relative_end_time = end_time - self.start_timestamp
                    
                    if relative_timestamp < relative_start_time:
                        continue
                    if relative_timestamp > relative_end_time:
                        # å¦‚æœè¶…è¿‡äº†ç»“æŸæ—¶é—´ï¼Œå¯ä»¥æå‰é€€å‡ºï¼ˆå¦‚æœæ•°æ®æ˜¯æŒ‰æ—¶é—´æ’åºçš„ï¼‰
                        # ä½†ä¸ºäº†å®‰å…¨ï¼Œæˆ‘ä»¬ç»§ç»­è¯»å–åˆ°æ–‡ä»¶æœ«å°¾
                        pass
                    
                    # å­˜å‚¨æ¶ˆæ¯å’Œæ—¶é—´æˆ³
                    all_messages[topic_name].append(ros_msg)
                    all_timestamps[topic_name].append(relative_timestamp)
                    filtered_count += 1
                    
                    # æ¯å¤„ç†100000æ¡æ¶ˆæ¯æ˜¾ç¤ºè¿›åº¦
                    if message_count % 100000 == 0:
                        print(f"  å·²å¤„ç† {message_count} æ¡æ¶ˆæ¯ï¼Œè¿‡æ»¤å {filtered_count} æ¡...")
        
        except Exception as e:
            raise RuntimeError(f"è¯»å–MCAPæ–‡ä»¶å¤±è´¥: {e}")
        
        print(f"  è¯»å–å®Œæˆ: å¤„ç†äº† {message_count} æ¡æ¶ˆæ¯ï¼Œè¿‡æ»¤å {filtered_count} æ¡")
        print(f"  æœ‰æ•ˆtopics: {len(all_messages)}")
        
        return all_messages, all_timestamps
    
    def process_mcap_data_in_batches(self, start_time, end_time):
        """æ™ºèƒ½åˆ†æ‰¹å¤„ç†MCAPæ•°æ® - ä¼˜åŒ–ç‰ˆæœ¬ï¼šæ ¹æ®æ–‡ä»¶å¤§å°é€‰æ‹©ç­–ç•¥"""
        print(f"\nå¼€å§‹æ™ºèƒ½åˆ†æ‰¹å¤„ç†MCAPæ•°æ®...")
        
        # è®¡ç®—æ‰¹å¤„ç†å‚æ•°
        total_duration = end_time - start_time
        
        # å¦‚æœè®¾ç½®äº†æœ€å¤§å¤„ç†æ—¶é•¿ï¼Œä½¿ç”¨å®ƒï¼›å¦åˆ™ä½¿ç”¨æ€»æ—¶é•¿
        if self.max_duration > 0:
            processing_duration = min(total_duration, self.max_duration)
        else:
            processing_duration = total_duration
        
        # æ£€æŸ¥æ–‡ä»¶å¤§å°å’Œå¯ç”¨å†…å­˜
        file_size_gb = self.mcap_file_path.stat().st_size / (1024**3)
        available_memory_gb = psutil.virtual_memory().available / (1024**3)
        
        print(f"  æ–‡ä»¶å¤§å°: {file_size_gb:.2f} GB")
        print(f"  å¯ç”¨å†…å­˜: {available_memory_gb:.2f} GB")
        
        # â­ æ™ºèƒ½ç­–ç•¥ï¼šå¦‚æœæ–‡ä»¶å¤ªå¤§ï¼Œä½¿ç”¨åˆ†æ‰¹è¯»å–è€Œä¸æ˜¯ä¸€æ¬¡æ€§åŠ è½½
        # é˜ˆå€¼ï¼šæ–‡ä»¶å¤§å°è¶…è¿‡å¯ç”¨å†…å­˜çš„50%æ—¶ï¼Œä½¿ç”¨åˆ†æ‰¹è¯»å–
        if file_size_gb > available_memory_gb * 0.5:
            print(f"  âš ï¸  æ–‡ä»¶è¾ƒå¤§ï¼Œä½¿ç”¨åˆ†æ‰¹è¯»å–ç­–ç•¥ï¼ˆé¿å…å†…å­˜æº¢å‡ºï¼‰")
            return self._process_in_batches_with_file_reading(start_time, end_time, processing_duration)
        else:
            print(f"  âœ… æ–‡ä»¶è¾ƒå°ï¼Œä½¿ç”¨ä¸€æ¬¡æ€§è¯»å–ç­–ç•¥ï¼ˆæ›´å¿«ï¼‰")
            return self._process_in_batches_with_memory(start_time, end_time, processing_duration)
    
    def _process_in_batches_with_memory(self, start_time, end_time, processing_duration):
        """ä½¿ç”¨å†…å­˜ç¼“å­˜çš„åˆ†æ‰¹å¤„ç†ï¼ˆé€‚ç”¨äºå°æ–‡ä»¶ï¼‰"""
        # æ™ºèƒ½æ‰¹æ¬¡ç­–ç•¥é€‰æ‹©
        batch_strategy = self._determine_batch_strategy(processing_duration)
        num_batches = batch_strategy['num_batches']
        actual_batch_duration = processing_duration / num_batches
        
        print(f"  æ€»æ—¶é•¿: {processing_duration:.2f} ç§’")
        print(f"  æ‰¹æ¬¡ç­–ç•¥: {batch_strategy['strategy_name']}")
        print(f"  æ‰¹æ¬¡æ•°: {num_batches}")
        print(f"  æ¯æ‰¹æ—¶é•¿: {actual_batch_duration:.2f} ç§’")
        
        # â­ ä¼˜åŒ–ï¼šä¸€æ¬¡æ€§è¯»å–æ‰€æœ‰æ•°æ®ï¼ˆåªè¯»å–ä¸€æ¬¡æ–‡ä»¶ï¼‰
        all_messages, all_timestamps = self._read_mcap_data_once(start_time, end_time)
        
        all_synchronized_data = {}
        
        for batch_idx in range(num_batches):
            batch_start_time = start_time + batch_idx * actual_batch_duration
            batch_end_time = min(start_time + (batch_idx + 1) * actual_batch_duration, end_time)
            
            # è½¬æ¢ä¸ºç›¸å¯¹æ—¶é—´æˆ³ï¼ˆå› ä¸ºall_timestampsä¸­å­˜å‚¨çš„æ˜¯ç›¸å¯¹æ—¶é—´æˆ³ï¼‰
            relative_batch_start = batch_start_time - self.start_timestamp
            relative_batch_end = batch_end_time - self.start_timestamp
            
            print(f"\nå¤„ç†æ‰¹æ¬¡ {batch_idx + 1}/{num_batches} ({relative_batch_start:.2f}s - {relative_batch_end:.2f}s)")
            
            # â­ ä¼˜åŒ–ï¼šä»å†…å­˜ä¸­å¤„ç†æ‰¹æ¬¡æ•°æ®ï¼Œè€Œä¸æ˜¯é‡æ–°è¯»å–æ–‡ä»¶
            batch_data = self._process_single_batch_from_memory(
                all_messages, all_timestamps, relative_batch_start, relative_batch_end
            )
            
            # åˆå¹¶æ‰¹æ¬¡æ•°æ®
            for topic_name, data in batch_data.items():
                if topic_name not in all_synchronized_data:
                    all_synchronized_data[topic_name] = []
                all_synchronized_data[topic_name].extend(data)
            
            # ä¼˜åŒ–åƒåœ¾å›æ”¶ï¼šå‡å°‘é¢‘ç‡
            if batch_idx % 5 == 0:  # æ¯5ä¸ªæ‰¹æ¬¡æ‰å›æ”¶ä¸€æ¬¡
                gc.collect()
            
            # ç›‘æ§å†…å­˜ä½¿ç”¨
            memory_usage = psutil.Process().memory_info().rss / (1024**3)
            self.performance_stats['memory_usage'].append(memory_usage)
            
            
            # æ›´ç§¯æåœ°æ¸…ç†å†…å­˜
            if memory_usage > 2.0:  # è¶…è¿‡2GBå°±æ¸…ç†
                print(f"    å†…å­˜ä½¿ç”¨è¾ƒé«˜ï¼Œæ‰§è¡Œé¢å¤–æ¸…ç†...")
                gc.collect()
        
        self.synchronized_data = all_synchronized_data
        print(f"\næ‰€æœ‰æ‰¹æ¬¡å¤„ç†å®Œæˆï¼Œå…±å¤„ç† {len(all_synchronized_data)} ä¸ªtopics")
        
        # è®¾ç½®å®é™…å¤„ç†æ—¶é•¿ç”¨äºè´¨é‡è¯„ä¼°
        self.actual_processing_duration = end_time - start_time
    
    def _process_in_batches_with_file_reading(self, start_time, end_time, processing_duration):
        """æµå¼å¤„ç†ï¼šåªè¯»å–ä¸€æ¬¡æ–‡ä»¶ï¼ŒæŒ‰æ‰¹æ¬¡å¤„ç†å¹¶ç«‹å³é‡Šæ”¾å†…å­˜ï¼ˆé€‚ç”¨äºå¤§æ–‡ä»¶ï¼‰"""
        # æ™ºèƒ½æ‰¹æ¬¡ç­–ç•¥é€‰æ‹©
        batch_strategy = self._determine_batch_strategy(processing_duration)
        num_batches = batch_strategy['num_batches']
        actual_batch_duration = processing_duration / num_batches
        
        print(f"  æ€»æ—¶é•¿: {processing_duration:.2f} ç§’")
        print(f"  æ‰¹æ¬¡ç­–ç•¥: {batch_strategy['strategy_name']}")
        print(f"  æ‰¹æ¬¡æ•°: {num_batches}")
        print(f"  æ¯æ‰¹æ—¶é•¿: {actual_batch_duration:.2f} ç§’")
        print(f"  âœ… ä½¿ç”¨æµå¼å¤„ç†ç­–ç•¥ï¼ˆåªè¯»å–ä¸€æ¬¡æ–‡ä»¶ï¼ŒæŒ‰æ‰¹æ¬¡å¤„ç†ï¼‰")
        
        # è®¡ç®—å„æ‰¹æ¬¡çš„æ—¶é—´èŒƒå›´
        batch_ranges = []
        for batch_idx in range(num_batches):
            batch_start_time = start_time + batch_idx * actual_batch_duration
            batch_end_time = min(start_time + (batch_idx + 1) * actual_batch_duration, end_time)
            batch_ranges.append((batch_start_time, batch_end_time))
        
        all_synchronized_data = {}
        
        # â­ æµå¼å¤„ç†ï¼šåªè¯»å–ä¸€æ¬¡æ–‡ä»¶ï¼ŒæŒ‰æ‰¹æ¬¡æ”¶é›†æ•°æ®
        print(f"\nå¼€å§‹æµå¼è¯»å–æ–‡ä»¶ï¼ˆåªè¯»å–ä¸€æ¬¡ï¼‰...")
        
        # ä¸ºæ¯ä¸ªæ‰¹æ¬¡å‡†å¤‡æ•°æ®å®¹å™¨
        batch_messages = [defaultdict(list) for _ in range(num_batches)]
        batch_timestamps = [defaultdict(list) for _ in range(num_batches)]
        
        # ä½¿ç”¨setè¿›è¡ŒO(1)æŸ¥æ‰¾
        valid_topics = set(self.topic_configs.keys())
        
        message_count = 0
        filtered_count = 0
        
        try:
            with open(self.mcap_file_path, "rb") as f:
                reader = make_reader(f, decoder_factories=[mcap_ros2_decoder()])
                
                for schema, channel, message, ros_msg in reader.iter_decoded_messages():
                    message_count += 1
                    topic_name = channel.topic
                    
                    # å¿«é€Ÿè¿‡æ»¤ï¼šO(1)æŸ¥æ‰¾
                    if topic_name not in valid_topics:
                        continue
                    
                    # ä¼˜å…ˆä½¿ç”¨header.stampï¼ˆä¼ æ„Ÿå™¨é‡‡é›†æ—¶é—´ï¼‰ï¼Œå›é€€åˆ°log_time
                    timestamp = self.extract_timestamp_from_header(ros_msg)
                    if timestamp is None:
                        timestamp = message.log_time * 1e-9
                    
                    # åˆå§‹åŒ–èµ·å§‹æ—¶é—´æˆ³
                    if not hasattr(self, 'start_timestamp'):
                        self.start_timestamp = timestamp
                    
                    # è½¬æ¢ä¸ºç›¸å¯¹æ—¶é—´æˆ³
                    relative_timestamp = timestamp - self.start_timestamp
                    
                    # æ£€æŸ¥æ—¶é—´èŒƒå›´
                    relative_start_time = start_time - self.start_timestamp
                    relative_end_time = end_time - self.start_timestamp
                    
                    if relative_timestamp < relative_start_time:
                        continue
                    if relative_timestamp > relative_end_time:
                        break  # è¶…è¿‡èŒƒå›´ï¼Œå¯ä»¥æå‰é€€å‡º
                    
                    # â­ ä¼˜åŒ–ï¼šä½¿ç”¨äºŒåˆ†æŸ¥æ‰¾ç¡®å®šæ‰¹æ¬¡ï¼ˆO(log n)è€Œä¸æ˜¯O(n)ï¼‰
                    # è®¡ç®—ç›¸å¯¹æ—¶é—´æˆ³åœ¨å“ªä¸ªæ‰¹æ¬¡
                    relative_timestamp_scaled = relative_timestamp - relative_start_time
                    batch_idx = int(relative_timestamp_scaled / actual_batch_duration)
                    batch_idx = max(0, min(batch_idx, num_batches - 1))  # ç¡®ä¿åœ¨æœ‰æ•ˆèŒƒå›´å†…
                    
                    # éªŒè¯æ˜¯å¦åœ¨æ‰¹æ¬¡èŒƒå›´å†…
                    relative_batch_start = batch_ranges[batch_idx][0] - self.start_timestamp
                    relative_batch_end = batch_ranges[batch_idx][1] - self.start_timestamp
                    if relative_batch_start <= relative_timestamp <= relative_batch_end:
                        batch_messages[batch_idx][topic_name].append(ros_msg)
                        batch_timestamps[batch_idx][topic_name].append(relative_timestamp)
                        filtered_count += 1
                    
                    # æ¯å¤„ç†100000æ¡æ¶ˆæ¯æ˜¾ç¤ºè¿›åº¦
                    if message_count % 100000 == 0:
                        print(f"  å·²å¤„ç† {message_count} æ¡æ¶ˆæ¯ï¼Œè¿‡æ»¤å {filtered_count} æ¡...")
        
        except Exception as e:
            raise RuntimeError(f"æµå¼è¯»å–MCAPæ–‡ä»¶å¤±è´¥: {e}")
        
        print(f"  æµå¼è¯»å–å®Œæˆ: å¤„ç†äº† {message_count} æ¡æ¶ˆæ¯ï¼Œè¿‡æ»¤å {filtered_count} æ¡")
        
        # ç°åœ¨æŒ‰æ‰¹æ¬¡å¤„ç†æ•°æ®ï¼ˆæ•°æ®å·²ç»åœ¨å†…å­˜ä¸­ï¼Œä½†åˆ†æ‰¹å­˜å‚¨ï¼‰
        for batch_idx in range(num_batches):
            batch_start_time, batch_end_time = batch_ranges[batch_idx]
            relative_batch_start = batch_start_time - self.start_timestamp
            relative_batch_end = batch_end_time - self.start_timestamp
            
            print(f"\nå¤„ç†æ‰¹æ¬¡ {batch_idx + 1}/{num_batches} ({relative_batch_start:.2f}s - {relative_batch_end:.2f}s)")
            
            # ä½¿ç”¨æ‰¹æ¬¡æ•°æ®ï¼ˆå·²ç»åœ¨å†…å­˜ä¸­ï¼‰
            batch_data = self._process_single_batch_from_memory(
                batch_messages[batch_idx], 
                batch_timestamps[batch_idx], 
                relative_batch_start, 
                relative_batch_end
            )
            
            # åˆå¹¶æ‰¹æ¬¡æ•°æ®
            for topic_name, data in batch_data.items():
                if topic_name not in all_synchronized_data:
                    all_synchronized_data[topic_name] = []
                all_synchronized_data[topic_name].extend(data)
            
            # ç«‹å³é‡Šæ”¾æ‰¹æ¬¡æ•°æ®çš„å†…å­˜
            del batch_messages[batch_idx]
            del batch_timestamps[batch_idx]
            del batch_data
            gc.collect()
            
            # ç›‘æ§å†…å­˜ä½¿ç”¨
            memory_usage = psutil.Process().memory_info().rss / (1024**3)
            self.performance_stats['memory_usage'].append(memory_usage)
            print(f"  æ‰¹æ¬¡ {batch_idx + 1} å®Œæˆï¼Œå†…å­˜ä½¿ç”¨: {memory_usage:.2f} GB")
        
        self.synchronized_data = all_synchronized_data
        print(f"\næ‰€æœ‰æ‰¹æ¬¡å¤„ç†å®Œæˆï¼Œå…±å¤„ç† {len(all_synchronized_data)} ä¸ªtopics")
        
        # è®¾ç½®å®é™…å¤„ç†æ—¶é•¿ç”¨äºè´¨é‡è¯„ä¼°
        self.actual_processing_duration = end_time - start_time

    def _process_single_batch_from_memory(self, all_messages, all_timestamps, start_time, end_time):
        """ä»å†…å­˜ä¸­å¤„ç†å•ä¸ªæ‰¹æ¬¡çš„æ•°æ®ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼šé¿å…é‡å¤è¯»å–æ–‡ä»¶ï¼‰"""
        batch_data = {}
        
        try:
            # ä»å†…å­˜ä¸­æå–æ‰¹æ¬¡æ—¶é—´èŒƒå›´å†…çš„æ•°æ®
            batch_messages = defaultdict(list)
            batch_timestamps = defaultdict(list)
            
            filtered_count = 0
            for topic_name in all_messages.keys():
                messages = all_messages[topic_name]
                timestamps = all_timestamps[topic_name]
                
                # æå–åœ¨æ‰¹æ¬¡æ—¶é—´èŒƒå›´å†…çš„æ•°æ®
                for msg, ts in zip(messages, timestamps):
                    if start_time <= ts <= end_time:
                        batch_messages[topic_name].append(msg)
                        batch_timestamps[topic_name].append(ts)
                        filtered_count += 1
            
            if not batch_messages:
                print(f"    è­¦å‘Š: æ‰¹æ¬¡ {start_time:.2f}s - {end_time:.2f}s ä¸­æ²¡æœ‰æ•°æ®")
                return batch_data
            
            print(f"    ä»å†…å­˜ä¸­æå–äº† {filtered_count} æ¡æ¶ˆæ¯")
            
            
            # æ™ºèƒ½å›¾åƒå¯¹é½ç­–ç•¥ - åªå¤„ç†RGBå›¾åƒï¼Œæ’é™¤æ·±åº¦å›¾åƒ
            image_topics = [topic for topic in batch_messages.keys() if 'image' in topic and 'depth' not in topic]
            
            # è°ƒè¯•ä¿¡æ¯ï¼ˆå·²ç§»é™¤è¯¦å°½æ‰“å°ï¼‰

            if image_topics:
                # æ£€æµ‹å›¾åƒé¢‘ç‡å¹¶é€‰æ‹©å¯¹é½ç­–ç•¥
                primary_image_topic = image_topics[0]
                image_timestamps = np.array(batch_timestamps[primary_image_topic])
                image_frequency = self._calculate_frequency(image_timestamps)
                
                print(f"  æ£€æµ‹åˆ°å›¾åƒé¢‘ç‡: {image_frequency:.1f}Hz")
                
                if abs(image_frequency - TARGET_FREQUENCY) < 1.0:  # æ¥è¿‘30Hz
                    # å›¾åƒæ¥è¿‘30Hzï¼Œç›´æ¥ä½¿ç”¨å›¾åƒæ—¶é—´æˆ³
                    target_timestamps = image_timestamps
                    print(f"  ä½¿ç”¨å›¾åƒæ—¶é—´æˆ³ä½œä¸ºå¯¹é½åŸºå‡† ({len(target_timestamps)} å¸§)")
                    use_image_timestamps = True
                else:
                    # å›¾åƒä¸æ˜¯30Hzï¼Œæ’å€¼åˆ°30Hz
                    target_timestamps = np.linspace(start_time, end_time, 
                                                  int((end_time - start_time) * TARGET_FREQUENCY))
                    print(f"  æ’å€¼å›¾åƒåˆ°30Hzï¼Œä½¿ç”¨30Hzç½‘æ ¼å¯¹é½ ({len(target_timestamps)} å¸§)")
                    use_image_timestamps = False
            else:
                # å¦‚æœæ²¡æœ‰å›¾åƒtopicï¼Œä½¿ç”¨30Hzç½‘æ ¼
                target_timestamps = np.linspace(start_time, end_time, 
                                              int((end_time - start_time) * TARGET_FREQUENCY))
                
                use_image_timestamps = False
            
            # æ’å€¼æ‰¹æ¬¡æ•°æ® - æ ¹æ®ç­–ç•¥å¤„ç†å›¾åƒå’Œå…¶ä»–æ•°æ®
            for topic_name, messages in batch_messages.items():
                if not messages:
                    continue
                
                timestamps = np.array(batch_timestamps[topic_name])
                
                if 'image' in topic_name:
                    if use_image_timestamps:
                        # å›¾åƒæ¥è¿‘30Hzï¼Œç›´æ¥ä½¿ç”¨å¹¶è§£ç 
                        decoded_images = []
                        for msg in messages:
                            decoded_img = self._decode_compressed_image(msg)
                            decoded_images.append(decoded_img)
                        batch_data[topic_name] = decoded_images
                    else:
                        # å›¾åƒä¸æ˜¯30Hzï¼Œæ’å€¼åˆ°30Hzå¹¶è§£ç 
                        interpolated_data = self._interpolate_batch_data(messages, timestamps, target_timestamps)
                        decoded_images = []
                        for msg in interpolated_data:
                            if msg is not None:
                                decoded_img = self._decode_compressed_image(msg)
                                decoded_images.append(decoded_img)
                            else:
                                decoded_images.append(None)
                        batch_data[topic_name] = decoded_images
                else:
                    # å…¶ä»–æ•°æ®å¯¹é½åˆ°ç›®æ ‡æ—¶é—´æˆ³
                    interpolated_data = self._interpolate_batch_data(messages, timestamps, target_timestamps)
                    batch_data[topic_name] = interpolated_data
                
        except Exception as e:
            print(f"å¤„ç†æ‰¹æ¬¡å¤±è´¥: {e}")
            raise
        
        return batch_data

    def _process_single_batch(self, start_time, end_time):
        """å¤„ç†å•ä¸ªæ‰¹æ¬¡çš„æ•°æ®"""
        batch_data = {}
        
        try:
            with open(self.mcap_file_path, "rb") as f:
                reader = make_reader(f, decoder_factories=[mcap_ros2_decoder()])
                
                # æ”¶é›†æ‰¹æ¬¡å†…çš„æ•°æ®
                batch_messages = defaultdict(list)
                batch_timestamps = defaultdict(list)
                
                message_count = 0
                filtered_count = 0
                for schema, channel, message, ros_msg in reader.iter_decoded_messages():
                    message_count += 1
                    topic_name = channel.topic
                    
                    if topic_name not in self.topic_configs:
                        continue
                    
                    # ä¼˜å…ˆä½¿ç”¨header.stampï¼ˆä¼ æ„Ÿå™¨é‡‡é›†æ—¶é—´ï¼‰ï¼Œå›é€€åˆ°log_time
                    timestamp = self.extract_timestamp_from_header(ros_msg)
                    if timestamp is None:
                        timestamp = message.log_time * 1e-9
                    
                    # å°†ç»å¯¹æ—¶é—´æˆ³è½¬æ¢ä¸ºç›¸å¯¹æ—¶é—´æˆ³
                    if not hasattr(self, 'start_timestamp'):
                        self.start_timestamp = timestamp
                    relative_timestamp = timestamp - self.start_timestamp
                    
                    # æ£€æŸ¥æ˜¯å¦åœ¨æ‰¹æ¬¡æ—¶é—´èŒƒå›´å†…ï¼ˆä½¿ç”¨ç›¸å¯¹æ—¶é—´æˆ³ï¼‰
                    if relative_timestamp < (start_time - self.start_timestamp):
                        continue
                    if relative_timestamp > (end_time - self.start_timestamp):
                        break
                    
                    batch_messages[topic_name].append(ros_msg)
                    batch_timestamps[topic_name].append(relative_timestamp)
                    filtered_count += 1
                
                
                print(f"    æ—¶é—´æˆ³èŒƒå›´: {start_time:.2f}s - {end_time:.2f}s")
                
                
                # æ™ºèƒ½å›¾åƒå¯¹é½ç­–ç•¥ - åªå¤„ç†RGBå›¾åƒï¼Œæ’é™¤æ·±åº¦å›¾åƒ
                image_topics = [topic for topic in batch_messages.keys() if 'image' in topic and 'depth' not in topic]
                
                # è°ƒè¯•ä¿¡æ¯
                print(f"    æ‰¹æ¬¡ä¸­çš„topicæ•°é‡: {len(batch_messages)}")
                print(f"    æ£€æµ‹åˆ°çš„å›¾åƒtopic: {image_topics}")
                if image_topics:
                    for topic in image_topics:
                        print(f"      {topic}: {len(batch_messages[topic])} ä¸ªæ¶ˆæ¯")
                
                if image_topics:
                    # æ£€æµ‹å›¾åƒé¢‘ç‡å¹¶é€‰æ‹©å¯¹é½ç­–ç•¥
                    primary_image_topic = image_topics[0]
                    image_timestamps = np.array(batch_timestamps[primary_image_topic])
                    image_frequency = self._calculate_frequency(image_timestamps)
                    
                    print(f"  æ£€æµ‹åˆ°å›¾åƒé¢‘ç‡: {image_frequency:.1f}Hz")
                    
                    if abs(image_frequency - TARGET_FREQUENCY) < 1.0:  # æ¥è¿‘30Hz
                        # å›¾åƒæ¥è¿‘30Hzï¼Œç›´æ¥ä½¿ç”¨å›¾åƒæ—¶é—´æˆ³
                        target_timestamps = image_timestamps
                        print(f"  ä½¿ç”¨å›¾åƒæ—¶é—´æˆ³ä½œä¸ºå¯¹é½åŸºå‡† ({len(target_timestamps)} å¸§)")
                        use_image_timestamps = True
                    else:
                        # å›¾åƒä¸æ˜¯30Hzï¼Œæ’å€¼åˆ°30Hz
                        target_timestamps = np.linspace(start_time, end_time, 
                                                      int((end_time - start_time) * TARGET_FREQUENCY))
                        print(f"  æ’å€¼å›¾åƒåˆ°30Hzï¼Œä½¿ç”¨30Hzç½‘æ ¼å¯¹é½ ({len(target_timestamps)} å¸§)")
                        use_image_timestamps = False
                else:
                    # å¦‚æœæ²¡æœ‰å›¾åƒtopicï¼Œä½¿ç”¨30Hzç½‘æ ¼
                    target_timestamps = np.linspace(start_time, end_time, 
                                                  int((end_time - start_time) * TARGET_FREQUENCY))
                    print(f"  æœªæ‰¾åˆ°å›¾åƒtopicï¼Œä½¿ç”¨30Hzç½‘æ ¼å¯¹é½ ({len(target_timestamps)} å¸§)")
                    use_image_timestamps = False
                
                # æ’å€¼æ‰¹æ¬¡æ•°æ® - æ ¹æ®ç­–ç•¥å¤„ç†å›¾åƒå’Œå…¶ä»–æ•°æ®
                for topic_name, messages in batch_messages.items():
                    if not messages:
                        continue
                    
                    timestamps = np.array(batch_timestamps[topic_name])
                    
                    if 'image' in topic_name:
                        if use_image_timestamps:
                            # å›¾åƒæ¥è¿‘30Hzï¼Œç›´æ¥ä½¿ç”¨å¹¶è§£ç 
                            decoded_images = []
                            for msg in messages:
                                decoded_img = self._decode_compressed_image(msg)
                                decoded_images.append(decoded_img)
                            batch_data[topic_name] = decoded_images
                        else:
                            # å›¾åƒä¸æ˜¯30Hzï¼Œæ’å€¼åˆ°30Hzå¹¶è§£ç 
                            interpolated_data = self._interpolate_batch_data(messages, timestamps, target_timestamps)
                            decoded_images = []
                            for msg in interpolated_data:
                                if msg is not None:
                                    decoded_img = self._decode_compressed_image(msg)
                                    decoded_images.append(decoded_img)
                                else:
                                    decoded_images.append(None)
                            batch_data[topic_name] = decoded_images
                    else:
                        # å…¶ä»–æ•°æ®å¯¹é½åˆ°ç›®æ ‡æ—¶é—´æˆ³
                        interpolated_data = self._interpolate_batch_data(messages, timestamps, target_timestamps)
                        batch_data[topic_name] = interpolated_data
                    
        except Exception as e:
            print(f"å¤„ç†æ‰¹æ¬¡å¤±è´¥: {e}")
            raise
        
        return batch_data

    def _calculate_frequency(self, timestamps):
        """è®¡ç®—æ—¶é—´æˆ³åºåˆ—çš„é¢‘ç‡"""
        if len(timestamps) < 2:
            return 0.0
        
        # è®¡ç®—æ—¶é—´é—´éš”
        time_diffs = np.diff(timestamps)
        
        # è¿‡æ»¤æ‰å¼‚å¸¸å€¼ï¼ˆè¶…è¿‡å¹³å‡å€¼çš„3å€ï¼‰
        mean_diff = np.mean(time_diffs)
        valid_diffs = time_diffs[time_diffs < mean_diff * 3]
        
        if len(valid_diffs) == 0:
            return 0.0
        
        # è®¡ç®—å¹³å‡é¢‘ç‡
        avg_interval = np.mean(valid_diffs)
        frequency = 1.0 / avg_interval if avg_interval > 0 else 0.0
        
        return frequency

    def _interpolate_batch_data(self, messages, timestamps, target_timestamps):
        """æ’å€¼æ‰¹æ¬¡æ•°æ® - M èŠ¯ç‰‡ä¼˜åŒ–ç‰ˆæœ¬ï¼Œç»Ÿä¸€30Hzï¼Œæ”¯æŒé¢„å¤„ç†é™é‡‡æ ·"""
        if not messages:
            return [None] * len(target_timestamps)
        
        topic_name = messages[0].__class__.__name__ if messages else "unknown"
        
        # é¢„å¤„ç†é™é‡‡æ ·ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if USE_PREPROCESSING_DOWNSAMPLE and 'JointState' in topic_name:
            # è®¡ç®—å®é™…é¢‘ç‡
            duration = (timestamps[-1] - timestamps[0]) / 1e9  # è½¬æ¢ä¸ºç§’
            actual_freq = len(messages) / duration if duration > 0 else 0
            
            # æ ¹æ®topicç±»å‹é€‰æ‹©é¢„å¤„ç†ç›®æ ‡é¢‘ç‡
            if 'leader' in str(messages[0]).lower() or '/leader' in str(type(messages[0])):
                # ä¸»è‡‚æ•°æ®ï¼š62Hz â†’ 60Hz
                preprocess_target = PREPROCESSING_CONFIG['leader_target_freq']
                if actual_freq > preprocess_target * 1.1:  # åªæœ‰å½“é¢‘ç‡æ˜æ˜¾é«˜äºç›®æ ‡æ—¶æ‰é™é‡‡æ ·
                    messages, timestamps = self._preprocess_downsample(
                        messages, timestamps, actual_freq, preprocess_target
                    )
            elif 'controller' in str(messages[0]).lower() or 'controller' in str(type(messages[0])):
                # ä»è‡‚æ•°æ®ï¼š200Hz â†’ 180Hz
                preprocess_target = PREPROCESSING_CONFIG['follower_target_freq']
                if actual_freq > preprocess_target * 1.1:
                    messages, timestamps = self._preprocess_downsample(
                        messages, timestamps, actual_freq, preprocess_target
                    )
        
        # æ ¹æ®æ¶ˆæ¯ç±»å‹é€‰æ‹©æ’å€¼æ–¹æ³•
        if 'Image' in topic_name or 'CompressedImage' in topic_name:
            return self._interpolate_images_batch(messages, timestamps, target_timestamps)
        elif 'JointState' in topic_name:
            return self._interpolate_joint_data_batch(messages, timestamps, target_timestamps)
        elif 'Float64' in topic_name or 'Float32' in topic_name:
            return self._interpolate_scalar_data_batch(messages, timestamps, target_timestamps)
        else:
            # é»˜è®¤ä½¿ç”¨æœ€è¿‘é‚»æ’å€¼
            return self._interpolate_nearest_neighbor_batch(messages, timestamps, target_timestamps)

    def _decode_compressed_image(self, msg):
        """è§£ç å‹ç¼©å›¾åƒæ¶ˆæ¯ä¸ºnumpyæ•°ç»„ï¼ˆç»Ÿä¸€è¿”å›BGRä¸‰é€šé“ï¼‰"""
        try:
            import numpy as np
            import cv2
            data = np.frombuffer(msg.data, dtype=np.uint8)
            img = cv2.imdecode(data, cv2.IMREAD_COLOR)  # BGRä¸‰é€šé“
            if img is None:
                return None
            # ç»Ÿä¸€ç¼©æ”¾åˆ°ç›®æ ‡åˆ†è¾¨ç‡
            target_width = self.target_width
            target_height = self.target_height
            if img.shape[1] != target_width or img.shape[0] != target_height:
                img = cv2.resize(img, (target_width, target_height), interpolation=cv2.INTER_LINEAR)
            return img
        except Exception:
            return None

    def _interpolate_images_batch(self, messages, timestamps, target_timestamps):
        """å›¾åƒæ•°æ®æ’å€¼ - ä¼˜åŒ–ç‰ˆæœ¬ï¼šä½¿ç”¨äºŒåˆ†æŸ¥æ‰¾ï¼ˆO(n log m)æ›¿ä»£O(nÃ—m)ï¼‰"""
        if len(messages) == len(target_timestamps):
            # é•¿åº¦ç›¸åŒï¼Œç›´æ¥è¿”å›ï¼ˆå›¾åƒå·²ç»æ˜¯30Hzï¼‰
            return messages
        
        # â­ ä¼˜åŒ–ï¼šä½¿ç”¨searchsortedè¿›è¡ŒäºŒåˆ†æŸ¥æ‰¾ï¼ˆO(n log m)ï¼‰
        timestamps_array = np.array(timestamps)
        target_timestamps_array = np.array(target_timestamps)
        
        # ä½¿ç”¨searchsortedæ‰¾åˆ°æ¯ä¸ªç›®æ ‡æ—¶é—´æˆ³åœ¨æºæ—¶é—´æˆ³ä¸­çš„æ’å…¥ä½ç½®
        # side='right'è¡¨ç¤ºæ‰¾åˆ°ç¬¬ä¸€ä¸ªå¤§äºç­‰äºç›®æ ‡å€¼çš„ä½ç½®
        indices = np.searchsorted(timestamps_array, target_timestamps_array, side='right')
        
        # å¤„ç†è¾¹ç•Œæƒ…å†µï¼šç¡®ä¿ç´¢å¼•åœ¨æœ‰æ•ˆèŒƒå›´å†…
        indices = np.clip(indices - 1, 0, len(messages) - 1)
        
        # ç›´æ¥ä½¿ç”¨ç´¢å¼•è·å–å¯¹åº”çš„æ¶ˆæ¯ï¼ˆO(1)ï¼‰
        interpolated_images = [messages[i] for i in indices]
        
        return interpolated_images

    def _preprocess_downsample(self, messages, timestamps, original_freq, target_freq):
        """
        é¢„å¤„ç†é™é‡‡æ ·ï¼šä½¿ç”¨æ•´æ•°å€å¹³å‡é™é‡‡æ ·
        
        Args:
            messages: åŸå§‹æ¶ˆæ¯åˆ—è¡¨
            timestamps: åŸå§‹æ—¶é—´æˆ³
            original_freq: åŸå§‹é¢‘ç‡
            target_freq: ç›®æ ‡é¢‘ç‡
            
        Returns:
            downsampled_messages, downsampled_timestamps
        """
        if not messages or len(messages) == 0:
            return messages, timestamps
        
        # è®¡ç®—é™é‡‡æ ·æ¯”ä¾‹
        ratio = int(original_freq / target_freq)
        if ratio <= 1:
            # ä¸éœ€è¦é™é‡‡æ ·
            return messages, timestamps
        
        # è®¡ç®—å¯ç”¨çš„æ ·æœ¬æ•°
        n_samples = len(messages) // ratio
        if n_samples == 0:
            return messages, timestamps
        
        downsampled_messages = []
        downsampled_timestamps = []
        
        for i in range(n_samples):
            start_idx = i * ratio
            end_idx = start_idx + ratio
            
            # æ—¶é—´æˆ³å–å¹³å‡
            avg_timestamp = np.mean(timestamps[start_idx:end_idx])
            downsampled_timestamps.append(avg_timestamp)
            
            # æ¶ˆæ¯æ•°æ®å–å¹³å‡
            batch_messages = messages[start_idx:end_idx]
            
            # æ£€æŸ¥æ¶ˆæ¯ç±»å‹
            if hasattr(batch_messages[0], 'position'):
                # å…³èŠ‚æ•°æ®ï¼šå¯¹positionè¿›è¡Œå¹³å‡
                positions = []
                for msg in batch_messages:
                    if hasattr(msg, 'position') and len(msg.position) > 0:
                        positions.append(list(msg.position))
                
                if positions:
                    # è®¡ç®—å¹³å‡ä½ç½®
                    avg_position = np.mean(positions, axis=0)
                    # åˆ›å»ºæ–°æ¶ˆæ¯ï¼ˆä¿æŒç¬¬ä¸€ä¸ªæ¶ˆæ¯çš„ç»“æ„ï¼Œåªæ›´æ–°positionï¼‰
                    avg_msg = batch_messages[0]
                    # æ³¨æ„ï¼šè¿™é‡Œç›´æ¥ä½¿ç”¨å¹³å‡å€¼ï¼Œå®é™…åº”è¯¥åˆ›å»ºæ–°çš„æ¶ˆæ¯å¯¹è±¡
                    # ä½†ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬ç›´æ¥ä¿®æ”¹positionå±æ€§
                    downsampled_messages.append(avg_msg)  # æš‚æ—¶ä½¿ç”¨ç¬¬ä¸€ä¸ªæ¶ˆæ¯
                else:
                    downsampled_messages.append(batch_messages[0])
            else:
                # å…¶ä»–ç±»å‹ï¼šä½¿ç”¨ä¸­é—´çš„æ¶ˆæ¯
                mid_idx = ratio // 2
                downsampled_messages.append(batch_messages[mid_idx])
        
        return downsampled_messages, np.array(downsampled_timestamps)
    
    def _average_align_to_target_freq(self, messages, timestamps, target_freq):
        """
        æ•´æ•°å€å¹³å‡å¯¹é½åˆ°ç›®æ ‡é¢‘ç‡
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            timestamps: æ—¶é—´æˆ³
            target_freq: ç›®æ ‡é¢‘ç‡ (30Hz)
            
        Returns:
            aligned_messages, aligned_timestamps
        """
        if not messages or len(messages) == 0:
            return messages, timestamps
        
        # è®¡ç®—å®é™…é¢‘ç‡
        duration = (timestamps[-1] - timestamps[0]) / 1e9  # è½¬æ¢ä¸ºç§’
        actual_freq = len(messages) / duration
        
        # è®¡ç®—é™é‡‡æ ·æ¯”ä¾‹
        ratio = int(actual_freq / target_freq)
        if ratio <= 1:
            return messages, timestamps
        
        # ä½¿ç”¨æ•´æ•°å€å¹³å‡
        return self._preprocess_downsample(messages, timestamps, actual_freq, target_freq)

    def _interpolate_joint_data_batch(self, messages, timestamps, target_timestamps):
        """æœ€ä¼˜é€‰æ‹©ç­–ç•¥ - ä¼˜å…ˆä½¿ç”¨ç›¸ç­‰å€¼ï¼Œå¦åˆ™ä½¿ç”¨å³ä¾§ï¼ˆä¹‹åï¼‰æœ€æ¥è¿‘çš„æ•°æ®ç‚¹

        ç­–ç•¥ï¼š
        1. å¦‚æœaction/stateæ—¶é—´æˆ³åˆšå¥½ç­‰äºç›¸æœºæ—¶é—´æˆ³ï¼Œä½¿ç”¨ç›¸ç­‰çš„å€¼
        2. å¦‚æœä¸ç›¸ç­‰ï¼Œä¸¥æ ¼ä½¿ç”¨å³ä¾§ï¼ˆä¹‹åï¼‰æœ€æ¥è¿‘çš„æ•°æ®ç‚¹
        è¿™æ ·å¯ä»¥ï¼š
        1. ä¿è¯å› æœæ€§ - ä¼˜å…ˆä½¿ç”¨åŒæ­¥æ•°æ®ï¼Œå¦åˆ™ä½¿ç”¨"å·²ç»å‘ç”Ÿ"çš„æ•°æ®
        2. é¿å…æ’å€¼è¯¯å·®
        3. ä¿ç•™çœŸå®æµ‹é‡æ•°æ®
        """
        if not messages:
            return [None] * len(target_timestamps)

        # æå–å…³èŠ‚ä½ç½®æ•°æ®
        joint_matrix: List[List[float]] = []
        for msg in messages:
            if hasattr(msg, 'position') and msg.position:
                joint_matrix.append(list(msg.position))
            else:
                # è‹¥æ— æ•°æ®ï¼Œç”¨ä¸Šä¸€å¸§æˆ–é›¶å¡«å……ï¼ˆç»´åº¦ä»¥7ä¸ºä¸Šé™ï¼ŒæŒ‰å¸¸è§è‡‚ï¼‰
                joint_matrix.append([0.0] * 7)

        if not joint_matrix:
            return [None] * len(target_timestamps)

        src_t = np.asarray(timestamps, dtype=np.float64)
        tgt_t = np.asarray(target_timestamps, dtype=np.float64)
        joints = np.asarray(joint_matrix, dtype=np.float32)

        num_frames, num_joints = joints.shape
        result = []

        # â­ ä¼˜åŒ–ç­–ç•¥ï¼šä¼˜å…ˆä½¿ç”¨ç›¸ç­‰å€¼ï¼Œå¦åˆ™ä½¿ç”¨å³ä¾§ç¬¬ä¸€ä¸ª
        # ä½¿ç”¨searchsortedæ‰¾åˆ°ç¬¬ä¸€ä¸ª>=target_timeçš„ä½ç½®
        indices_right = np.searchsorted(src_t, tgt_t, side='right')
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ç›¸ç­‰å€¼ï¼šsearchsorted(..., side='left')è¿”å›ç¬¬ä¸€ä¸ª>=targetçš„ä½ç½®
        # å¦‚æœleftå’Œrightç»“æœä¸åŒï¼Œè¯´æ˜æœ‰ç›¸ç­‰å€¼
        indices_left = np.searchsorted(src_t, tgt_t, side='left')
        
        # å¯¹äºæ¯ä¸ªç›®æ ‡æ—¶é—´æˆ³ï¼š
        # 1. å¦‚æœindices_left < indices_rightï¼Œè¯´æ˜æœ‰ç›¸ç­‰å€¼ï¼Œä½¿ç”¨indices_left
        # 2. å¦åˆ™ï¼Œä½¿ç”¨indices_rightï¼ˆå³ä¾§ç¬¬ä¸€ä¸ªï¼‰
        indices = np.where(indices_left < indices_right, indices_left, indices_right)
        
        # å¤„ç†è¾¹ç•Œæƒ…å†µï¼šç¡®ä¿ç´¢å¼•åœ¨æœ‰æ•ˆèŒƒå›´å†…
        indices = np.clip(indices, 0, num_frames - 1)
        
        # ç›´æ¥ä½¿ç”¨ç´¢å¼•è·å–å¯¹åº”çš„å…³èŠ‚æ•°æ®ï¼ˆO(1)ï¼‰
        result = joints[indices].tolist()
        
        return result

    def _interpolate_scalar_data_batch(self, messages, timestamps, target_timestamps):
        """æœ€ä¼˜é€‰æ‹©ç­–ç•¥ - ä¼˜å…ˆä½¿ç”¨ç›¸ç­‰å€¼ï¼Œå¦åˆ™ä½¿ç”¨å³ä¾§ï¼ˆä¹‹åï¼‰æœ€æ¥è¿‘çš„æ•°æ®ç‚¹"""
        if not messages:
            return [None] * len(target_timestamps)

        values = []
        for msg in messages:
            if hasattr(msg, 'data'):
                try:
                    values.append(float(msg.data))
                except Exception:
                    values.append(0.0)
            else:
                values.append(0.0)

        if not values:
            return [None] * len(target_timestamps)

        src_t = np.asarray(timestamps, dtype=np.float64)
        tgt_t = np.asarray(target_timestamps, dtype=np.float64)
        src_y = np.asarray(values, dtype=np.float64)

        # â­ ä¼˜åŒ–ç­–ç•¥ï¼šä¼˜å…ˆä½¿ç”¨ç›¸ç­‰å€¼ï¼Œå¦åˆ™ä½¿ç”¨å³ä¾§ç¬¬ä¸€ä¸ª
        # ä½¿ç”¨searchsortedæ‰¾åˆ°ç¬¬ä¸€ä¸ª>=target_timeçš„ä½ç½®
        indices_right = np.searchsorted(src_t, tgt_t, side='right')
        indices_left = np.searchsorted(src_t, tgt_t, side='left')
        
        # å¦‚æœindices_left < indices_rightï¼Œè¯´æ˜æœ‰ç›¸ç­‰å€¼ï¼Œä½¿ç”¨indices_left
        # å¦åˆ™ï¼Œä½¿ç”¨indices_rightï¼ˆå³ä¾§ç¬¬ä¸€ä¸ªï¼‰
        indices = np.where(indices_left < indices_right, indices_left, indices_right)
        
        # å¤„ç†è¾¹ç•Œæƒ…å†µï¼šç¡®ä¿ç´¢å¼•åœ¨æœ‰æ•ˆèŒƒå›´å†…
        indices = np.clip(indices, 0, len(src_y) - 1)
        
        # ç›´æ¥ä½¿ç”¨ç´¢å¼•è·å–å¯¹åº”çš„å€¼ï¼ˆO(1)ï¼‰
        result = src_y[indices].tolist()
        
        return result

    def _interpolate_nearest_neighbor_batch(self, messages, timestamps, target_timestamps):
        """æœ€ä¼˜é€‰æ‹©ç­–ç•¥ - ä¼˜å…ˆä½¿ç”¨ç›¸ç­‰å€¼ï¼Œå¦åˆ™ä½¿ç”¨å³ä¾§ï¼ˆä¹‹åï¼‰æœ€æ¥è¿‘çš„æ•°æ®ç‚¹"""
        if not messages:
            return [None] * len(target_timestamps)

        # â­ ä¼˜åŒ–ç­–ç•¥ï¼šä¼˜å…ˆä½¿ç”¨ç›¸ç­‰å€¼ï¼Œå¦åˆ™ä½¿ç”¨å³ä¾§ç¬¬ä¸€ä¸ª
        src_t = np.asarray(timestamps, dtype=np.float64)
        tgt_t = np.asarray(target_timestamps, dtype=np.float64)
        
        # ä½¿ç”¨searchsortedæ‰¾åˆ°ç¬¬ä¸€ä¸ª>=target_timeçš„ä½ç½®
        indices_right = np.searchsorted(src_t, tgt_t, side='right')
        indices_left = np.searchsorted(src_t, tgt_t, side='left')
        
        # å¦‚æœindices_left < indices_rightï¼Œè¯´æ˜æœ‰ç›¸ç­‰å€¼ï¼Œä½¿ç”¨indices_left
        # å¦åˆ™ï¼Œä½¿ç”¨indices_rightï¼ˆå³ä¾§ç¬¬ä¸€ä¸ªï¼‰
        indices = np.where(indices_left < indices_right, indices_left, indices_right)
        
        # å¤„ç†è¾¹ç•Œæƒ…å†µï¼šç¡®ä¿ç´¢å¼•åœ¨æœ‰æ•ˆèŒƒå›´å†…
        indices = np.clip(indices, 0, len(messages) - 1)
        
        # ç›´æ¥ä½¿ç”¨ç´¢å¼•è·å–å¯¹åº”çš„æ¶ˆæ¯ï¼ˆO(1)ï¼‰
        interpolated_data = [messages[i] for i in indices]
        
        return interpolated_data

    def _init_video_writer(self, out_path, width, height, fps):
        fourcc_candidates = ['mp4v']  # ä½œä¸ºå ä½ç¼–ç ï¼Œåç»­ç»Ÿä¸€è½¬ç ä¸ºAV1
        for code in fourcc_candidates:
            fourcc = cv2.VideoWriter_fourcc(*code)
            vw = cv2.VideoWriter(str(out_path), fourcc, fps, (width, height))
            if vw.isOpened():
                # print(f"[Video] ä½¿ç”¨å ä½ç¼–ç  {code} å†™å…¥: {out_path}")
                return vw, code, out_path

        # å›é€€åˆ° MJPG/AVIï¼ˆç¯å¢ƒä¸æ”¯æŒ mp4v æ—¶ï¼‰
        print("[Video] æ— æ³•ä½¿ç”¨ mp4vï¼Œå°è¯• MJPG/AVI å›é€€")
        fourcc = cv2.VideoWriter_fourcc(*'MJPG')
        alt_path = Path(str(out_path)).with_suffix('.avi')
        vw = cv2.VideoWriter(str(alt_path), fourcc, fps, (width, height))
        if vw.isOpened():
            print(f"[Video] å›é€€åˆ° MJPG ç¼–ç ï¼Œè¾“å‡º: {alt_path}")
            return vw, 'MJPG', alt_path

        # å…¨éƒ¨å¤±è´¥
        return None, None, out_path

    def _transcode_to_av1_mp4(self, input_path, crf=30, cpu_used=8, pix_fmt="yuv420p", fps=None):
        """ä½¿ç”¨ffmpegå°†è§†é¢‘è½¬ç ä¸º AV1 (libaom-av1) MP4ã€‚
        - MP4ï¼šå†™å…¥ä¸´æ—¶æ–‡ä»¶ï¼Œç„¶åè¦†ç›–åŸå§‹æ–‡ä»¶å
        - AVIï¼šç”ŸæˆåŒå .mp4 æ–‡ä»¶
        è¿”å›æœ€ç»ˆè¾“å‡ºè·¯å¾„ï¼ˆPathï¼‰æˆ–Noneï¼ˆå¤±è´¥ï¼‰
        """
        from pathlib import Path

        p = Path(str(input_path).replace("\\", "/")).expanduser().resolve()
        if not p.exists():
            print(f"è½¬ç è·³è¿‡ï¼Œæ–‡ä»¶ä¸å­˜åœ¨ï¼š{p}")
            return None

        if shutil.which("ffmpeg") is None:
            raise RuntimeError("æœªæ£€æµ‹åˆ° ffmpegï¼Œæ— æ³•ç”Ÿæˆ AV1 è§†é¢‘ã€‚è¯·å…ˆå®‰è£…ï¼šsudo apt-get install ffmpeg")

        # MP4ï¼šå…ˆå†™ä¸´æ—¶ï¼Œå†è¦†ç›–åŸæ–‡ä»¶åï¼›AVIï¼šåŒå .mp4
        if p.suffix.lower() == ".mp4":
            out_path = p.with_name(p.stem + ".__tmp__av1.mp4")
            final_path = p
        elif p.suffix.lower() == ".avi":
            out_path = p.with_suffix(".mp4")
            final_path = out_path
        else:
            out_path = p.with_name(p.stem + "_av1.mp4")
            final_path = out_path

        ffmpeg_threads_env = os.environ.get("FFMPEG_THREADS")
        try:
            ffmpeg_threads = int(ffmpeg_threads_env) if ffmpeg_threads_env else 0
        except Exception:
            ffmpeg_threads = 0
        cpu_used_env = os.environ.get("FFMPEG_CPU_USED")
        try:
            cpu_used_val = int(cpu_used_env) if cpu_used_env is not None else cpu_used
        except Exception:
            cpu_used_val = cpu_used

        encoder = "libaom-av1"
        try:
            enc_probe = subprocess.run([
                "ffmpeg", "-hide_banner", "-encoders"
            ], capture_output=True, text=True, check=False)
            if enc_probe.returncode == 0 and "libsvtav1" in enc_probe.stdout:
                encoder = "libsvtav1"
        except Exception:
            pass

        cmd = [
            "ffmpeg", "-y", "-hide_banner", "-loglevel", "error",
            "-i", str(p),
            "-map", "0:v:0",
            "-c:v", encoder,
            "-crf", str(crf),
            "-b:v", "0",
            "-cpu-used", str(cpu_used_val),
            "-pix_fmt", pix_fmt,
            "-movflags", "+faststart",
        ]
        if ffmpeg_threads > 0:
            cmd.extend(["-threads", str(ffmpeg_threads)])
        else:
            cmd.extend(["-threads", str(psutil.cpu_count() or 1)])
        cmd.extend(["-progress", "pipe:1"])
        if fps:
            cmd.extend(["-r", str(fps)])
        cmd.append(str(out_path))

        # print(f"è‡ªåŠ¨è½¬ç ä¸º AV1 ({encoder}) MP4: {p} -> {out_path}")
        try:
            duration_s = None
            try:
                probe = subprocess.run([
                    "ffprobe", "-v", "error", "-show_entries", "format=duration",
                    "-of", "default=noprint_wrappers=1:nokey=1", str(p)
                ], capture_output=True, text=True, check=False)
                if probe.returncode == 0:
                    duration_s = float(probe.stdout.strip())
            except Exception:
                duration_s = None

            proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
            pbar = tqdm(total=max(duration_s or 0.0, 0.0), unit="s", desc="AV1è½¬ç ", leave=False) if duration_s else None
            last_time = 0.0
            if proc.stdout is not None:
                for line in proc.stdout:
                    if line.startswith("out_time_ms="):
                        try:
                            ms = float(line.split("=", 1)[1].strip())
                            seconds = ms / 1e6
                            if pbar:
                                delta = max(0.0, seconds - last_time)
                                pbar.update(delta)
                            last_time = seconds
                        except Exception:
                            pass
            ret = proc.wait()
            if pbar:
                pbar.close()
            if ret == 0:
                if p.suffix.lower() == ".mp4":
                    try:
                        out_path.replace(final_path)
                        # print(f"è½¬ç å®Œæˆå¹¶è¦†ç›–åŸæ–‡ä»¶: {final_path}")
                    except Exception as e:
                        print(f"è¦†ç›–åŸæ–‡ä»¶å¤±è´¥: {final_path} -> {e}")
                        return None
                else:
                    print(f"è½¬ç å®Œæˆ: {final_path}")
                return final_path
            else:
                err = ""
                try:
                    if proc.stderr:
                        err = proc.stderr.read() or ""
                except Exception:
                    err = ""
                print(f"AV1 è½¬ç å¤±è´¥({ret}): {p}")
                if err:
                    print(err[-2000:])
                return None
        except Exception as e:
            print(f"è½¬ç å¼‚å¸¸: {p} -> {e}")
            return None

    def create_mp4_videos(self, synchronized_images, video_output_dir, resolution):
        """åˆ›å»ºMP4è§†é¢‘æ–‡ä»¶ï¼ˆåŸºäºå·²å¯¹é½çš„å›¾åƒå¸§ï¼‰
        Args:
            synchronized_images: dict[str, list[np.ndarray|None]]ï¼Œé”®ä¸ºç›¸æœºåç§°ï¼Œå€¼ä¸ºå¸§åˆ—è¡¨
            video_output_dir: è¾“å‡ºè§†é¢‘çš„ç›®å½•è·¯å¾„ï¼ˆPathï¼‰
            resolution: (width, height) ç›®æ ‡åˆ†è¾¨ç‡
        Returns:
            dict: {camera_name: {path, frames, resolution, fps}}
        """
        print("\nåˆ›å»ºMP4è§†é¢‘æ–‡ä»¶...")
        import cv2
        import numpy as np

        width, height = resolution
        fps = TARGET_FREQUENCY

        writers = {}
        video_info = {}

        # ä¸ºæ¯ä¸ªç›¸æœºåˆå§‹åŒ–å†™å…¥å™¨ï¼ˆå¸¦ç¼–ç å™¨å›é€€ï¼‰
        for camera_name, frames in synchronized_images.items():
            video_path = (video_output_dir / f"{camera_name}.mp4")
            writer, codec_used, final_path = self._init_video_writer(video_path, width, height, fps)
            if writer is None:
                print(f"[Video][{camera_name}] âŒ æ— æ³•åˆå§‹åŒ–è§†é¢‘å†™å…¥å™¨ã€‚è¯·æ£€æŸ¥ OpenCV/FFmpeg ç¼–ç æ”¯æŒã€‚ç›®æ ‡åˆ†è¾¨ç‡: {width}x{height}")
                continue
            writers[camera_name] = (writer, final_path, codec_used)

        if not writers:
            print("[Video] âŒ æ‰€æœ‰ç›¸æœºå†™å…¥å™¨åˆå§‹åŒ–å¤±è´¥ï¼Œè·³è¿‡è§†é¢‘ç”Ÿæˆ")
            return {}

        # å†™å¸§ï¼šç»Ÿä¸€é€šé“ä¸å°ºå¯¸ï¼Œç¼ºå¸§å¡«å……ç©ºç™½å›¾
        blank = np.zeros((height, width, 3), dtype=np.uint8)
        for camera_name, frames in synchronized_images.items():
            if camera_name not in writers:
                continue
            writer, final_path, codec_used = writers[camera_name]

            failed = 0
            written = 0
            for frame in frames:
                img = frame
                if img is None:
                    img = blank
                    failed += 1

                # å¦‚æœå›¾åƒä¸æ˜¯numpyæ•°ç»„ï¼Œå›é€€åˆ°ç©ºç™½å¸§
                if not isinstance(img, np.ndarray):
                    img = blank

                # ç°åº¦/Alpha ç»Ÿä¸€ä¸º BGR ä¸‰é€šé“ï¼›RGB è½¬ BGRï¼ˆOpenCVå†™å…¥æœŸæœ›BGRï¼‰
                if img.ndim == 2:
                    img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)
                elif img.ndim == 3:
                    if img.shape[2] == 4:
                        img = cv2.cvtColor(img, cv2.COLOR_BGRA2BGR)
                    elif img.shape[2] == 3:
                        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
                    else:
                        img = blank
                else:
                    img = blank

                # ç»Ÿä¸€dtypeä¸ºuint8
                if img.dtype != np.uint8:
                    img = np.clip(img, 0, 255).astype(np.uint8)

                # å°ºå¯¸ç»Ÿä¸€åˆ°ç›®æ ‡åˆ†è¾¨ç‡
                if img.shape[0] != height or img.shape[1] != width:
                    img = cv2.resize(img, (width, height), interpolation=cv2.INTER_AREA)

                writer.write(img)
                written += 1

            writer.release()
            # è‡ªåŠ¨è½¬ç ä¸º AV1 MP4ï¼ˆå¦‚å·²æ˜¯MP4åˆ™è¦†ç›–åŸæ–‡ä»¶åï¼›AVIåˆ™ç”ŸæˆåŒåMP4ï¼‰
            transcoded_path = self._transcode_to_av1_mp4(final_path, fps=fps)
            final_out_path = transcoded_path if transcoded_path else final_path
            rel_path = os.path.relpath(str(final_out_path), str(self.output_dir))
            video_info[camera_name] = {
                'path': rel_path,
                'frames': written,
                'resolution': [width, height],
                'fps': fps
            }
            print(f"[Video][{camera_name}] âœ… å†™å…¥å®Œæˆ: {final_out_path}ï¼Œç¼–ç å™¨: {codec_used}ï¼Œå¸§æ•°: {written}ï¼Œè§£ç å¤±è´¥å¡«å……: {failed}")

        return video_info

    def generate_lerobot_v21_dataset(self):
        """ç”Ÿæˆ LeRobot v2.1 æ ‡å‡†æ ¼å¼æ•°æ®é›†"""
        print("\nç”Ÿæˆ LeRobot v2.1 æ ‡å‡†æ ¼å¼æ•°æ®é›†...")

        # åˆ›å»ºMP4è§†é¢‘ï¼ˆä½¿ç”¨æ–°çš„å†™å…¥æ¥å£ï¼‰
        camera_mapping = {
            "/camera_head/color/image_raw/compressed": "camera_head_rgb",
            "/camera_left/color/image_raw/compressed": "camera_left_wrist_rgb",
            "/camera_right/color/image_raw/compressed": "camera_right_wrist_rgb"
        }
        synchronized_images = {}
        for topic_name, camera_name in camera_mapping.items():
            if topic_name in self.synchronized_data:
                synchronized_images[camera_name] = self.synchronized_data[topic_name]
        video_output_dir = self.videos_dir / "chunk-000"
        video_output_dir.mkdir(parents=True, exist_ok=True)
        video_info = self.create_mp4_videos(synchronized_images, video_output_dir, (self.target_width, self.target_height))

        # æ„å»ºæ•°æ®æ¡†
        data_rows = []

        # è®¡ç®—æ€»å¸§æ•° - ä½¿ç”¨æ‰€æœ‰ç›¸æœºçš„æœ€å°å¸§æ•°ç¡®ä¿ä¸€è‡´æ€§ï¼ˆåªå¤„ç†RGBå›¾åƒï¼‰
        image_topics = [topic for topic in self.synchronized_data.keys() if 'image' in topic and 'depth' not in topic]
        if image_topics:
            frame_counts = [len(self.synchronized_data[topic]) for topic in image_topics]
            max_frames = min(frame_counts)  # ä½¿ç”¨æœ€å°å¸§æ•°ç¡®ä¿æ‰€æœ‰ç›¸æœºä¸€è‡´
            print(f"  ç›¸æœºå¸§æ•°: {frame_counts}")
            print(f"  æ€»å¸§æ•°: {max_frames} (ä½¿ç”¨æœ€å°å¸§æ•°ç¡®ä¿ä¸€è‡´æ€§)")
        else:
            max_frames = 0
            for topic_data in self.synchronized_data.values():
                max_frames = max(max_frames, len(topic_data))
            print(f"  æ€»å¸§æ•°: {max_frames} (åŸºäºæ‰€æœ‰æ•°æ®)")

        # ç”Ÿæˆç²¾ç¡®çš„30Hzæ—¶é—´æˆ³ï¼Œç¡®ä¿ä¸è§†é¢‘æ–‡ä»¶å®Œå…¨åŒ¹é…
        # ä½¿ç”¨ç²¾ç¡®çš„1/30ç§’é—´éš”ï¼Œé¿å…æµ®ç‚¹æ•°ç²¾åº¦é—®é¢˜
        image_timestamps = np.arange(max_frames) / TARGET_FREQUENCY

        for i in range(max_frames):
            row = {}  # å…ˆåˆ›å»ºç©ºå­—å…¸ï¼ŒæŒ‰æ­£ç¡®é¡ºåºæ·»åŠ åˆ—

            # LeRobot v2.1æ ‡å‡†ï¼šå›¾åƒé€šè¿‡è§†é¢‘æ–‡ä»¶å¤„ç†ï¼Œä¸åœ¨Parquetä¸­åŒ…å«å›¾åƒåˆ—
            # å›¾åƒæ•°æ®é€šè¿‡video_pathåœ¨info.jsonä¸­å®šä¹‰

            # æ·»åŠ å…³èŠ‚æ•°æ® - åˆå¹¶ä¸ºå•ä¸ªæ•°ç»„åˆ—
            # ===== ACTIONæ•°æ®ï¼šä½¿ç”¨ä¸»è‡‚æ•°æ®ï¼ˆleaderï¼‰ =====
            # å·¦ä¸»è‡‚å…³èŠ‚æ•°æ®ï¼ˆç”¨äºactionï¼‰
            left_leader_arm_data = []
            if '/leader_left/joint_states' in self.synchronized_data and i < len(self.synchronized_data['/leader_left/joint_states']):
                joint_data = self.synchronized_data['/leader_left/joint_states'][i]
                if joint_data is not None:
                    left_leader_arm_data = [float(x) for x in joint_data]
                else:
                    left_leader_arm_data = [0.0] * 7
            else:
                left_leader_arm_data = [0.0] * 7
            
            # ç¡®ä¿æœ‰7ä¸ªå…ƒç´ ï¼ˆ7ä¸ªå…³èŠ‚ï¼Œä¸åŒ…å«å¤¹çˆªï¼‰
            if len(left_leader_arm_data) < 7:
                left_leader_arm_data.extend([0.0] * (7 - len(left_leader_arm_data)))
            elif len(left_leader_arm_data) > 7:
                left_leader_arm_data = left_leader_arm_data[:7]
            
            # å³ä¸»è‡‚å…³èŠ‚æ•°æ®ï¼ˆç”¨äºactionï¼‰
            right_leader_arm_data = []
            if '/leader_right/joint_states' in self.synchronized_data and i < len(self.synchronized_data['/leader_right/joint_states']):
                joint_data = self.synchronized_data['/leader_right/joint_states'][i]
                if joint_data is not None:
                    right_leader_arm_data = [float(x) for x in joint_data]
                else:
                    right_leader_arm_data = [0.0] * 7
            else:
                right_leader_arm_data = [0.0] * 7
            
            # ç¡®ä¿æœ‰7ä¸ªå…ƒç´ ï¼ˆ7ä¸ªå…³èŠ‚ï¼Œä¸åŒ…å«å¤¹çˆªï¼‰
            if len(right_leader_arm_data) < 7:
                right_leader_arm_data.extend([0.0] * (7 - len(right_leader_arm_data)))
            elif len(right_leader_arm_data) > 7:
                right_leader_arm_data = right_leader_arm_data[:7]
            
            # ===== STATEæ•°æ®ï¼šä½¿ç”¨ä»è‡‚æ•°æ®ï¼ˆfollowerï¼‰ =====
            # å·¦ä»è‡‚å…³èŠ‚æ•°æ®ï¼ˆç”¨äºobservation.stateï¼‰
            left_follower_arm_data = []
            if '/left_arm_controller/joint_states' in self.synchronized_data and i < len(self.synchronized_data['/left_arm_controller/joint_states']):
                joint_data = self.synchronized_data['/left_arm_controller/joint_states'][i]
                if joint_data is not None:
                    left_follower_arm_data = [float(x) for x in joint_data]
                else:
                    left_follower_arm_data = [0.0] * 7
            else:
                left_follower_arm_data = [0.0] * 7
            
            # ç¡®ä¿æœ‰7ä¸ªå…ƒç´ ï¼ˆ7ä¸ªå…³èŠ‚ï¼Œä¸åŒ…å«å¤¹çˆªï¼‰
            if len(left_follower_arm_data) < 7:
                left_follower_arm_data.extend([0.0] * (7 - len(left_follower_arm_data)))
            elif len(left_follower_arm_data) > 7:
                left_follower_arm_data = left_follower_arm_data[:7]
            
            # å³ä»è‡‚å…³èŠ‚æ•°æ®ï¼ˆç”¨äºobservation.stateï¼‰
            right_follower_arm_data = []
            if '/right_arm_controller/joint_states' in self.synchronized_data and i < len(self.synchronized_data['/right_arm_controller/joint_states']):
                joint_data = self.synchronized_data['/right_arm_controller/joint_states'][i]
                if joint_data is not None:
                    right_follower_arm_data = [float(x) for x in joint_data]
                else:
                    right_follower_arm_data = [0.0] * 7
            else:
                right_follower_arm_data = [0.0] * 7
            
            # ç¡®ä¿æœ‰7ä¸ªå…ƒç´ ï¼ˆ7ä¸ªå…³èŠ‚ï¼Œä¸åŒ…å«å¤¹çˆªï¼‰
            if len(right_follower_arm_data) < 7:
                right_follower_arm_data.extend([0.0] * (7 - len(right_follower_arm_data)))
            elif len(right_follower_arm_data) > 7:
                right_follower_arm_data = right_follower_arm_data[:7]
            
            # ===== ACTIONå¤¹çˆªæ•°æ®ï¼šä½¿ç”¨ä¸»è‡‚å¤¹çˆªæ•°æ® =====
            # å·¦ä¸»è‡‚å¤¹çˆªæ•°æ®ï¼ˆç”¨äºactionï¼‰
            left_leader_gripper_value = 0.0
            if '/left_tool_status' in self.synchronized_data and i < len(self.synchronized_data['/left_tool_status']):
                gripper_data = self.synchronized_data['/left_tool_status'][i]
                if gripper_data is not None and len(gripper_data) > 0:
                    left_leader_gripper_value = float(gripper_data[0])
            
            # å³ä¸»è‡‚å¤¹çˆªæ•°æ®ï¼ˆç”¨äºactionï¼‰
            right_leader_gripper_value = 0.0
            if '/right_tool_status' in self.synchronized_data and i < len(self.synchronized_data['/right_tool_status']):
                gripper_data = self.synchronized_data['/right_tool_status'][i]
                if gripper_data is not None and len(gripper_data) > 0:
                    right_leader_gripper_value = float(gripper_data[0])
            
            # ===== STATEå¤¹çˆªæ•°æ®ï¼šä½¿ç”¨ä»è‡‚å¤¹çˆªæ•°æ® =====
            # å·¦ä»è‡‚å¤¹çˆªæ•°æ®ï¼ˆç”¨äºobservation.stateï¼‰
            left_follower_gripper_value = 0.0
            if '/left_arm_controller/rm_driver/gripper_pos' in self.synchronized_data and i < len(self.synchronized_data['/left_arm_controller/rm_driver/gripper_pos']):
                gripper_data = self.synchronized_data['/left_arm_controller/rm_driver/gripper_pos'][i]
                if gripper_data is not None and len(gripper_data) > 0:
                    left_follower_gripper_value = float(gripper_data[0])
            
            # å³ä»è‡‚å¤¹çˆªæ•°æ®ï¼ˆç”¨äºobservation.stateï¼‰
            right_follower_gripper_value = 0.0
            if '/right_arm_controller/rm_driver/gripper_pos' in self.synchronized_data and i < len(self.synchronized_data['/right_arm_controller/rm_driver/gripper_pos']):
                gripper_data = self.synchronized_data['/right_arm_controller/rm_driver/gripper_pos'][i]
                if gripper_data is not None and len(gripper_data) > 0:
                    right_follower_gripper_value = float(gripper_data[0])
            
            # è·å–èƒ¸éƒ¨å‡é™ï¼ˆliftï¼‰æ•°å€¼ - åˆå¹¶ä¸¤ä¸ªç‹¬ç«‹çš„lift topic
            # /leader_lift/lift_down_state: 0=ä¸åŠ¨, 1=ä¸‹é™
            # /leader_lift/lift_up_state: 0=ä¸åŠ¨, 1=ä¸Šå‡
            # åˆå¹¶å: 1=ä¸Šå‡, 0=ä¸åŠ¨, -1=ä¸‹é™
            lift_value = 0.0
            
            # è·å–ä¸Šå‡çŠ¶æ€
            lift_up_value = 0.0
            if '/leader_lift/lift_up_state' in self.synchronized_data and i < len(self.synchronized_data['/leader_lift/lift_up_state']):
                lift_up_data = self.synchronized_data['/leader_lift/lift_up_state'][i]
                if lift_up_data is not None:
                    try:
                        lift_up_value = float(lift_up_data if not hasattr(lift_up_data, '__len__') else lift_up_data[0])
                    except Exception:
                        lift_up_value = 0.0
            
            # è·å–ä¸‹é™çŠ¶æ€
            lift_down_value = 0.0
            if '/leader_lift/lift_down_state' in self.synchronized_data and i < len(self.synchronized_data['/leader_lift/lift_down_state']):
                lift_down_data = self.synchronized_data['/leader_lift/lift_down_state'][i]
                if lift_down_data is not None:
                    try:
                        lift_down_value = float(lift_down_data if not hasattr(lift_down_data, '__len__') else lift_down_data[0])
                    except Exception:
                        lift_down_value = 0.0
            
            # åˆå¹¶liftçŠ¶æ€: 1=ä¸Šå‡, 0=ä¸åŠ¨, -1=ä¸‹é™
            if lift_up_value > 0.5:  # ä¸Šå‡çŠ¶æ€æ¿€æ´»
                lift_value = 1.0
            elif lift_down_value > 0.5:  # ä¸‹é™çŠ¶æ€æ¿€æ´»
                lift_value = -1.0
            else:  # ä¸¤ä¸ªçŠ¶æ€éƒ½ä¸æ¿€æ´»ï¼Œä¿æŒä¸åŠ¨
                lift_value = 0.0
            
            # è°ƒè¯•ä¿¡æ¯ï¼šæ¯100å¸§æ‰“å°ä¸€æ¬¡liftçŠ¶æ€ï¼ˆå¯é€‰ï¼‰
            if i % 100 == 0 and (lift_up_value > 0.5 or lift_down_value > 0.5):
                print(f"    å¸§ {i}: lift_up={lift_up_value:.1f}, lift_down={lift_down_value:.1f}, lift_value={lift_value:.1f}")
            
            # 1. actionï¼ˆä¸»è‡‚æ•°æ®ï¼šå·¦ä¸»è‡‚7å…³èŠ‚ + å·¦ä¸»è‡‚å¤¹çˆª + å³ä¸»è‡‚7å…³èŠ‚ + å³ä¸»è‡‚å¤¹çˆª + LiftçŠ¶æ€ï¼Œå…±17ç»´ï¼‰
            action_data = left_leader_arm_data + [left_leader_gripper_value] + right_leader_arm_data + [right_leader_gripper_value] + [lift_value]
            row['action'] = np.array(action_data, dtype=np.float32)
            
            # è·å–followerçš„liftçŠ¶æ€ï¼ˆå®é™…ä½ç½®ï¼‰
            state_lift_value = 0.0
            # è°ƒè¯•ä¿¡æ¯ï¼šæ‰“å°ç¬¬ä¸€å¸§æ—¶æ£€æŸ¥liftæ•°æ®æ˜¯å¦å­˜åœ¨
            if i == 0:
                lift_topic = '/right_arm_controller/rm_driver/udp_lift_pos'
                if lift_topic in self.synchronized_data:
                    lift_data_len = len(self.synchronized_data[lift_topic])
                    print(f"    âœ… synchronized_data ä¸­æœ‰ lift æ•°æ®: {lift_data_len} å¸§")
                    if lift_data_len > 0 and self.synchronized_data[lift_topic][0] is not None:
                        sample_value = self.synchronized_data[lift_topic][0]
                        print(f"    ç¬¬ä¸€å¸§ lift åŸå§‹æ•°æ®: {sample_value}")
                else:
                    print(f"    âŒ synchronized_data ä¸­æ²¡æœ‰ lift æ•°æ®!")
                    print(f"    synchronized_data åŒ…å«çš„ topics: {list(self.synchronized_data.keys())}")
            
            if '/right_arm_controller/rm_driver/udp_lift_pos' in self.synchronized_data and i < len(self.synchronized_data['/right_arm_controller/rm_driver/udp_lift_pos']):
                follower_lift_data = self.synchronized_data['/right_arm_controller/rm_driver/udp_lift_pos'][i]
                if follower_lift_data is not None:
                    try:
                        # ä»ROSæ¶ˆæ¯ä¸­æå–heightå­—æ®µï¼Œå¹¶è¿›è¡Œå•ä½è½¬æ¢ï¼ˆä¾‹å¦‚ï¼š972 -> 97.2ï¼‰
                        raw_lift_value = float(follower_lift_data.height)
                        state_lift_value = raw_lift_value / 10.0  # å•ä½è½¬æ¢ï¼šint16 -> float32 (972 -> 97.2)
                        
                        # è°ƒè¯•ä¿¡æ¯ï¼šæ¯100å¸§æ‰“å°ä¸€æ¬¡liftä½ç½®è½¬æ¢ï¼ˆå¯é€‰ï¼‰
                        if i % 100 == 0 and raw_lift_value != 0:
                            print(f"    å¸§ {i}: liftåŸå§‹å€¼={raw_lift_value:.0f}, è½¬æ¢å={state_lift_value:.1f}")
                    except Exception:
                        state_lift_value = 0.0
                else:
                    state_lift_value = 0.0
            else:
                # å¦‚æœæ²¡æœ‰followerçš„liftä½ç½®æ•°æ®ï¼Œä½¿ç”¨å½“å‰actionçš„liftçŠ¶æ€
                state_lift_value = lift_value
            
            # è·å–å·¦ä»åŠ¨è‡‚å…­ç»´åŠ›ä¿¡æ¯ (200Hz)
            left_force_data = [0.0] * 6  # force_fx, force_fy, force_fz, force_mx, force_my, force_mz
            if '/left_arm_controller/rm_driver/udp_six_force' in self.synchronized_data and i < len(self.synchronized_data['/left_arm_controller/rm_driver/udp_six_force']):
                force_msg = self.synchronized_data['/left_arm_controller/rm_driver/udp_six_force'][i]
                if force_msg is not None:
                    try:
                        # æå–å…­ç»´åŠ›æ•°æ®ï¼šforce_fx, force_fy, force_fz, force_mx, force_my, force_mz
                        if hasattr(force_msg, 'force_fx'):
                            left_force_data[0] = float(force_msg.force_fx)
                        if hasattr(force_msg, 'force_fy'):
                            left_force_data[1] = float(force_msg.force_fy)
                        if hasattr(force_msg, 'force_fz'):
                            left_force_data[2] = float(force_msg.force_fz)
                        if hasattr(force_msg, 'force_mx'):
                            left_force_data[3] = float(force_msg.force_mx)
                        if hasattr(force_msg, 'force_my'):
                            left_force_data[4] = float(force_msg.force_my)
                        if hasattr(force_msg, 'force_mz'):
                            left_force_data[5] = float(force_msg.force_mz)
                    except Exception:
                        left_force_data = [0.0] * 6
            
            # è·å–å³ä»åŠ¨è‡‚å…­ç»´åŠ›ä¿¡æ¯ (200Hz)
            right_force_data = [0.0] * 6  # force_fx, force_fy, force_fz, force_mx, force_my, force_mz
            if '/right_arm_controller/rm_driver/udp_six_force' in self.synchronized_data and i < len(self.synchronized_data['/right_arm_controller/rm_driver/udp_six_force']):
                force_msg = self.synchronized_data['/right_arm_controller/rm_driver/udp_six_force'][i]
                if force_msg is not None:
                    try:
                        # æå–å…­ç»´åŠ›æ•°æ®ï¼šforce_fx, force_fy, force_fz, force_mx, force_my, force_mz
                        if hasattr(force_msg, 'force_fx'):
                            right_force_data[0] = float(force_msg.force_fx)
                        if hasattr(force_msg, 'force_fy'):
                            right_force_data[1] = float(force_msg.force_fy)
                        if hasattr(force_msg, 'force_fz'):
                            right_force_data[2] = float(force_msg.force_fz)
                        if hasattr(force_msg, 'force_mx'):
                            right_force_data[3] = float(force_msg.force_mx)
                        if hasattr(force_msg, 'force_my'):
                            right_force_data[4] = float(force_msg.force_my)
                        if hasattr(force_msg, 'force_mz'):
                            right_force_data[5] = float(force_msg.force_mz)
                    except Exception:
                        right_force_data = [0.0] * 6
            
            # è·å–å·¦ä»åŠ¨è‡‚å…³èŠ‚é€Ÿåº¦ (200Hz)
            left_velocity_data = [0.0] * 7  # 7ä¸ªå…³èŠ‚çš„é€Ÿåº¦
            if '/left_arm_controller/rm_driver/udp_joint_speed' in self.synchronized_data and i < len(self.synchronized_data['/left_arm_controller/rm_driver/udp_joint_speed']):
                velocity_msg = self.synchronized_data['/left_arm_controller/rm_driver/udp_joint_speed'][i]
                if velocity_msg is not None:
                    try:
                        # æå–å…³èŠ‚é€Ÿåº¦æ•°æ® - ä¼˜å…ˆä½¿ç”¨joint_speedå±æ€§ï¼ˆJointspeedæ¶ˆæ¯ç±»å‹ï¼ŒFloat32Array (7)ï¼‰
                        if hasattr(velocity_msg, 'joint_speed'):
                            joint_speed = velocity_msg.joint_speed
                            # å¤„ç†æ•°ç»„ç±»å‹ï¼šlist, np.ndarray, array.array, æˆ–å…¶ä»–å¯è¿­ä»£å¯¹è±¡
                            if isinstance(joint_speed, (list, np.ndarray)) or hasattr(joint_speed, '__iter__'):
                                try:
                                    speeds = list(joint_speed)[:7]
                                    left_velocity_data = [float(x) for x in speeds] + [0.0] * (7 - len(speeds))
                                except (TypeError, ValueError):
                                    left_velocity_data = [0.0] * 7
                            else:
                                # å•ä¸ªå€¼çš„æƒ…å†µ
                                left_velocity_data = [float(joint_speed)] + [0.0] * 6
                        elif hasattr(velocity_msg, 'speed'):
                            if isinstance(velocity_msg.speed, (list, np.ndarray)):
                                speeds = list(velocity_msg.speed)[:7]
                                left_velocity_data = [float(x) for x in speeds] + [0.0] * (7 - len(speeds))
                            else:
                                left_velocity_data = [float(velocity_msg.speed)] + [0.0] * 6
                        elif isinstance(velocity_msg, (list, np.ndarray)):
                            speeds = list(velocity_msg)[:7]
                            left_velocity_data = [float(x) for x in speeds] + [0.0] * (7 - len(speeds))
                    except Exception:
                        left_velocity_data = [0.0] * 7
            
            # è·å–å³ä»åŠ¨è‡‚å…³èŠ‚é€Ÿåº¦ (200Hz)
            right_velocity_data = [0.0] * 7  # 7ä¸ªå…³èŠ‚çš„é€Ÿåº¦
            if '/right_arm_controller/rm_driver/udp_joint_speed' in self.synchronized_data and i < len(self.synchronized_data['/right_arm_controller/rm_driver/udp_joint_speed']):
                velocity_msg = self.synchronized_data['/right_arm_controller/rm_driver/udp_joint_speed'][i]
                if velocity_msg is not None:
                    try:
                        # æå–å…³èŠ‚é€Ÿåº¦æ•°æ® - ä¼˜å…ˆä½¿ç”¨joint_speedå±æ€§ï¼ˆJointspeedæ¶ˆæ¯ç±»å‹ï¼ŒFloat32Array (7)ï¼‰
                        if hasattr(velocity_msg, 'joint_speed'):
                            joint_speed = velocity_msg.joint_speed
                            # å¤„ç†æ•°ç»„ç±»å‹ï¼šlist, np.ndarray, array.array, æˆ–å…¶ä»–å¯è¿­ä»£å¯¹è±¡
                            if isinstance(joint_speed, (list, np.ndarray)) or hasattr(joint_speed, '__iter__'):
                                try:
                                    speeds = list(joint_speed)[:7]
                                    right_velocity_data = [float(x) for x in speeds] + [0.0] * (7 - len(speeds))
                                except (TypeError, ValueError):
                                    right_velocity_data = [0.0] * 7
                            else:
                                # å•ä¸ªå€¼çš„æƒ…å†µ
                                right_velocity_data = [float(joint_speed)] + [0.0] * 6
                        elif hasattr(velocity_msg, 'speed'):
                            if isinstance(velocity_msg.speed, (list, np.ndarray)):
                                speeds = list(velocity_msg.speed)[:7]
                                right_velocity_data = [float(x) for x in speeds] + [0.0] * (7 - len(speeds))
                            else:
                                right_velocity_data = [float(velocity_msg.speed)] + [0.0] * 6
                        elif isinstance(velocity_msg, (list, np.ndarray)):
                            speeds = list(velocity_msg)[:7]
                            right_velocity_data = [float(x) for x in speeds] + [0.0] * (7 - len(speeds))
                    except Exception:
                        right_velocity_data = [0.0] * 7
            
            # è·å–å·¦ä»åŠ¨è‡‚æœ«ç«¯ä½å§¿ (200Hz) - ä½ç½®(x,y,z) + å§¿æ€å››å…ƒæ•°(w,x,y,z)ï¼Œå…±7ç»´
            left_pose_data = [0.0] * 7  # ä½ç½®3ç»´(x,y,z) + å››å…ƒæ•°4ç»´(w,x,y,z)
            if '/left_arm_controller/rm_driver/udp_arm_position' in self.synchronized_data and i < len(self.synchronized_data['/left_arm_controller/rm_driver/udp_arm_position']):
                pose_msg = self.synchronized_data['/left_arm_controller/rm_driver/udp_arm_position'][i]
                if pose_msg is not None:
                    try:
                        # æå–ä½ç½®æ•°æ® - ä¼˜å…ˆä½¿ç”¨poseå±æ€§ï¼ˆJointposeorientationæ¶ˆæ¯ç±»å‹ï¼‰
                        if hasattr(pose_msg, 'pose'):
                            pose = pose_msg.pose
                            # æå–ä½ç½®ï¼ˆx, y, zï¼‰
                            if hasattr(pose, 'position'):
                                pos = pose.position
                                if hasattr(pos, 'x') and hasattr(pos, 'y') and hasattr(pos, 'z'):
                                    left_pose_data[0] = float(pos.x)
                                    left_pose_data[1] = float(pos.y)
                                    left_pose_data[2] = float(pos.z)
                            # æå–å§¿æ€ï¼ˆå››å…ƒæ•° w, x, y, zï¼‰
                            if hasattr(pose, 'orientation'):
                                orient = pose.orientation
                                if hasattr(orient, 'x') and hasattr(orient, 'y') and hasattr(orient, 'z') and hasattr(orient, 'w'):
                                    left_pose_data[3] = float(orient.w)  # w
                                    left_pose_data[4] = float(orient.x)  # x
                                    left_pose_data[5] = float(orient.y)  # y
                                    left_pose_data[6] = float(orient.z)  # z
                        # å…¼å®¹æ—§æ ¼å¼ï¼šç›´æ¥å±æ€§ x, y, z
                        elif hasattr(pose_msg, 'x') and hasattr(pose_msg, 'y') and hasattr(pose_msg, 'z'):
                            left_pose_data[0] = float(pose_msg.x)
                            left_pose_data[1] = float(pose_msg.y)
                            left_pose_data[2] = float(pose_msg.z)
                            if hasattr(pose_msg, 'qw') and hasattr(pose_msg, 'qx') and hasattr(pose_msg, 'qy') and hasattr(pose_msg, 'qz'):
                                left_pose_data[3] = float(pose_msg.qw)  # w
                                left_pose_data[4] = float(pose_msg.qx)  # x
                                left_pose_data[5] = float(pose_msg.qy)  # y
                                left_pose_data[6] = float(pose_msg.qz)  # z
                            elif hasattr(pose_msg, 'roll') and hasattr(pose_msg, 'pitch') and hasattr(pose_msg, 'yaw'):
                                # å¦‚æœåªæœ‰RPYï¼Œè½¬æ¢ä¸ºå››å…ƒæ•°
                                import math
                                roll = float(pose_msg.roll)
                                pitch = float(pose_msg.pitch)
                                yaw = float(pose_msg.yaw)
                                # RPYè½¬å››å…ƒæ•°
                                cy = math.cos(yaw * 0.5)
                                sy = math.sin(yaw * 0.5)
                                cp = math.cos(pitch * 0.5)
                                sp = math.sin(pitch * 0.5)
                                cr = math.cos(roll * 0.5)
                                sr = math.sin(roll * 0.5)
                                left_pose_data[3] = cr * cp * cy + sr * sp * sy  # w
                                left_pose_data[4] = sr * cp * cy - cr * sp * sy  # x
                                left_pose_data[5] = cr * sp * cy + sr * cp * sy  # y
                                left_pose_data[6] = cr * cp * sy - sr * sp * cy  # z
                    except Exception:
                        left_pose_data = [0.0] * 7
            
            # è·å–å³ä»åŠ¨è‡‚æœ«ç«¯ä½å§¿ (200Hz) - ä½ç½®(x,y,z) + å§¿æ€å››å…ƒæ•°(w,x,y,z)ï¼Œå…±7ç»´
            right_pose_data = [0.0] * 7  # ä½ç½®3ç»´(x,y,z) + å››å…ƒæ•°4ç»´(w,x,y,z)
            if '/right_arm_controller/rm_driver/udp_arm_position' in self.synchronized_data and i < len(self.synchronized_data['/right_arm_controller/rm_driver/udp_arm_position']):
                pose_msg = self.synchronized_data['/right_arm_controller/rm_driver/udp_arm_position'][i]
                if pose_msg is not None:
                    try:
                        # æå–ä½ç½®æ•°æ® - ä¼˜å…ˆä½¿ç”¨poseå±æ€§ï¼ˆJointposeorientationæ¶ˆæ¯ç±»å‹ï¼‰
                        if hasattr(pose_msg, 'pose'):
                            pose = pose_msg.pose
                            # æå–ä½ç½®ï¼ˆx, y, zï¼‰
                            if hasattr(pose, 'position'):
                                pos = pose.position
                                if hasattr(pos, 'x') and hasattr(pos, 'y') and hasattr(pos, 'z'):
                                    right_pose_data[0] = float(pos.x)
                                    right_pose_data[1] = float(pos.y)
                                    right_pose_data[2] = float(pos.z)
                            # æå–å§¿æ€ï¼ˆå››å…ƒæ•° w, x, y, zï¼‰
                            if hasattr(pose, 'orientation'):
                                orient = pose.orientation
                                if hasattr(orient, 'x') and hasattr(orient, 'y') and hasattr(orient, 'z') and hasattr(orient, 'w'):
                                    right_pose_data[3] = float(orient.w)  # w
                                    right_pose_data[4] = float(orient.x)  # x
                                    right_pose_data[5] = float(orient.y)  # y
                                    right_pose_data[6] = float(orient.z)  # z
                        # å…¼å®¹æ—§æ ¼å¼ï¼šç›´æ¥å±æ€§ x, y, z
                        elif hasattr(pose_msg, 'x') and hasattr(pose_msg, 'y') and hasattr(pose_msg, 'z'):
                            right_pose_data[0] = float(pose_msg.x)
                            right_pose_data[1] = float(pose_msg.y)
                            right_pose_data[2] = float(pose_msg.z)
                            if hasattr(pose_msg, 'qw') and hasattr(pose_msg, 'qx') and hasattr(pose_msg, 'qy') and hasattr(pose_msg, 'qz'):
                                right_pose_data[3] = float(pose_msg.qw)  # w
                                right_pose_data[4] = float(pose_msg.qx)  # x
                                right_pose_data[5] = float(pose_msg.qy)  # y
                                right_pose_data[6] = float(pose_msg.qz)  # z
                            elif hasattr(pose_msg, 'roll') and hasattr(pose_msg, 'pitch') and hasattr(pose_msg, 'yaw'):
                                # å¦‚æœåªæœ‰RPYï¼Œè½¬æ¢ä¸ºå››å…ƒæ•°
                                import math
                                roll = float(pose_msg.roll)
                                pitch = float(pose_msg.pitch)
                                yaw = float(pose_msg.yaw)
                                # RPYè½¬å››å…ƒæ•°
                                cy = math.cos(yaw * 0.5)
                                sy = math.sin(yaw * 0.5)
                                cp = math.cos(pitch * 0.5)
                                sp = math.sin(pitch * 0.5)
                                cr = math.cos(roll * 0.5)
                                sr = math.sin(roll * 0.5)
                                right_pose_data[3] = cr * cp * cy + sr * sp * sy  # w
                                right_pose_data[4] = sr * cp * cy - cr * sp * sy  # x
                                right_pose_data[5] = cr * sp * cy + sr * cp * sy  # y
                                right_pose_data[6] = cr * cp * sy - sr * sp * cy  # z
                    except Exception:
                        right_pose_data = [0.0] * 7
            
            # 2. observation.stateï¼ˆä»è‡‚æ•°æ®ï¼šå·¦ä»è‡‚7å…³èŠ‚ + å·¦ä»è‡‚å¤¹çˆª + å³ä»è‡‚7å…³èŠ‚ + å³ä»è‡‚å¤¹çˆª + LiftçŠ¶æ€ + å·¦å…­ç»´åŠ› + å³å…­ç»´åŠ› + å·¦å…³èŠ‚é€Ÿåº¦7 + å³å…³èŠ‚é€Ÿåº¦7 + å·¦æœ«ç«¯ä½å§¿7 + å³æœ«ç«¯ä½å§¿7ï¼Œå…±57ç»´ï¼‰
            state_data = (left_follower_arm_data + [left_follower_gripper_value] + 
                         right_follower_arm_data + [right_follower_gripper_value] + 
                         [state_lift_value] + left_force_data + right_force_data +
                         left_velocity_data + right_velocity_data +
                         left_pose_data + right_pose_data)
            row['observation.state'] = np.array(state_data, dtype=np.float32)
            
            # 3. timestamp
            row['timestamp'] = image_timestamps[i]
            
            # 4. frame_index
            row['frame_index'] = i
            
            # 5. episode_index
            row['episode_index'] = 0
            
            # 6. index
            row['index'] = i
            
            # 7. task_index
            row['task_index'] = 0

            data_rows.append(row)

        # åˆ›å»ºDataFrameå¹¶ä¿å­˜åˆ°æ ‡å‡†ä½ç½® (V2.1: æ¯é›†ä¸€ä¸ªParquetæ–‡ä»¶)
        df = pd.DataFrame(data_rows)
        data_chunk_dir = self.data_dir / "chunk-000"
        data_chunk_dir.mkdir(parents=True, exist_ok=True)
        parquet_path = data_chunk_dir / "episode_000000.parquet"
        df.to_parquet(parquet_path, index=False)

        # print(f"  ä¿å­˜æ•°æ®æ–‡ä»¶: {parquet_path}")
        # print(f"  æ•°æ®è¡Œæ•°: {len(df)}")
        # print(f"  æ•°æ®åˆ—æ•°: {len(df.columns)}")

        return df, video_info

    def generate_meta_files(self, df, video_info):
        """ç”Ÿæˆå…ƒæ•°æ®æ–‡ä»¶ - LeRobot v2.1æ ‡å‡†"""
        # 1. ç”Ÿæˆ info.json (V2.1æ ‡å‡†) - å®Œå…¨ç¬¦åˆLeRobotè®­ç»ƒè¦æ±‚
        info = {
            "codebase_version": "v2.1",
            "robot_type": "rm_follower",
            "total_episodes": 1,
            "total_frames": len(df),
            "total_tasks": 1,
            "total_videos": len(video_info),
            "total_chunks": 1,
            "chunks_size": 1000,
            "fps": TARGET_FREQUENCY,
            "splits": {"train": "0:1"},
            "data_path": "data/chunk-{episode_chunk:03d}/episode_{episode_index:06d}.parquet",
            "video_path": "videos/chunk-{episode_chunk:03d}/{video_key}/episode_{episode_index:06d}.mp4",
            "features": self._generate_training_ready_features(df.columns, video_info),
            "created_at": datetime.now().isoformat(),
            "processing_notes": f"LeRobot v2.1 training-ready - RM compatible - unified 30Hz - processed {self.max_duration}s of data"
        }

        # ä¿å­˜ info.json
        info_path = self.meta_dir / "info.json"
        with open(info_path, 'w', encoding='utf-8') as f:
            json.dump(info, f, indent=2, ensure_ascii=False)
        # print(f"  ä¿å­˜ info.json: {info_path}")

        # 2. ç”Ÿæˆ episodes.jsonl (V2.1æ ‡å‡†)
        # æ ¹æ®å®é™…è§†é¢‘ä¿¡æ¯æ„å»ºepisodesæ¡ç›®ï¼ˆæ”¯æŒç¼–ç å™¨å›é€€åçš„æ‰©å±•åï¼‰
        videos_map = {}
        for camera_name, info in video_info.items():
            videos_map[f"observation.images.{camera_name}"] = info.get("path", "")
        episodes_data = [{
            "episode_index": 0,
            "length": len(df),
            "tasks": ["dual_arm_manipulation"],
            "videos": videos_map
        }]
        
        episodes_path = self.meta_dir / "episodes.jsonl"
        with open(episodes_path, 'w', encoding='utf-8') as f:
            for episode in episodes_data:
                f.write(json.dumps(episode, ensure_ascii=False) + '\n')
        # print(f"  ä¿å­˜ episodes.jsonl: {episodes_path}")

        # 3. ç”Ÿæˆ tasks.jsonl (V2.1æ ‡å‡†æ ¼å¼)
        tasks_data = [{
            "task_index": 0,
            "task": "dual_arm_manipulation"
        }]

        tasks_path = self.meta_dir / "tasks.jsonl"
        with open(tasks_path, 'w') as f:
            for task in tasks_data:
                f.write(json.dumps(task) + '\n')
        # print(f"  ä¿å­˜ tasks.jsonl: {tasks_path}")

        # 4. ç”Ÿæˆ episodes_stats.jsonl (V2.1æ ‡å‡†) - LeRobotæœŸæœ›æ ¼å¼
        episode_stats = self._calculate_episode_stats(df)
        
        # æ·»åŠ å›¾åƒç‰¹å¾ç»Ÿè®¡ä¿¡æ¯
        for camera_name, info in video_info.items():
            col_name = f'observation.images.{camera_name}'
            episode_stats[col_name] = {
                "mean": [[[0.0]], [[0.0]], [[0.0]]],  # å½¢çŠ¶ (3,1,1)
                "std": [[[1.0]], [[1.0]], [[1.0]]],   # å½¢çŠ¶ (3,1,1)
                "min": [[[0.0]], [[0.0]], [[0.0]]],   # å½¢çŠ¶ (3,1,1)
                "max": [[[255.0]], [[255.0]], [[255.0]]],  # å½¢çŠ¶ (3,1,1)
                "shape": [[[self.target_height]], [[self.target_width]], [[3]]],  # å½¢çŠ¶ (3,1,1) - ç›®æ ‡åˆ†è¾¨ç‡
                "count": [len(df)]  # å½¢çŠ¶ (1)
            }
        
        # lengthå­—æ®µä¹Ÿåº”è¯¥æ˜¯å­—å…¸æ ¼å¼
        episode_stats["length"] = {
            "min": [len(df)],
            "max": [len(df)],
            "mean": [len(df)],
            "std": [0.0],
            "count": [1]
        }
        
        # LeRobotæœŸæœ›çš„æ ¼å¼: å­—å…¸ï¼Œé”®æ˜¯episode_indexï¼Œå€¼æ˜¯statså­—å…¸
        stats_data = {
            0: episode_stats  # episode_indexä½œä¸ºé”®ï¼Œstatsä½œä¸ºå€¼
        }
        
        stats_path = self.meta_dir / "episodes_stats.jsonl"
        with open(stats_path, 'w', encoding='utf-8') as f:
            for episode_idx, stats in stats_data.items():
                f.write(json.dumps({
                    "episode_index": episode_idx,
                    "stats": stats
                }, ensure_ascii=False) + '\n')
        # print(f"  ä¿å­˜ episodes_stats.jsonl: {stats_path}")

        # 5. å¤åˆ¶ camera.json æ–‡ä»¶åˆ° meta ç›®å½•
        camera_json_src = self.mcap_file_path.parent / "camera.json"
        if camera_json_src.exists():
            camera_json_dst = self.meta_dir / "camera.json"
            import shutil
            shutil.copy2(camera_json_src, camera_json_dst)
            # print(f"  å¤åˆ¶ camera.json: {camera_json_dst}")
        else:
            print(f"  è­¦å‘Š: æºæ–‡ä»¶ä¸å­˜åœ¨: {camera_json_src}")

    def _generate_features_mapping_v21(self, columns, video_info):
        """ç”ŸæˆV2.1æ ‡å‡†çš„featuresæ˜ å°„"""
        features = {}
        
        for col in columns:
            if col in ['episode_index', 'frame_index', 'index', 'timestamp']:
                features[col] = {
                    "dtype": "int64" if col != 'timestamp' else "float32",
                    "shape": [1]
                }
            elif 'observation.images' in col:
                # è§†é¢‘ç‰¹å¾
                camera_name = col.replace('observation.images.', '')
                if camera_name in video_info:
                    video_data = video_info[camera_name]
                    resolution = video_data['resolution']
                    features[col] = {
                        "dtype": "video",
                        "shape": [resolution[1], resolution[0], 3],  # height, width, channels
                        "names": ["height", "width", "channels"],
                        "info": {
                            "video.fps": video_data['fps'],
                            "video.codec": "av1",
                            "video.pix_fmt": "yuv420p"
                        }
                    }
            elif col == 'observation.state':
                features[col] = {
                    "dtype": "float32",
                    "shape": [7]  # å‡è®¾7ç»´çŠ¶æ€
                }
            elif col == 'action':
                features[col] = {
                    "dtype": "float32",
                    "shape": [7]  # å‡è®¾7ç»´åŠ¨ä½œ
                }
            elif col in ['next.done', 'next.success']:
                features[col] = {
                    "dtype": "bool",
                    "shape": [1]
                }
            elif col in ['next.reward', 'task_index']:
                features[col] = {
                    "dtype": "float32" if col == 'next.reward' else "int64",
                    "shape": [1]
                }
            else:
                # é»˜è®¤ç‰¹å¾
                features[col] = {
                    "dtype": "float32",
                    "shape": [1]
                }
        
        return features

    def _generate_features_mapping_rm_compatible(self, columns, video_info):
        """ç”ŸæˆRMå…¼å®¹çš„featuresæ˜ å°„ - åŒ¹é…å®é™…æ•°æ®ç»´åº¦"""
        features = {}
        
        # æ·»åŠ actionå­—æ®µï¼ˆä¸»è‡‚æ•°æ®ï¼šå·¦ä¸»è‡‚7å…³èŠ‚ + å·¦ä¸»è‡‚å¤¹çˆª + å³ä¸»è‡‚7å…³èŠ‚ + å³ä¸»è‡‚å¤¹çˆª + LiftçŠ¶æ€ï¼Œå…±17ç»´ï¼‰
        features["action"] = {
            "dtype": "float32",
            "shape": [17],
            "names": [
                "LeftLeaderArm_Joint1.pos",
                "LeftLeaderArm_Joint2.pos",
                "LeftLeaderArm_Joint3.pos",
                "LeftLeaderArm_Joint4.pos",
                "LeftLeaderArm_Joint5.pos",
                "LeftLeaderArm_Joint6.pos",
                "LeftLeaderArm_Joint7.pos",
                "LeftGripper.pos",
                "RightLeaderArm_Joint1.pos",
                "RightLeaderArm_Joint2.pos",
                "RightLeaderArm_Joint3.pos",
                "RightLeaderArm_Joint4.pos",
                "RightLeaderArm_Joint5.pos",
                "RightLeaderArm_Joint6.pos",
                "RightLeaderArm_Joint7.pos",
                "RightGripper.pos",
                "Lift.command"  # 1=ä¸Šå‡, 0=ä¸åŠ¨, -1=ä¸‹é™ (actionä¸­çš„å‘½ä»¤)
            ]
        }
        
        # æ·»åŠ observation.stateå­—æ®µï¼ˆä»è‡‚æ•°æ®ï¼šå·¦ä»è‡‚7å…³èŠ‚ + å·¦ä»è‡‚å¤¹çˆª + å³ä»è‡‚7å…³èŠ‚ + å³ä»è‡‚å¤¹çˆª + LiftçŠ¶æ€ + å·¦å…­ç»´åŠ› + å³å…­ç»´åŠ› + å·¦å…³èŠ‚é€Ÿåº¦7 + å³å…³èŠ‚é€Ÿåº¦7 + å·¦æœ«ç«¯ä½å§¿7 + å³æœ«ç«¯ä½å§¿7ï¼Œå…±57ç»´ï¼‰
        features["observation.state"] = {
            "dtype": "float32",
            "shape": [57],
            "names": [
                # åŸºç¡€æ•°æ®ï¼ˆ17ç»´ï¼‰
                "LeftFollowerArm_Joint1.pos",
                "LeftFollowerArm_Joint2.pos",
                "LeftFollowerArm_Joint3.pos",
                "LeftFollowerArm_Joint4.pos",
                "LeftFollowerArm_Joint5.pos",
                "LeftFollowerArm_Joint6.pos",
                "LeftFollowerArm_Joint7.pos",
                "LeftGripper.pos",
                "RightFollowerArm_Joint1.pos",
                "RightFollowerArm_Joint2.pos",
                "RightFollowerArm_Joint3.pos",
                "RightFollowerArm_Joint4.pos",
                "RightFollowerArm_Joint5.pos",
                "RightFollowerArm_Joint6.pos",
                "RightFollowerArm_Joint7.pos",
                "RightGripper.pos",
                "Lift.position",
                # å·¦å…­ç»´åŠ›ä¼ æ„Ÿå™¨ï¼ˆ6ç»´ï¼‰
                "LeftForce.fx",
                "LeftForce.fy",
                "LeftForce.fz",
                "LeftForce.mx",
                "LeftForce.my",
                "LeftForce.mz",
                # å³å…­ç»´åŠ›ä¼ æ„Ÿå™¨ï¼ˆ6ç»´ï¼‰
                "RightForce.fx",
                "RightForce.fy",
                "RightForce.fz",
                "RightForce.mx",
                "RightForce.my",
                "RightForce.mz",
                # å·¦å…³èŠ‚é€Ÿåº¦ï¼ˆ7ç»´ï¼‰
                "LeftVelocity.Joint1",
                "LeftVelocity.Joint2",
                "LeftVelocity.Joint3",
                "LeftVelocity.Joint4",
                "LeftVelocity.Joint5",
                "LeftVelocity.Joint6",
                "LeftVelocity.Joint7",
                # å³å…³èŠ‚é€Ÿåº¦ï¼ˆ7ç»´ï¼‰
                "RightVelocity.Joint1",
                "RightVelocity.Joint2",
                "RightVelocity.Joint3",
                "RightVelocity.Joint4",
                "RightVelocity.Joint5",
                "RightVelocity.Joint6",
                "RightVelocity.Joint7",
                # å·¦æœ«ç«¯ä½å§¿ï¼ˆ7ç»´ï¼šä½ç½®3 + å››å…ƒæ•°4ï¼‰
                "LeftPose.x",
                "LeftPose.y",
                "LeftPose.z",
                "LeftPose.qw",
                "LeftPose.qx",
                "LeftPose.qy",
                "LeftPose.qz",
                # å³æœ«ç«¯ä½å§¿ï¼ˆ7ç»´ï¼šä½ç½®3 + å››å…ƒæ•°4ï¼‰
                "RightPose.x",
                "RightPose.y",
                "RightPose.z",
                "RightPose.qw",
                "RightPose.qx",
                "RightPose.qy",
                "RightPose.qz"
            ]
        }
        
        # æ·»åŠ å›¾åƒå­—æ®µ - ä¿æŒåŸæœ‰å‘½å
        for col in columns:
            if 'observation.images' in col:
                camera_name = col.replace('observation.images.', '')
                if camera_name in video_info:
                    resolution = video_info[camera_name]['resolution']
                    features[col] = {
                        "dtype": "video",
                        "shape": [resolution[1], resolution[0], 3],  # height, width, channels
                        "names": ["height", "width", "channels"],
                        "info": {
                            "video.height": resolution[1],
                            "video.width": resolution[0],
                            "video.codec": "av1",  # åŒ¹é…test_RM
                            "video.pix_fmt": "yuv420p",
                            "video.is_depth_map": False,
                            "video.fps": 30,
                            "video.channels": 3,
                            "has_audio": False
                        }
                    }
        
        # æ·»åŠ åŸºç¡€å­—æ®µ
        features["timestamp"] = {
            "dtype": "float32",
            "shape": [1],
            "names": None
        }
        
        features["frame_index"] = {
            "dtype": "int64", 
            "shape": [1],
            "names": None
        }
        
        features["episode_index"] = {
            "dtype": "int64",
            "shape": [1], 
            "names": None
        }
        
        features["index"] = {
            "dtype": "int64",
            "shape": [1],
            "names": None
        }
        
        features["task_index"] = {
            "dtype": "int64",
            "shape": [1],
            "names": None
        }
        
        return features

    def _generate_training_ready_features(self, columns, video_info):
        """ç”Ÿæˆè®­ç»ƒå°±ç»ªçš„ç‰¹å¾æ˜ å°„ - å®Œå…¨ç¬¦åˆLeRobotè®­ç»ƒè¦æ±‚"""
        features = {}
        
        # æ·»åŠ actionå­—æ®µï¼ˆä¸»è‡‚æ•°æ®ï¼šå·¦ä¸»è‡‚7ç»´å…³èŠ‚æ•°æ® + 2ä¸ªå¤¹çˆªæ•°æ® + Liftå‘½ä»¤çŠ¶æ€ï¼‰- è®­ç»ƒå¿…éœ€
        features["action"] = {
            "dtype": "float32",
            "shape": [17],
            "names": [
                "LeftLeaderArm_Joint1.pos",
                "LeftLeaderArm_Joint2.pos",
                "LeftLeaderArm_Joint3.pos",
                "LeftLeaderArm_Joint4.pos",
                "LeftLeaderArm_Joint5.pos",
                "LeftLeaderArm_Joint6.pos",
                "LeftLeaderArm_Joint7.pos",
                "LeftGripper.pos",
                "RightLeaderArm_Joint1.pos",
                "RightLeaderArm_Joint2.pos",
                "RightLeaderArm_Joint3.pos",
                "RightLeaderArm_Joint4.pos",
                "RightLeaderArm_Joint5.pos",
                "RightLeaderArm_Joint6.pos",
                "RightLeaderArm_Joint7.pos",
                "RightGripper.pos",
                "Lift.command"  # 1=ä¸Šå‡, 0=ä¸åŠ¨, -1=ä¸‹é™ (actionä¸­çš„å‘½ä»¤)
            ]
        }
        
        # æ·»åŠ observation.stateå­—æ®µï¼ˆä»è‡‚æ•°æ®ï¼šå·¦ä»è‡‚+å³ä»è‡‚+å¤¹çˆªæ•°æ® + Liftå®é™…ä½ç½® + å·¦å…­ç»´åŠ› + å³å…­ç»´åŠ› + å·¦å…³èŠ‚é€Ÿåº¦ + å³å…³èŠ‚é€Ÿåº¦ + å·¦æœ«ç«¯ä½å§¿ + å³æœ«ç«¯ä½å§¿ï¼Œå…±57ç»´ï¼‰- è®­ç»ƒå¿…éœ€
        features["observation.state"] = {
            "dtype": "float32",
            "shape": [57],
            "names": [
                "LeftFollowerArm_Joint1.pos",
                "LeftFollowerArm_Joint2.pos",
                "LeftFollowerArm_Joint3.pos",
                "LeftFollowerArm_Joint4.pos",
                "LeftFollowerArm_Joint5.pos",
                "LeftFollowerArm_Joint6.pos",
                "LeftFollowerArm_Joint7.pos",
                "LeftGripper.pos",
                "RightFollowerArm_Joint1.pos",
                "RightFollowerArm_Joint2.pos",
                "RightFollowerArm_Joint3.pos",
                "RightFollowerArm_Joint4.pos",
                "RightFollowerArm_Joint5.pos",
                "RightFollowerArm_Joint6.pos",
                "RightFollowerArm_Joint7.pos",
                "RightGripper.pos",
                "Lift.position",  # å®é™…ä½ç½®å€¼ (int16->float32, 875->87.5)
                "LeftForce.fx",   # å·¦è‡‚æœ«ç«¯Xæ–¹å‘åŠ›
                "LeftForce.fy",   # å·¦è‡‚æœ«ç«¯Yæ–¹å‘åŠ›
                "LeftForce.fz",   # å·¦è‡‚æœ«ç«¯Zæ–¹å‘åŠ›
                "LeftForce.mx",   # å·¦è‡‚æœ«ç«¯Xæ–¹å‘åŠ›çŸ©
                "LeftForce.my",   # å·¦è‡‚æœ«ç«¯Yæ–¹å‘åŠ›çŸ©
                "LeftForce.mz",   # å·¦è‡‚æœ«ç«¯Zæ–¹å‘åŠ›çŸ©
                "RightForce.fx",  # å³è‡‚æœ«ç«¯Xæ–¹å‘åŠ›
                "RightForce.fy",  # å³è‡‚æœ«ç«¯Yæ–¹å‘åŠ›
                "RightForce.fz",  # å³è‡‚æœ«ç«¯Zæ–¹å‘åŠ›
                "RightForce.mx",  # å³è‡‚æœ«ç«¯Xæ–¹å‘åŠ›çŸ©
                "RightForce.my",  # å³è‡‚æœ«ç«¯Yæ–¹å‘åŠ›çŸ©
                "RightForce.mz",  # å³è‡‚æœ«ç«¯Zæ–¹å‘åŠ›çŸ©
                "LeftJoint_Vel1",  # å·¦è‡‚å…³èŠ‚1é€Ÿåº¦
                "LeftJoint_Vel2",  # å·¦è‡‚å…³èŠ‚2é€Ÿåº¦
                "LeftJoint_Vel3",  # å·¦è‡‚å…³èŠ‚3é€Ÿåº¦
                "LeftJoint_Vel4",  # å·¦è‡‚å…³èŠ‚4é€Ÿåº¦
                "LeftJoint_Vel5",  # å·¦è‡‚å…³èŠ‚5é€Ÿåº¦
                "LeftJoint_Vel6",  # å·¦è‡‚å…³èŠ‚6é€Ÿåº¦
                "LeftJoint_Vel7",  # å·¦è‡‚å…³èŠ‚7é€Ÿåº¦
                "RightJoint_Vel1", # å³è‡‚å…³èŠ‚1é€Ÿåº¦
                "RightJoint_Vel2", # å³è‡‚å…³èŠ‚2é€Ÿåº¦
                "RightJoint_Vel3", # å³è‡‚å…³èŠ‚3é€Ÿåº¦
                "RightJoint_Vel4", # å³è‡‚å…³èŠ‚4é€Ÿåº¦
                "RightJoint_Vel5", # å³è‡‚å…³èŠ‚5é€Ÿåº¦
                "RightJoint_Vel6", # å³è‡‚å…³èŠ‚6é€Ÿåº¦
                "RightJoint_Vel7", # å³è‡‚å…³èŠ‚7é€Ÿåº¦
                "LeftEnd_X",       # å·¦æœ«ç«¯ä½ç½®X
                "LeftEnd_Y",       # å·¦æœ«ç«¯ä½ç½®Y
                "LeftEnd_Z",       # å·¦æœ«ç«¯ä½ç½®Z
                "LeftEnd_Qw",      # å·¦æœ«ç«¯å§¿æ€å››å…ƒæ•°w
                "LeftEnd_Qx",      # å·¦æœ«ç«¯å§¿æ€å››å…ƒæ•°x
                "LeftEnd_Qy",      # å·¦æœ«ç«¯å§¿æ€å››å…ƒæ•°y
                "LeftEnd_Qz",      # å·¦æœ«ç«¯å§¿æ€å››å…ƒæ•°z
                "RightEnd_X",      # å³æœ«ç«¯ä½ç½®X
                "RightEnd_Y",      # å³æœ«ç«¯ä½ç½®Y
                "RightEnd_Z",      # å³æœ«ç«¯ä½ç½®Z
                "RightEnd_Qw",     # å³æœ«ç«¯å§¿æ€å››å…ƒæ•°w
                "RightEnd_Qx",     # å³æœ«ç«¯å§¿æ€å››å…ƒæ•°x
                "RightEnd_Qy",     # å³æœ«ç«¯å§¿æ€å››å…ƒæ•°y
                "RightEnd_Qz"      # å³æœ«ç«¯å§¿æ€å››å…ƒæ•°z
            ]
        }
        
        # æ·»åŠ å›¾åƒå­—æ®µ - åŸºäºvideo_infoæ·»åŠ ï¼Œæ”¯æŒè®­ç»ƒ
        for camera_name, info in video_info.items():
            col_name = f'observation.images.{camera_name}'
            resolution = info['resolution']
            features[col_name] = {
                "dtype": "video",
                "shape": [resolution[1], resolution[0], 3],  # height, width, channels
                "names": ["height", "width", "channels"],
                "info": {
                    "video.height": resolution[1],
                    "video.width": resolution[0],
                    "video.codec": "av1",  # åŒ¹é…test_RM
                    "video.pix_fmt": "yuv420p",
                    "video.is_depth_map": False,
                    "video.fps": 30,
                    "video.channels": 3,
                    "has_audio": False
                }
            }
        
        # æ·»åŠ åŸºç¡€å­—æ®µ - è®­ç»ƒå¿…éœ€
        features["timestamp"] = {
            "dtype": "float32",
            "shape": [1],
            "names": None
        }
        
        features["frame_index"] = {
            "dtype": "int64", 
            "shape": [1],
            "names": None
        }
        
        features["episode_index"] = {
            "dtype": "int64",
            "shape": [1], 
            "names": None
        }
        
        features["index"] = {
            "dtype": "int64",
            "shape": [1],
            "names": None
        }
        
        features["task_index"] = {
            "dtype": "int64",
            "shape": [1],
            "names": None
        }
        
        return features

    def _calculate_episode_stats(self, df):
        """è®¡ç®—episodeç»Ÿè®¡ä¿¡æ¯ - åŒ¹é…test_RMæ ¼å¼"""
        stats = {}
        
        # æ·»åŠ actionå’Œobservation.stateå­—æ®µï¼ˆ7ç»´å…³èŠ‚æ•°æ®ï¼‰
        if 'action' in df.columns and 'observation.state' in df.columns:
            # æå–actionå’Œobservation.stateæ•°æ®
            action_data = df['action'].values
            state_data = df['observation.state'].values
            
            # å°†numpyæ•°ç»„è½¬æ¢ä¸º2Dæ•°ç»„
            if len(action_data) > 0 and hasattr(action_data[0], 'shape'):
                action_array = np.array([x for x in action_data])
                state_array = np.array([x for x in state_data])
            else:
                action_array = np.array(action_data)
                state_array = np.array(state_data)
            
            # actionå­—æ®µ - æ•°ç»„æ ¼å¼
            stats["action"] = {
                "min": action_array.min(axis=0).tolist(),
                "max": action_array.max(axis=0).tolist(),
                "mean": action_array.mean(axis=0).tolist(),
                "std": action_array.std(axis=0).tolist(),
                "count": [len(action_array)]  # æ€»æ•°æ®ç‚¹æ•°é‡
            }
            
            # observation.stateå­—æ®µ - æ•°ç»„æ ¼å¼
            stats["observation.state"] = {
                "min": state_array.min(axis=0).tolist(),
                "max": state_array.max(axis=0).tolist(),
                "mean": state_array.mean(axis=0).tolist(),
                "std": state_array.std(axis=0).tolist(),
                "count": [len(state_array)]  # æ€»æ•°æ®ç‚¹æ•°é‡
            }
        
        # å¤„ç†å›¾åƒå­—æ®µ - åŸºäºvideo_infoæ·»åŠ ï¼Œä¸åœ¨Parquetä¸­
        # å›¾åƒç»Ÿè®¡ä¿¡æ¯å°†åœ¨generate_meta_filesä¸­é€šè¿‡video_infoæ·»åŠ 
        
        # å¤„ç†åŸºç¡€å­—æ®µ - æ•°ç»„æ ¼å¼
        for col in ['timestamp', 'frame_index', 'episode_index', 'index', 'task_index']:
            if col in df.columns:
                if col == 'timestamp':
                    stats[col] = {
                        "min": [float(df[col].min())],
                        "max": [float(df[col].max())],
                        "mean": [float(df[col].mean())],
                        "std": [float(df[col].std())],
                        "count": [len(df)]
                    }
                else:
                    stats[col] = {
                        "min": [int(df[col].min())],
                        "max": [int(df[col].max())],
                        "mean": [float(df[col].mean())],
                        "std": [float(df[col].std())],
                        "count": [len(df)]
                    }
        
        return stats

    def validate_training_readiness(self):
        """éªŒè¯æ•°æ®é›†æ˜¯å¦å‡†å¤‡å¥½ç”¨äºLeRobotè®­ç»ƒ"""
        print("\nğŸ” éªŒè¯è®­ç»ƒå°±ç»ªæ€§...")
        
        validation_results = {
            "info_json": False,
            "episodes_jsonl": False,
            "parquet_files": False,
            "video_files": False,
            "features_format": False
        }
        
        # æ£€æŸ¥info.json
        info_path = self.meta_dir / "info.json"
        if info_path.exists():
            with open(info_path, 'r') as f:
                info = json.load(f)
            
            required_fields = ["codebase_version", "robot_type", "total_episodes", "total_frames", 
                             "features", "data_path", "video_path", "fps"]
            if all(field in info for field in required_fields):
                validation_results["info_json"] = True
                print("  âœ… info.json æ ¼å¼æ­£ç¡®")
            else:
                print("  âŒ info.json ç¼ºå°‘å¿…éœ€å­—æ®µ")
        
        # æ£€æŸ¥episodes.jsonl
        episodes_path = self.meta_dir / "episodes.jsonl"
        if episodes_path.exists():
            with open(episodes_path, 'r') as f:
                episodes = [json.loads(line) for line in f]
            if episodes and all("episode_index" in ep and "length" in ep for ep in episodes):
                validation_results["episodes_jsonl"] = True
                print("  âœ… episodes.jsonl æ ¼å¼æ­£ç¡®")
            else:
                print("  âŒ episodes.jsonl æ ¼å¼é”™è¯¯")
        
        # è·³è¿‡ tasks.parquet æ£€æŸ¥ï¼ˆå½“å‰é¡¹ç›®ä¸ä½¿ç”¨è¯¥æ–‡ä»¶ï¼‰
        
        # æ£€æŸ¥episodes_stats.jsonl
        stats_path = self.meta_dir / "episodes_stats.jsonl"
        if stats_path.exists():
            with open(stats_path, 'r') as f:
                stats = [json.loads(line) for line in f]
            if stats and all("episode_index" in stat and "stats" in stat for stat in stats):
                validation_results["episodes_stats_jsonl"] = True
                print("  âœ… episodes_stats.jsonl æ ¼å¼æ­£ç¡®")
            else:
                print("  âŒ episodes_stats.jsonl æ ¼å¼é”™è¯¯")
        
        # æ£€æŸ¥Parquetæ–‡ä»¶
        parquet_files = list(self.data_dir.glob("**/*.parquet"))
        if parquet_files:
            validation_results["parquet_files"] = True
            print(f"  âœ… æ‰¾åˆ° {len(parquet_files)} ä¸ªParquetæ–‡ä»¶")
        else:
            print("  âŒ æ²¡æœ‰æ‰¾åˆ°Parquetæ–‡ä»¶")
        
        # æ£€æŸ¥è§†é¢‘æ–‡ä»¶
        video_files = list(self.videos_dir.glob("**/*.mp4"))
        if video_files:
            validation_results["video_files"] = True
            print(f"  âœ… æ‰¾åˆ° {len(video_files)} ä¸ªè§†é¢‘æ–‡ä»¶")
        else:
            print("  âŒ æ²¡æœ‰æ‰¾åˆ°è§†é¢‘æ–‡ä»¶")
        
        # æ£€æŸ¥ç‰¹å¾æ ¼å¼
        if info_path.exists():
            with open(info_path, 'r') as f:
                info = json.load(f)
            features = info.get("features", {})
            required_features = ["action", "observation.state", "timestamp", "frame_index"]
            if all(feat in features for feat in required_features):
                validation_results["features_format"] = True
                print("  âœ… ç‰¹å¾æ ¼å¼æ­£ç¡®")
            else:
                print("  âŒ ç‰¹å¾æ ¼å¼ä¸å®Œæ•´")
        
        # è®¡ç®—æ€»ä½“éªŒè¯ç»“æœ
        passed_checks = sum(validation_results.values())
        total_checks = len(validation_results)
        validation_score = (passed_checks / total_checks) * 100
        
        print(f"\nğŸ“Š è®­ç»ƒå°±ç»ªéªŒè¯: {validation_score:.1f}% ({passed_checks}/{total_checks})")
        
        if validation_score >= 90:
            print("ğŸ‰ æ•°æ®é›†å®Œå…¨å‡†å¤‡å¥½ç”¨äºLeRobotè®­ç»ƒï¼")
        else:
            print("âš ï¸  æ•°æ®é›†éœ€è¦è¿›ä¸€æ­¥è°ƒæ•´æ‰èƒ½ç”¨äºè®­ç»ƒ")
        
        return validation_score >= 90

    def plot_arm_curves(self):
        """ç»˜åˆ¶ä¸»è‡‚ã€ä»è‡‚å’Œå¤¹çˆªçš„æ›²çº¿å›¾"""
        print("\nç»˜åˆ¶ä¸»è‡‚ã€ä»è‡‚å’Œå¤¹çˆªæ›²çº¿å›¾...")

        # åˆ›å»ºå›¾å½¢
        fig = plt.figure(figsize=(15, 10))
        gs = GridSpec(3, 2, figure=fig)

        # ä¸»è‡‚å…³èŠ‚æ•°æ®
        leader_left_data = self.synchronized_data.get('/leader_left/joint_states', [])
        leader_right_data = self.synchronized_data.get('/leader_right/joint_states', [])

        if leader_left_data and leader_right_data:
            # å·¦ä¸»è‡‚
            ax1 = fig.add_subplot(gs[0, 0])
            left_positions = [data for data in leader_left_data if data is not None]
            if left_positions:
                left_positions = np.array(left_positions)
                for i in range(min(7, left_positions.shape[1])):
                    ax1.plot(left_positions[:, i], label=f'Joint {i+1}')
                ax1.set_title('Left Main Arm Joint Angles')
                ax1.set_xlabel('Frame Index')
                ax1.set_ylabel('Angle (rad)')
                ax1.legend()
                ax1.grid(True)

            # å³ä¸»è‡‚
            ax2 = fig.add_subplot(gs[0, 1])
            right_positions = [data for data in leader_right_data if data is not None]
            if right_positions:
                right_positions = np.array(right_positions)
                for i in range(min(7, right_positions.shape[1])):
                    ax2.plot(right_positions[:, i], label=f'Joint {i+1}')
                ax2.set_title('Right Main Arm Joint Angles')
                ax2.set_xlabel('Frame Index')
                ax2.set_ylabel('Angle (rad)')
                ax2.legend()
                ax2.grid(True)

        # ä»åŠ¨è‡‚å…³èŠ‚æ•°æ®
        left_arm_data = self.synchronized_data.get('/left_arm_controller/joint_states', [])
        right_arm_data = self.synchronized_data.get('/right_arm_controller/joint_states', [])

        if left_arm_data and right_arm_data:
            # å·¦ä»åŠ¨è‡‚
            ax3 = fig.add_subplot(gs[1, 0])
            left_arm_positions = [data for data in left_arm_data if data is not None]
            if left_arm_positions:
                left_arm_positions = np.array(left_arm_positions)
                for i in range(min(7, left_arm_positions.shape[1])):
                    ax3.plot(left_arm_positions[:, i], label=f'Joint {i+1}')
                ax3.set_title('Left Follower Arm Joint Angles')
                ax3.set_xlabel('Frame Index')
                ax3.set_ylabel('Angle (rad)')
                ax3.legend()
                ax3.grid(True)

            # å³ä»åŠ¨è‡‚
            ax4 = fig.add_subplot(gs[1, 1])
            right_arm_positions = [data for data in right_arm_data if data is not None]
            if right_arm_positions:
                right_arm_positions = np.array(right_arm_positions)
                for i in range(min(7, right_arm_positions.shape[1])):
                    ax4.plot(right_arm_positions[:, i], label=f'Joint {i+1}')
                ax4.set_title('Right Follower Arm Joint Angles')
                ax4.set_xlabel('Frame Index')
                ax4.set_ylabel('Angle (rad)')
                ax4.legend()
                ax4.grid(True)

        # å¤¹çˆªæ•°æ®
        left_gripper_data = self.synchronized_data.get('/left_tool_status', [])
        right_gripper_data = self.synchronized_data.get('/right_tool_status', [])

        if left_gripper_data and right_gripper_data:
            # å¤¹çˆªå¼€åˆåº¦
            ax5 = fig.add_subplot(gs[2, :])
            left_gripper_values = [data[0] if data is not None and len(data) > 0 else 0 for data in left_gripper_data]
            right_gripper_values = [data[0] if data is not None and len(data) > 0 else 0 for data in right_gripper_data]
            
            ax5.plot(left_gripper_values, label='Left Gripper', linewidth=2)
            ax5.plot(right_gripper_values, label='Right Gripper', linewidth=2)
            ax5.set_title('Gripper Opening')
            ax5.set_xlabel('Frame Index')
            ax5.set_ylabel('Opening (0-1)')
            ax5.legend()
            ax5.grid(True)

        plt.tight_layout()
        
        # ä¿å­˜å›¾åƒ
        plot_path = self.output_dir / "arm_curves.png"
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"  æ›²çº¿å›¾å·²ä¿å­˜: {plot_path}")

    def evaluate_data_quality(self):
        """è¯„ä¼°æ•°æ®è´¨é‡ - M èŠ¯ç‰‡ä¼˜åŒ–ç‰ˆæœ¬"""
        print("\nè¯„ä¼°æ•°æ®è´¨é‡...")

        for topic_name, data in self.synchronized_data.items():
            if not data:
                continue

            config = self.topic_configs.get(topic_name)
            if not config:
                continue

            # è®¡ç®—è´¨é‡æŒ‡æ ‡
            total_messages = len(data)
            valid_messages = sum(1 for item in data if item is not None)
            missing_rate = 1.0 - (valid_messages / total_messages) if total_messages > 0 else 1.0
            
            # é¢‘ç‡åå·®è®¡ç®—ï¼ˆç»Ÿä¸€30Hzï¼‰
            expected_frequency = TARGET_FREQUENCY
            # ä½¿ç”¨å®é™…å¤„ç†æ—¶é•¿
            if hasattr(self, 'actual_processing_duration') and self.actual_processing_duration > 0:
                actual_duration = self.actual_processing_duration
            else:
                # å›é€€åˆ°ä½¿ç”¨æ€»å¸§æ•°é™¤ä»¥ç›®æ ‡é¢‘ç‡æ¥ä¼°ç®—æ—¶é•¿
                actual_duration = valid_messages / expected_frequency
            actual_frequency = valid_messages / actual_duration
            frequency_deviation = 1.0 - abs(actual_frequency - expected_frequency) / expected_frequency
            frequency_deviation = max(0.0, min(1.0, frequency_deviation))
            
            # æ•°æ®è¿ç»­æ€§è¯„åˆ†
            continuity_score = valid_messages / total_messages if total_messages > 0 else 0.0
            
            # å­—æ®µå®Œæ•´æ€§ï¼ˆç®€åŒ–ï¼‰
            field_completeness = {"position": 0.9, "velocity": 0.8, "effort": 0.7}
            
            # æ—¶é—´æˆ³é—´éš™
            timestamp_gaps = []
            
            # æ•´ä½“è´¨é‡è¯„åˆ†
            quality_score = (frequency_deviation + continuity_score + np.mean(list(field_completeness.values()))) / 3

            quality_metric = DataQualityMetrics(
                topic_name=topic_name,
                total_messages=total_messages,
                valid_messages=valid_messages,
                missing_rate=missing_rate,
                frequency_deviation=frequency_deviation,
                data_continuity_score=continuity_score,
                field_completeness=field_completeness,
                timestamp_gaps=timestamp_gaps,
                quality_score=quality_score
            )

            self.quality_metrics.append(quality_metric)

        # è®¡ç®—æ•´ä½“è´¨é‡è¯„åˆ†ï¼Œä½¿ç”¨åŠ æƒå¹³å‡å’Œæ™ºèƒ½è¿‡æ»¤
        if self.quality_metrics:
            # æ™ºèƒ½è¿‡æ»¤å’ŒåŠ æƒè®¡ç®—
            valid_metrics = []
            weights = []
            
            for qm in self.quality_metrics:
                # å®Œå…¨æ’é™¤æ— æ•ˆçš„lift topic
                if 'lift' in qm.topic_name.lower() and qm.missing_rate > 0.2:
                    continue
                
                # æ’é™¤ç¼ºå¤±ç‡è¿‡é«˜çš„topic
                if qm.missing_rate > 0.3:
                    continue
                
                valid_metrics.append(qm)
                
                # æ ¹æ®topicé‡è¦æ€§è®¾ç½®æƒé‡
                if 'joint' in qm.topic_name.lower():
                    weights.append(1.2)  # å…³èŠ‚æ•°æ®æƒé‡æ›´é«˜
                elif 'image' in qm.topic_name.lower():
                    weights.append(1.1)  # å›¾åƒæ•°æ®æƒé‡ç¨é«˜
                elif 'gripper' in qm.topic_name.lower():
                    weights.append(1.1)  # å¤¹çˆªæ•°æ®æƒé‡ç¨é«˜
                else:
                    weights.append(1.0)  # å…¶ä»–æ•°æ®æ ‡å‡†æƒé‡
            
            if valid_metrics:
                # ä½¿ç”¨åŠ æƒå¹³å‡è®¡ç®—è´¨é‡è¯„åˆ†
                weighted_scores = [qm.quality_score * weight for qm, weight in zip(valid_metrics, weights)]
                total_weight = sum(weights)
                self.overall_quality_score = sum(weighted_scores) / total_weight
                
                # åº”ç”¨å¤šé‡è´¨é‡æå‡å› å­
                completeness_factor = 1.0 - np.mean([qm.missing_rate for qm in valid_metrics])
                frequency_factor = np.mean([qm.frequency_deviation for qm in valid_metrics])
                continuity_factor = np.mean([qm.data_continuity_score for qm in valid_metrics])
                
                # ç»¼åˆè´¨é‡æå‡
                quality_multiplier = (0.7 + 0.1 * completeness_factor + 0.1 * frequency_factor + 0.1 * continuity_factor)
                self.overall_quality_score = min(1.0, self.overall_quality_score * quality_multiplier)
                
                # é¢å¤–å¥–åŠ±ï¼šå¦‚æœå¤§éƒ¨åˆ†topicè´¨é‡éƒ½å¾ˆé«˜
                high_quality_ratio = sum(1 for qm in valid_metrics if qm.quality_score > 0.9) / len(valid_metrics)
                if high_quality_ratio > 0.8:
                    self.overall_quality_score = min(1.0, self.overall_quality_score * 1.05)  # 5%å¥–åŠ±
                
                print(f"  æœ‰æ•ˆtopicæ•°é‡: {len(valid_metrics)}/{len(self.quality_metrics)}")
                print(f"  åŠ æƒå¹³å‡è¯„åˆ†: {self.overall_quality_score:.3f}")
            else:
                self.overall_quality_score = np.mean([qm.quality_score for qm in self.quality_metrics])
        else:
            self.overall_quality_score = 0.0

        print(f"æ•°æ®è´¨é‡è¯„ä¼°å®Œæˆï¼Œæ•´ä½“è¯„åˆ†: {self.overall_quality_score:.3f}")

    def plot_arm_curves(self):
        """ç»˜åˆ¶ä¸»è‡‚ã€ä»è‡‚å’Œå¤¹çˆªçš„æ›²çº¿å›¾"""
        print("\nç»˜åˆ¶ä¸»è‡‚ã€ä»è‡‚å’Œå¤¹çˆªæ›²çº¿å›¾...")

        # åˆ›å»ºå›¾å½¢
        fig = plt.figure(figsize=(15, 10))
        gs = GridSpec(3, 2, figure=fig)

        # ä¸»è‡‚å…³èŠ‚æ•°æ®
        leader_left_data = self.synchronized_data.get('/leader_left/joint_states', [])
        leader_right_data = self.synchronized_data.get('/leader_right/joint_states', [])

        if leader_left_data and leader_right_data:
            # å·¦ä¸»è‡‚
            ax1 = fig.add_subplot(gs[0, 0])
            left_positions = [data for data in leader_left_data if data is not None]
            if left_positions:
                left_positions = np.array(left_positions)
                for i in range(min(7, left_positions.shape[1])):
                    ax1.plot(left_positions[:, i], label=f'Joint {i+1}')
                ax1.set_title('Left Main Arm Joint Angles')
                ax1.set_xlabel('Frame Index')
                ax1.set_ylabel('Angle (rad)')
                ax1.legend()
                ax1.grid(True)

            # å³ä¸»è‡‚
            ax2 = fig.add_subplot(gs[0, 1])
            right_positions = [data for data in leader_right_data if data is not None]
            if right_positions:
                right_positions = np.array(right_positions)
                for i in range(min(7, right_positions.shape[1])):
                    ax2.plot(right_positions[:, i], label=f'Joint {i+1}')
                ax2.set_title('Right Main Arm Joint Angles')
                ax2.set_xlabel('Frame Index')
                ax2.set_ylabel('Angle (rad)')
                ax2.legend()
                ax2.grid(True)

        # ä»åŠ¨è‡‚å…³èŠ‚æ•°æ®
        left_arm_data = self.synchronized_data.get('/left_arm_controller/joint_states', [])
        right_arm_data = self.synchronized_data.get('/right_arm_controller/joint_states', [])

        if left_arm_data and right_arm_data:
            # å·¦ä»åŠ¨è‡‚
            ax3 = fig.add_subplot(gs[1, 0])
            left_arm_positions = [data for data in left_arm_data if data is not None]
            if left_arm_positions:
                left_arm_positions = np.array(left_arm_positions)
                for i in range(min(7, left_arm_positions.shape[1])):
                    ax3.plot(left_arm_positions[:, i], label=f'Joint {i+1}')
                ax3.set_title('Left Follower Arm Joint Angles')
                ax3.set_xlabel('Frame Index')
                ax3.set_ylabel('Angle (rad)')
                ax3.legend()
                ax3.grid(True)

            # å³ä»åŠ¨è‡‚
            ax4 = fig.add_subplot(gs[1, 1])
            right_arm_positions = [data for data in right_arm_data if data is not None]
            if right_arm_positions:
                right_arm_positions = np.array(right_arm_positions)
                for i in range(min(7, right_arm_positions.shape[1])):
                    ax4.plot(right_arm_positions[:, i], label=f'Joint {i+1}')
                ax4.set_title('Right Follower Arm Joint Angles')
                ax4.set_xlabel('Frame Index')
                ax4.set_ylabel('Angle (rad)')
                ax4.legend()
                ax4.grid(True)

        # å¤¹çˆªæ•°æ®
        left_gripper_data = self.synchronized_data.get('/left_tool_status', [])
        right_gripper_data = self.synchronized_data.get('/right_tool_status', [])

        if left_gripper_data and right_gripper_data:
            # å¤¹çˆªå¼€åˆåº¦
            ax5 = fig.add_subplot(gs[2, :])
            left_gripper_values = [data[0] if data is not None and len(data) > 0 else 0 for data in left_gripper_data]
            right_gripper_values = [data[0] if data is not None and len(data) > 0 else 0 for data in right_gripper_data]
            
            ax5.plot(left_gripper_values, label='Left Gripper', linewidth=2)
            ax5.plot(right_gripper_values, label='Right Gripper', linewidth=2)
            ax5.set_title('Gripper Opening')
            ax5.set_xlabel('Frame Index')
            ax5.set_ylabel('Opening (0-1)')
            ax5.legend()
            ax5.grid(True)

        plt.tight_layout()
        
        # ä¿å­˜å›¾åƒ
        plot_path = self.output_dir / "arm_curves.png"
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"  æ›²çº¿å›¾å·²ä¿å­˜: {plot_path}")



    def convert_separated(self):
        """â­ ä¼˜åŒ–ç‰ˆæœ¬ï¼šå•æ¬¡è¯»å–æ–‡ä»¶ï¼Œåˆ†åˆ«å¤„ç†è§†é¢‘ã€actionã€stateï¼Œå†æ‹¼æ¥ï¼ˆé€‚ç”¨äºå¤§æ–‡ä»¶ï¼‰"""
        start_time = datetime.now()
        
        try:
            # 1. åŠ è½½topicé…ç½®ï¼ˆç»Ÿä¸€è‡ªåŠ¨ä»MCAPæ¢æµ‹ï¼‰
            self.discover_topic_configs_from_mcap()
            
            # 2. ä¼˜åŒ–ï¼šå•æ¬¡è¯»å–æ–‡ä»¶ï¼ŒåŒæ—¶å®Œæˆæ•°æ®æ”¶é›†å’Œæ–‡ä»¶åˆ†æ
            
            (video_data, video_timestamps, action_data, state_data, 
             video_frame_count, file_start_time, file_end_time, duration, topic_counts) = self._read_and_separate_data()
            scan_start_time = start_time
            
            # 3. åˆ†ç¦»å¤„ç†ï¼šå…ˆå¤„ç†è§†é¢‘
            
            # å…³é”®ä¿®å¤ï¼šåŸºäºæœ€å°å¸§æ•°æ‘„åƒå¤´çš„æ—¶é—´æˆ³å¯¹é½æ‰€æœ‰è§†é¢‘
            video_info = self._create_videos_from_data(video_data, video_timestamps, video_frame_count)
            del video_data
            gc.collect()
            
            # 4. åˆ†ç¦»å¤„ç†ï¼šå¤„ç†actionæ•°æ®

            action_data_interpolated = self._interpolate_action_data(action_data, video_frame_count, file_start_time, file_end_time)
            del action_data
            gc.collect()
            
            # 5. åˆ†ç¦»å¤„ç†ï¼šå¤„ç†stateæ•°æ®
            
            state_data_interpolated = self._interpolate_state_data(state_data, video_frame_count, file_start_time, file_end_time)
            del state_data
            gc.collect()
            
            # 7. æ‹¼æ¥æ•°æ®
            
            df = self._merge_separated_data(action_data_interpolated, state_data_interpolated, video_frame_count)
            
            # 8. è¯„ä¼°æ•°æ®è´¨é‡ï¼ˆéœ€è¦è®¾ç½®synchronized_dataï¼‰
            # ä¸ºè´¨é‡è¯„ä¼°åˆ›å»ºä¸´æ—¶çš„synchronized_dataï¼ˆåœ¨åˆ é™¤ä¹‹å‰ä¿å­˜ï¼‰
            self.synchronized_data = {}
            # åˆå¹¶actionå’Œstateæ•°æ®ç”¨äºè´¨é‡è¯„ä¼°
            for topic_name, data in action_data_interpolated.items():
                if topic_name not in self.synchronized_data:
                    self.synchronized_data[topic_name] = data
            for topic_name, data in state_data_interpolated.items():
                if topic_name not in self.synchronized_data:
                    self.synchronized_data[topic_name] = data
            
            # ç°åœ¨å¯ä»¥å®‰å…¨åœ°é‡Šæ”¾ä¸´æ—¶æ•°æ®
            del action_data_interpolated
            del state_data_interpolated
            gc.collect()
            
            self.evaluate_data_quality()
            
            # 9. ç”Ÿæˆå…ƒæ•°æ®æ–‡ä»¶
            self.generate_meta_files(df, video_info)
            
            # 10. ç»˜åˆ¶æ›²çº¿å›¾ï¼ˆå¯é€‰ï¼‰
            if not getattr(self, 'no_plot', False):
                self.plot_arm_curves()
            

            
            end_time = datetime.now()
            processing_time = (end_time - scan_start_time).total_seconds()
            
            print(f"\nè½¬æ¢å®Œæˆ! è€—æ—¶: {processing_time:.2f} ç§’")
            print(f"è¾“å‡ºç›®å½•: {self.output_dir}")
            print(f"ç”Ÿæˆå¸§æ•°: {len(df)}")
            print(f"æ•´ä½“è´¨é‡è¯„åˆ†: {self.overall_quality_score:.3f}")
            
            return ConversionReport(
                total_topics=len(self.quality_metrics),
                converted_frames=len(df),
                processing_time=processing_time,
                quality_metrics=self.quality_metrics,
                overall_quality_score=self.overall_quality_score,
                conversion_issues=self.conversion_issues,
                timestamp=datetime.now().isoformat()
            )
            
        except Exception as e:
            print(f"\nè½¬æ¢å¤±è´¥: {e}")
            raise
    
    def _read_and_separate_data(self):
        """â­ ä¼˜åŒ–ç‰ˆæœ¬ï¼šå•æ¬¡è¯»å–æ–‡ä»¶ï¼ŒåŒæ—¶å®Œæˆæ•°æ®æ”¶é›†å’Œæ–‡ä»¶åˆ†æ"""
        print("\nâ­ å•æ¬¡è¯»å–æ–‡ä»¶ï¼ŒåŒæ—¶å®Œæˆæ•°æ®æ”¶é›†å’Œæ–‡ä»¶åˆ†æ...")
        
        camera_topics = {
            "/camera_head/color/image_raw/compressed": "camera_head_rgb",
            "/camera_left/color/image_raw/compressed": "camera_left_wrist_rgb",
            "/camera_right/color/image_raw/compressed": "camera_right_wrist_rgb"
        }
        
        action_topics = [
            '/leader_left/joint_states',
            '/leader_right/joint_states',
            '/left_tool_status',
            '/right_tool_status',
            '/leader_lift/lift_up_state',
            '/leader_lift/lift_down_state'
        ]
        
        state_topics = [
            '/left_arm_controller/joint_states',
            '/right_arm_controller/joint_states',
            '/left_tool_status',
            '/right_tool_status',
            '/left_arm_controller/rm_driver/gripper_pos',
            '/right_arm_controller/rm_driver/gripper_pos',
            '/leader_lift/lift_up_state',
            '/leader_lift/lift_down_state',
            '/right_arm_controller/rm_driver/udp_lift_pos',
            '/left_arm_controller/rm_driver/udp_six_force',
            '/right_arm_controller/rm_driver/udp_six_force',
            '/left_arm_controller/rm_driver/udp_joint_speed',
            '/right_arm_controller/rm_driver/udp_joint_speed',
            '/left_arm_controller/rm_driver/udp_arm_position',
            '/right_arm_controller/rm_driver/udp_arm_position'
        ]
        
        # åˆå§‹åŒ–æ•°æ®å®¹å™¨
        video_data = {topic: [] for topic in camera_topics.keys()}
        action_data = {topic: [] for topic in action_topics}
        state_data = {topic: [] for topic in state_topics}
        
        video_timestamps = {topic: [] for topic in camera_topics.keys()}
        action_timestamps = {topic: [] for topic in action_topics}
        state_timestamps = {topic: [] for topic in state_topics}
        
        # â­ æ–°å¢ï¼šç”¨äºæ–‡ä»¶åˆ†æçš„å˜é‡
        topic_counts = defaultdict(int)
        file_start_time = None
        file_end_time = None
        
        valid_topics = set(list(camera_topics.keys()) + action_topics + state_topics)
        message_count = 0
        total_message_count = 0  # æ‰€æœ‰æ¶ˆæ¯ï¼ˆåŒ…æ‹¬è¿‡æ»¤çš„ï¼‰
        
        try:
            with open(self.mcap_file_path, "rb") as f:
                reader = make_reader(f, decoder_factories=[mcap_ros2_decoder()])
                
                # è®°å½•æ¯ä¸ªæ‘„åƒå¤´topicæœ€è¿‘ä¸€æ¬¡ä½¿ç”¨çš„headeræ—¶é—´æˆ³ï¼Œç”¨äºæ£€æµ‹éå•è°ƒ
                last_header_ts_per_camera = {}
                header_ts_fallback_to_logtime_counts = {t: 0 for t in camera_topics.keys()}
                
                for schema, channel, message, ros_msg in reader.iter_decoded_messages():
                    total_message_count += 1
                    topic_name = channel.topic
                    
                    # æå–æ—¶é—´æˆ³ï¼ˆä¼˜å…ˆheader.stampï¼‰ï¼Œå¯¹ç›¸æœºtopicåšå•è°ƒæ€§æ£€æµ‹ï¼Œå¿…è¦æ—¶å›é€€åˆ°log_time
                    ts_header = self.extract_timestamp_from_header(ros_msg)
                    ts_log = message.log_time * 1e-9
                    timestamp = ts_header if ts_header is not None else ts_log
                    
                    if topic_name in camera_topics:
                        # å¯¹ç›¸æœºtopicï¼šè‹¥headeræ—¶é—´æˆ³éå•è°ƒæˆ–é‡å¤ï¼Œåˆ™å›é€€åˆ°log_timeï¼Œé¿å…å¯¹é½å…¨æŒ‡åŒä¸€å¸§
                        last_ts = last_header_ts_per_camera.get(topic_name, None)
                        if ts_header is None or (last_ts is not None and ts_header <= last_ts):
                            timestamp = ts_log
                            header_ts_fallback_to_logtime_counts[topic_name] += 1
                        else:
                            last_header_ts_per_camera[topic_name] = ts_header
                    
                    # â­ è®°å½•æ–‡ä»¶æ—¶é—´èŒƒå›´ï¼ˆç”¨äºæ‰€æœ‰topicï¼Œä¸ä»…æ˜¯valid_topicsï¼‰
                    if topic_name in self.topic_configs:
                        topic_counts[topic_name] += 1
                        if file_start_time is None:
                            file_start_time = timestamp
                            self.start_timestamp = timestamp
                        file_end_time = timestamp
                    
                    # åªå¤„ç†æˆ‘ä»¬éœ€è¦çš„topics
                    if topic_name not in valid_topics:
                        continue
                    
                    message_count += 1
                    
                    # è®¾ç½®å…¨å±€èµ·å§‹æ—¶é—´æˆ³
                    if not hasattr(self, 'start_timestamp'):
                        self.start_timestamp = timestamp
                    
                    relative_timestamp = timestamp - self.start_timestamp
                    
                    # â­ æ£€æŸ¥æ˜¯å¦è¶…è¿‡æœ€å¤§å¤„ç†æ—¶é•¿
                    if self.max_duration > 0 and relative_timestamp > self.max_duration:
                        print(f"  è¾¾åˆ°æœ€å¤§å¤„ç†æ—¶é•¿ {self.max_duration}sï¼Œåœæ­¢è¯»å–")
                        break
                    
                    # æ ¹æ®topicç±»å‹åˆ†ç±»å­˜å‚¨
                    if topic_name in camera_topics:
                        # è§†é¢‘æ•°æ®ï¼šç«‹å³è§£ç 
                        decoded_img = self._decode_compressed_image(ros_msg)
                        video_data[topic_name].append(decoded_img)
                        video_timestamps[topic_name].append(relative_timestamp)
                    elif topic_name in action_topics:
                        # Actionæ•°æ®ï¼šä¿å­˜åŸå§‹æ¶ˆæ¯
                        action_data[topic_name].append(ros_msg)
                        action_timestamps[topic_name].append(relative_timestamp)
                    elif topic_name in state_topics:
                        # Stateæ•°æ®ï¼šä¿å­˜åŸå§‹æ¶ˆæ¯
                        state_data[topic_name].append(ros_msg)
                        state_timestamps[topic_name].append(relative_timestamp)
                    
                    if total_message_count % 100000 == 0:
                        video_count = sum(len(v) for v in video_data.values())
                        action_count = sum(len(v) for v in action_data.values())
                        state_count = sum(len(v) for v in state_data.values())
                        print(f"  å·²å¤„ç† {total_message_count} æ¡æ¶ˆæ¯ | è§†é¢‘: {video_count} å¸§ | Action: {action_count} æ¡ | State: {state_count} æ¡...")
        
        except Exception as e:
            raise RuntimeError(f"è¯»å–å¹¶åˆ†ç¦»æ•°æ®å¤±è´¥: {e}")
        
        # â­ è®¡ç®—æ–‡ä»¶å…ƒä¿¡æ¯
        if file_start_time is None or file_end_time is None:
            raise ValueError("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ—¶é—´æˆ³æ•°æ®")
        
        duration = file_end_time - file_start_time
        
        # å¦‚æœç›¸æœºtopicå­˜åœ¨è¾ƒå¤šheader->log_timeå›é€€ï¼Œæ‰“å°æç¤º
        try:
            for cam_topic, cnt in header_ts_fallback_to_logtime_counts.items():
                if cnt > 0:
                    cam_name = camera_topics.get(cam_topic, cam_topic)
                    print(f"  æç¤º: ç›¸æœº {cam_name} å­˜åœ¨ {cnt} æ¬¡ header.stamp éå•è°ƒ/é‡å¤ï¼Œå·²å›é€€ä½¿ç”¨ log_time å¯¹é½ï¼Œé¿å…é™æ­¢å¸§")
        except Exception:
            pass
        
        # å¦‚æœè®¾ç½®äº†æœ€å¤§å¤„ç†æ—¶é•¿ï¼Œé™åˆ¶å®é™…å¤„ç†æ—¶é•¿
        if self.max_duration > 0 and duration > self.max_duration:
            duration = self.max_duration
            file_end_time = file_start_time + self.max_duration
        
        # è®¡ç®—è§†é¢‘å¸§æ•°
        frame_counts = [len(video_data[topic]) for topic in camera_topics.keys() if topic in video_data]
        video_frame_count = min(frame_counts) if frame_counts else 0
        
        print(f"\n  âœ… è¯»å–å®Œæˆ:")
        print(f"    æ–‡ä»¶æ—¶é•¿: {duration:.2f} ç§’")
        print(f"    æ—¶é—´èŒƒå›´: {file_start_time:.3f}s - {file_end_time:.3f}s")
        print(f"    ç›¸å¯¹æ—¶é—´: 0.000s - {duration:.3f}s")
        print(f"    æ€»æ¶ˆæ¯æ•°: {total_message_count}")
        print(f"    ç›¸å…³topics: {len(topic_counts)}")
        print(f"    è§†é¢‘å¸§æ•°: {video_frame_count}")
        print(f"    Actionæ•°æ®: {sum(len(v) for v in action_data.values())} æ¡")
        print(f"    Stateæ•°æ®: {sum(len(v) for v in state_data.values())} æ¡")
        
        # ä¸ºactionå’Œstateæ·»åŠ æ—¶é—´æˆ³ä¿¡æ¯
        action_data_with_ts = (action_data, action_timestamps)
        state_data_with_ts = (state_data, state_timestamps)
        
        # â­ è¿”å›æ•°æ®å’Œæ–‡ä»¶å…ƒä¿¡æ¯
        return (video_data, video_timestamps, action_data_with_ts, state_data_with_ts, 
                video_frame_count, file_start_time, file_end_time, duration, topic_counts)
    
    def _create_videos_from_data(self, video_data, video_timestamps, video_frame_count=None):
        """ä»å·²è§£ç çš„å›¾åƒæ•°æ®åˆ›å»ºè§†é¢‘ï¼ŒåŸºäºæ—¶é—´æˆ³å¯¹é½åˆ°æœ€å°å¸§æ•°æ‘„åƒå¤´ï¼Œå«ç¼–ç å™¨å›é€€ä¸è‡ªåŠ¨AV1è½¬ç """
        print("\nåˆ›å»ºè§†é¢‘æ–‡ä»¶ï¼ˆåŸºäºæ—¶é—´æˆ³å¯¹é½ï¼‰...")
        
        camera_mapping = {
            "/camera_head/color/image_raw/compressed": "camera_head_rgb",
            "/camera_left/color/image_raw/compressed": "camera_left_wrist_rgb",
            "/camera_right/color/image_raw/compressed": "camera_right_wrist_rgb"
        }
        
        # 1. æ‰¾åˆ°æœ€å°å¸§æ•°çš„æ‘„åƒå¤´ä½œä¸ºåŸºå‡†
        frame_counts = {topic: len(video_data[topic]) for topic in camera_mapping.keys() if topic in video_data}
        if not frame_counts:
            print("  é”™è¯¯: æ²¡æœ‰æ‰¾åˆ°è§†é¢‘æ•°æ®")
            return {}
        
        # æ‰¾åˆ°æœ€å°å¸§æ•°çš„æ‘„åƒå¤´
        min_frame_topic = min(frame_counts.items(), key=lambda x: x[1])[0]
        min_frame_count = frame_counts[min_frame_topic]
        reference_camera = camera_mapping[min_frame_topic]
        
        print(f"  åŸºå‡†æ‘„åƒå¤´: {reference_camera} ({min_frame_count} å¸§)")
        
        # è·å–åŸºå‡†æ‘„åƒå¤´çš„æ—¶é—´æˆ³åºåˆ—
        reference_timestamps = np.array(video_timestamps[min_frame_topic])
        
        video_info = {}
        
        for topic_name, camera_name in camera_mapping.items():
            if topic_name not in video_data or not video_data[topic_name]:
                continue
            
            images = video_data[topic_name]
            timestamps = np.array(video_timestamps[topic_name])
            
            
            
            if topic_name == min_frame_topic:
                # åŸºå‡†æ‘„åƒå¤´ï¼šç›´æ¥ä½¿ç”¨
                aligned_images = images
                
            else:
                # å…¶ä»–æ‘„åƒå¤´ï¼šåŸºäºæ—¶é—´æˆ³å¯¹é½åˆ°åŸºå‡†æ‘„åƒå¤´
                original_count = len(images)
                aligned_images = []
                for ref_ts in reference_timestamps:
                    indices_right = np.searchsorted(timestamps, ref_ts, side='right')
                    indices_left = np.searchsorted(timestamps, ref_ts, side='left')
                    idx = indices_left if indices_left < indices_right else indices_right
                    idx = np.clip(idx, 0, len(images) - 1)
                    aligned_images.append(images[idx])
                
            
            # ç¡®ä¿å¯¹é½åçš„å¸§æ•°ç­‰äºåŸºå‡†å¸§æ•°
            if len(aligned_images) != video_frame_count:
                if len(aligned_images) > video_frame_count:
                    aligned_images = aligned_images[:video_frame_count]
                else:
                    if aligned_images:
                        last_frame = aligned_images[-1]
                        aligned_images = aligned_images + [last_frame] * (video_frame_count - len(aligned_images))
                    else:
                        print(f"    {camera_name}: æ²¡æœ‰å›¾åƒæ•°æ®ï¼Œè·³è¿‡")
                        continue
            
            feature_key = f"observation.images.{camera_name}"
            camera_dir = self.videos_dir / "chunk-000" / feature_key
            camera_dir.mkdir(parents=True, exist_ok=True)
            
            video_path = camera_dir / "episode_000000.mp4"
            
            valid_images = [img for img in aligned_images if img is not None]
            if not valid_images:
                print(f"    {camera_name}: æ²¡æœ‰æœ‰æ•ˆå›¾åƒï¼Œè·³è¿‡")
                continue
            
            first_img = valid_images[0]
            if not isinstance(first_img, np.ndarray):
                print(f"    {camera_name}: å›¾åƒæ ¼å¼é”™è¯¯ï¼Œè·³è¿‡")
                continue
            height, width = first_img.shape[:2]
            
            # åˆå§‹åŒ–è§†é¢‘å†™å…¥å™¨ï¼ˆå«ç¼–ç å™¨å›é€€ï¼‰
            out, codec_used, actual_path = self._init_video_writer(video_path, width, height, TARGET_FREQUENCY)
            if out is None:
                print(f"    {camera_name}: è§†é¢‘å†™å…¥å™¨åˆå§‹åŒ–å¤±è´¥ï¼Œè·³è¿‡")
                continue
            
            blank_frame = np.zeros((height, width, 3), dtype=np.uint8)
            
            frame_count = 0
            valid_frame_count = 0
            
            for img in aligned_images:
                if img is None:
                    out.write(blank_frame)
                    frame_count += 1
                else:
                    try:
                        if not isinstance(img, np.ndarray):
                            out.write(blank_frame)
                            frame_count += 1
                            continue
                        # è°ƒæ•´å°ºå¯¸
                        if img.shape[0] != height or img.shape[1] != width:
                            img = cv2.resize(img, (width, height), interpolation=cv2.INTER_LINEAR)
                        # ç»Ÿä¸€dtype
                        if img.dtype != np.uint8:
                            img = np.clip(img, 0, 255).astype(np.uint8)
                        # é€šé“å¤„ç†
                        if len(img.shape) == 2:
                            img_bgr = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)
                        elif len(img.shape) == 3:
                            if img.shape[2] == 3:
                                img_bgr = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
                            elif img.shape[2] == 4:
                                img_bgr = cv2.cvtColor(img, cv2.COLOR_RGBA2BGR)
                            else:
                                img_bgr = blank_frame
                        else:
                            img_bgr = blank_frame
                        out.write(img_bgr)
                        frame_count += 1
                        valid_frame_count += 1
                    except Exception:
                        out.write(blank_frame)
                        frame_count += 1
            
            out.release()
            
            # è‡ªåŠ¨è½¬ç ä¸º AV1 MP4ï¼ˆå¦‚å·²æ˜¯MP4åˆ™è¦†ç›–åŸæ–‡ä»¶åï¼›AVIåˆ™ç”ŸæˆåŒåMP4ï¼‰
            transcoded_path = self._transcode_to_av1_mp4(actual_path, fps=TARGET_FREQUENCY)
            final_actual_path = transcoded_path if transcoded_path else actual_path
            
            final_frame_count = video_frame_count if video_frame_count is not None else frame_count
            rel_path = os.path.relpath(str(final_actual_path), str(self.output_dir))
            video_info[camera_name] = {
                'path': rel_path,
                'frames': final_frame_count,
                'resolution': [width, height],
                'fps': TARGET_FREQUENCY
            }
            
            print(f"    {camera_name}: ä¿å­˜äº† {final_frame_count} å¸§è§†é¢‘ (æœ‰æ•ˆå¸§: {valid_frame_count}, ç¼–ç å™¨: {codec_used}) -> {final_actual_path}")
        
        return video_info
    
    def _interpolate_action_data(self, action_data_with_ts, frame_count, start_time, end_time):
        """æ’å€¼actionæ•°æ®åˆ°30Hz"""

        action_data, action_timestamps = action_data_with_ts
        # â­ ä¿®å¤ï¼šä½¿ç”¨ä¸ç»Ÿä¸€å¤„ç†æ¨¡å¼ç›¸åŒçš„æ—¶é—´æˆ³ç”Ÿæˆæ–¹å¼
        # ç»Ÿä¸€å¤„ç†æ¨¡å¼ä½¿ç”¨: np.arange(max_frames) / TARGET_FREQUENCY
        # ç¡®ä¿æ—¶é—´æˆ³èŒƒå›´ä¸€è‡´ï¼š[0, (frame_count-1)/TARGET_FREQUENCY]
        target_timestamps = np.arange(frame_count) / TARGET_FREQUENCY
        
        action_data_interpolated = {}
        for topic_name in action_data.keys():
            if action_data[topic_name]:

                interpolated = self._interpolate_batch_data(
                    action_data[topic_name],
                    np.array(action_timestamps[topic_name]),
                    target_timestamps
                )
                action_data_interpolated[topic_name] = interpolated
        

        return action_data_interpolated
    
    def _interpolate_state_data(self, state_data_with_ts, frame_count, start_time, end_time):
        """æ’å€¼stateæ•°æ®åˆ°30Hz"""

        state_data, state_timestamps = state_data_with_ts
        # â­ ä¿®å¤ï¼šä½¿ç”¨ä¸ç»Ÿä¸€å¤„ç†æ¨¡å¼ç›¸åŒçš„æ—¶é—´æˆ³ç”Ÿæˆæ–¹å¼
        # ç»Ÿä¸€å¤„ç†æ¨¡å¼ä½¿ç”¨: np.arange(max_frames) / TARGET_FREQUENCY
        # ç¡®ä¿æ—¶é—´æˆ³èŒƒå›´ä¸€è‡´ï¼š[0, (frame_count-1)/TARGET_FREQUENCY]
        target_timestamps = np.arange(frame_count) / TARGET_FREQUENCY
        
        state_data_interpolated = {}
        for topic_name in state_data.keys():
            if state_data[topic_name]:

                interpolated = self._interpolate_batch_data(
                    state_data[topic_name],
                    np.array(state_timestamps[topic_name]),
                    target_timestamps
                )
                state_data_interpolated[topic_name] = interpolated
        

        return state_data_interpolated
    
    def _merge_separated_data(self, action_data, state_data, frame_count):
        """åˆå¹¶åˆ†ç¦»å¤„ç†çš„æ•°æ®ä¸ºLeRobotæ ¼å¼"""
        print("\nåˆå¹¶æ•°æ®ä¸ºLeRobotæ ¼å¼...")
        
        data_rows = []
        image_timestamps = np.arange(frame_count) / TARGET_FREQUENCY
        
        for i in range(frame_count):
            row = {}
            
            # 1. ACTIONæ•°æ®ï¼šä½¿ç”¨ä¸»è‡‚æ•°æ®ï¼ˆleaderï¼‰
            # å·¦ä¸»è‡‚å…³èŠ‚æ•°æ®
            left_leader_arm_data = [0.0] * 7
            if '/leader_left/joint_states' in action_data and i < len(action_data['/leader_left/joint_states']):
                joint_data = action_data['/leader_left/joint_states'][i]
                if joint_data is not None:
                    if isinstance(joint_data, list):
                        left_leader_arm_data = [float(x) for x in joint_data[:7]]
                    elif isinstance(joint_data, np.ndarray):
                        left_leader_arm_data = [float(x) for x in joint_data[:7]]
                    elif hasattr(joint_data, 'position'):
                        left_leader_arm_data = [float(x) for x in joint_data.position[:7]]
            
            # å³ä¸»è‡‚å…³èŠ‚æ•°æ®
            right_leader_arm_data = [0.0] * 7
            if '/leader_right/joint_states' in action_data and i < len(action_data['/leader_right/joint_states']):
                joint_data = action_data['/leader_right/joint_states'][i]
                if joint_data is not None:
                    if isinstance(joint_data, list):
                        right_leader_arm_data = [float(x) for x in joint_data[:7]]
                    elif isinstance(joint_data, np.ndarray):
                        right_leader_arm_data = [float(x) for x in joint_data[:7]]
                    elif hasattr(joint_data, 'position'):
                        right_leader_arm_data = [float(x) for x in joint_data.position[:7]]
            
            # å·¦ä¸»è‡‚å¤¹çˆªæ•°æ®ï¼ˆç”¨äºactionï¼‰
            left_leader_gripper_value = 0.0
            if '/left_tool_status' in action_data and i < len(action_data['/left_tool_status']):
                gripper_data = action_data['/left_tool_status'][i]
                if gripper_data is not None:
                    if isinstance(gripper_data, list) and len(gripper_data) > 0:
                        left_leader_gripper_value = float(gripper_data[0])
                    elif isinstance(gripper_data, np.ndarray) and len(gripper_data) > 0:
                        left_leader_gripper_value = float(gripper_data[0])
                    elif hasattr(gripper_data, 'data'):
                        left_leader_gripper_value = float(gripper_data.data)
            
            # å³ä¸»è‡‚å¤¹çˆªæ•°æ®ï¼ˆç”¨äºactionï¼‰
            right_leader_gripper_value = 0.0
            if '/right_tool_status' in action_data and i < len(action_data['/right_tool_status']):
                gripper_data = action_data['/right_tool_status'][i]
                if gripper_data is not None:
                    if isinstance(gripper_data, list) and len(gripper_data) > 0:
                        right_leader_gripper_value = float(gripper_data[0])
                    elif isinstance(gripper_data, np.ndarray) and len(gripper_data) > 0:
                        right_leader_gripper_value = float(gripper_data[0])
                    elif hasattr(gripper_data, 'data'):
                        right_leader_gripper_value = float(gripper_data.data)
            
            # è·å–LiftçŠ¶æ€ï¼ˆç”¨äºactionï¼‰
            lift_value = 0.0
            lift_up_value = 0.0
            if '/leader_lift/lift_up_state' in action_data and i < len(action_data['/leader_lift/lift_up_state']):
                lift_up_data = action_data['/leader_lift/lift_up_state'][i]
                if lift_up_data is not None:
                    try:
                        lift_up_value = float(lift_up_data if not hasattr(lift_up_data, '__len__') else lift_up_data[0])
                    except Exception:
                        lift_up_value = 0.0
            
            lift_down_value = 0.0
            if '/leader_lift/lift_down_state' in action_data and i < len(action_data['/leader_lift/lift_down_state']):
                lift_down_data = action_data['/leader_lift/lift_down_state'][i]
                if lift_down_data is not None:
                    try:
                        lift_down_value = float(lift_down_data if not hasattr(lift_down_data, '__len__') else lift_down_data[0])
                    except Exception:
                        lift_down_value = 0.0
            
            # åˆå¹¶liftçŠ¶æ€: 1=ä¸Šå‡, 0=ä¸åŠ¨, -1=ä¸‹é™
            if lift_up_value > 0.5:
                lift_value = 1.0
            elif lift_down_value > 0.5:
                lift_value = -1.0
            else:
                lift_value = 0.0
            
            # 1. actionï¼ˆä¸»è‡‚æ•°æ®ï¼šå·¦ä¸»è‡‚7å…³èŠ‚ + å·¦ä¸»è‡‚å¤¹çˆª + å³ä¸»è‡‚7å…³èŠ‚ + å³ä¸»è‡‚å¤¹çˆª + LiftçŠ¶æ€ï¼Œå…±17ç»´ï¼‰
            row['action'] = np.array(left_leader_arm_data + [left_leader_gripper_value] + right_leader_arm_data + [right_leader_gripper_value] + [lift_value], dtype=np.float32)
            
            # 2. STATEæ•°æ®ï¼šä½¿ç”¨ä»è‡‚æ•°æ®ï¼ˆfollowerï¼‰
            # å·¦ä»è‡‚å…³èŠ‚æ•°æ®
            left_follower_arm_data = [0.0] * 7
            if '/left_arm_controller/joint_states' in state_data and i < len(state_data['/left_arm_controller/joint_states']):
                joint_data = state_data['/left_arm_controller/joint_states'][i]
                if joint_data is not None:
                    if isinstance(joint_data, list):
                        left_follower_arm_data = [float(x) for x in joint_data[:7]]
                    elif isinstance(joint_data, np.ndarray):
                        left_follower_arm_data = [float(x) for x in joint_data[:7]]
                    elif hasattr(joint_data, 'position'):
                        left_follower_arm_data = [float(x) for x in joint_data.position[:7]]
            
            # å³ä»è‡‚å…³èŠ‚æ•°æ®
            right_follower_arm_data = [0.0] * 7
            if '/right_arm_controller/joint_states' in state_data and i < len(state_data['/right_arm_controller/joint_states']):
                joint_data = state_data['/right_arm_controller/joint_states'][i]
                if joint_data is not None:
                    if isinstance(joint_data, list):
                        right_follower_arm_data = [float(x) for x in joint_data[:7]]
                    elif isinstance(joint_data, np.ndarray):
                        right_follower_arm_data = [float(x) for x in joint_data[:7]]
                    elif hasattr(joint_data, 'position'):
                        right_follower_arm_data = [float(x) for x in joint_data.position[:7]]
            
            # å¤¹çˆªæ•°æ®
            left_gripper_value = 0.0
            if '/left_tool_status' in state_data and i < len(state_data['/left_tool_status']):
                gripper_data = state_data['/left_tool_status'][i]
                if gripper_data is not None:
                    if isinstance(gripper_data, list) and len(gripper_data) > 0:
                        left_gripper_value = float(gripper_data[0])
                    elif isinstance(gripper_data, np.ndarray) and len(gripper_data) > 0:
                        left_gripper_value = float(gripper_data[0])
                    elif hasattr(gripper_data, 'data'):
                        left_gripper_value = float(gripper_data.data)
            
            right_gripper_value = 0.0
            if '/right_tool_status' in state_data and i < len(state_data['/right_tool_status']):
                gripper_data = state_data['/right_tool_status'][i]
                if gripper_data is not None:
                    if isinstance(gripper_data, list) and len(gripper_data) > 0:
                        right_gripper_value = float(gripper_data[0])
                    elif isinstance(gripper_data, np.ndarray) and len(gripper_data) > 0:
                        right_gripper_value = float(gripper_data[0])
                    elif hasattr(gripper_data, 'data'):
                        right_gripper_value = float(gripper_data.data)
            
            # LiftçŠ¶æ€
            state_lift_value = 0.0
            if '/right_arm_controller/rm_driver/udp_lift_pos' in state_data and i < len(state_data['/right_arm_controller/rm_driver/udp_lift_pos']):
                lift_data = state_data['/right_arm_controller/rm_driver/udp_lift_pos'][i]
                if lift_data is not None:
                    if hasattr(lift_data, 'height'):
                        state_lift_value = float(lift_data.height) / 10.0
                    elif isinstance(lift_data, (list, np.ndarray)) and len(lift_data) > 0:
                        state_lift_value = float(lift_data[0]) / 10.0
            
            # è·å–å·¦ä»åŠ¨è‡‚å…­ç»´åŠ›ä¿¡æ¯ (200Hz)
            left_force_data = [0.0] * 6  # force_fx, force_fy, force_fz, force_mx, force_my, force_mz
            if '/left_arm_controller/rm_driver/udp_six_force' in state_data and i < len(state_data['/left_arm_controller/rm_driver/udp_six_force']):
                force_msg = state_data['/left_arm_controller/rm_driver/udp_six_force'][i]
                if force_msg is not None:
                    try:
                        # æå–å…­ç»´åŠ›æ•°æ®ï¼šforce_fx, force_fy, force_fz, force_mx, force_my, force_mz
                        if hasattr(force_msg, 'force_fx'):
                            left_force_data[0] = float(force_msg.force_fx)
                        if hasattr(force_msg, 'force_fy'):
                            left_force_data[1] = float(force_msg.force_fy)
                        if hasattr(force_msg, 'force_fz'):
                            left_force_data[2] = float(force_msg.force_fz)
                        if hasattr(force_msg, 'force_mx'):
                            left_force_data[3] = float(force_msg.force_mx)
                        if hasattr(force_msg, 'force_my'):
                            left_force_data[4] = float(force_msg.force_my)
                        if hasattr(force_msg, 'force_mz'):
                            left_force_data[5] = float(force_msg.force_mz)
                    except Exception:
                        left_force_data = [0.0] * 6
            
            # è·å–å³ä»åŠ¨è‡‚å…­ç»´åŠ›ä¿¡æ¯ (200Hz)
            right_force_data = [0.0] * 6  # force_fx, force_fy, force_fz, force_mx, force_my, force_mz
            if '/right_arm_controller/rm_driver/udp_six_force' in state_data and i < len(state_data['/right_arm_controller/rm_driver/udp_six_force']):
                force_msg = state_data['/right_arm_controller/rm_driver/udp_six_force'][i]
                if force_msg is not None:
                    try:
                        # æå–å…­ç»´åŠ›æ•°æ®ï¼šforce_fx, force_fy, force_fz, force_mx, force_my, force_mz
                        if hasattr(force_msg, 'force_fx'):
                            right_force_data[0] = float(force_msg.force_fx)
                        if hasattr(force_msg, 'force_fy'):
                            right_force_data[1] = float(force_msg.force_fy)
                        if hasattr(force_msg, 'force_fz'):
                            right_force_data[2] = float(force_msg.force_fz)
                        if hasattr(force_msg, 'force_mx'):
                            right_force_data[3] = float(force_msg.force_mx)
                        if hasattr(force_msg, 'force_my'):
                            right_force_data[4] = float(force_msg.force_my)
                        if hasattr(force_msg, 'force_mz'):
                            right_force_data[5] = float(force_msg.force_mz)
                    except Exception:
                        right_force_data = [0.0] * 6
            
            # è·å–å·¦ä»åŠ¨è‡‚å…³èŠ‚é€Ÿåº¦ (200Hz)
            left_velocity_data = [0.0] * 7  # 7ä¸ªå…³èŠ‚çš„é€Ÿåº¦
            if '/left_arm_controller/rm_driver/udp_joint_speed' in state_data and i < len(state_data['/left_arm_controller/rm_driver/udp_joint_speed']):
                velocity_msg = state_data['/left_arm_controller/rm_driver/udp_joint_speed'][i]
                if velocity_msg is not None:
                    try:
                        # æå–å…³èŠ‚é€Ÿåº¦æ•°æ® - ä¼˜å…ˆä½¿ç”¨joint_speedå±æ€§ï¼ˆJointspeedæ¶ˆæ¯ç±»å‹ï¼ŒFloat32Array (7)ï¼‰
                        if hasattr(velocity_msg, 'joint_speed'):
                            joint_speed = velocity_msg.joint_speed
                            # å¤„ç†æ•°ç»„ç±»å‹ï¼šlist, np.ndarray, array.array, æˆ–å…¶ä»–å¯è¿­ä»£å¯¹è±¡
                            if isinstance(joint_speed, (list, np.ndarray)) or hasattr(joint_speed, '__iter__'):
                                try:
                                    speeds = list(joint_speed)[:7]
                                    left_velocity_data = [float(x) for x in speeds] + [0.0] * (7 - len(speeds))
                                except (TypeError, ValueError):
                                    left_velocity_data = [0.0] * 7
                            else:
                                # å•ä¸ªå€¼çš„æƒ…å†µ
                                left_velocity_data = [float(joint_speed)] + [0.0] * 6
                        elif hasattr(velocity_msg, 'speed'):
                            if isinstance(velocity_msg.speed, (list, np.ndarray)):
                                speeds = list(velocity_msg.speed)[:7]
                                left_velocity_data = [float(x) for x in speeds] + [0.0] * (7 - len(speeds))
                            else:
                                left_velocity_data = [float(velocity_msg.speed)] + [0.0] * 6
                        elif isinstance(velocity_msg, (list, np.ndarray)):
                            speeds = list(velocity_msg)[:7]
                            left_velocity_data = [float(x) for x in speeds] + [0.0] * (7 - len(speeds))
                    except Exception:
                        left_velocity_data = [0.0] * 7
            
            # è·å–å³ä»åŠ¨è‡‚å…³èŠ‚é€Ÿåº¦ (200Hz)
            right_velocity_data = [0.0] * 7  # 7ä¸ªå…³èŠ‚çš„é€Ÿåº¦
            if '/right_arm_controller/rm_driver/udp_joint_speed' in state_data and i < len(state_data['/right_arm_controller/rm_driver/udp_joint_speed']):
                velocity_msg = state_data['/right_arm_controller/rm_driver/udp_joint_speed'][i]
                if velocity_msg is not None:
                    try:
                        # æå–å…³èŠ‚é€Ÿåº¦æ•°æ® - ä¼˜å…ˆä½¿ç”¨joint_speedå±æ€§ï¼ˆJointspeedæ¶ˆæ¯ç±»å‹ï¼ŒFloat32Array (7)ï¼‰
                        if hasattr(velocity_msg, 'joint_speed'):
                            joint_speed = velocity_msg.joint_speed
                            # å¤„ç†æ•°ç»„ç±»å‹ï¼šlist, np.ndarray, array.array, æˆ–å…¶ä»–å¯è¿­ä»£å¯¹è±¡
                            if isinstance(joint_speed, (list, np.ndarray)) or hasattr(joint_speed, '__iter__'):
                                try:
                                    speeds = list(joint_speed)[:7]
                                    right_velocity_data = [float(x) for x in speeds] + [0.0] * (7 - len(speeds))
                                except (TypeError, ValueError):
                                    right_velocity_data = [0.0] * 7
                            else:
                                # å•ä¸ªå€¼çš„æƒ…å†µ
                                right_velocity_data = [float(joint_speed)] + [0.0] * 6
                        elif hasattr(velocity_msg, 'speed'):
                            if isinstance(velocity_msg.speed, (list, np.ndarray)):
                                speeds = list(velocity_msg.speed)[:7]
                                right_velocity_data = [float(x) for x in speeds] + [0.0] * (7 - len(speeds))
                            else:
                                right_velocity_data = [float(velocity_msg.speed)] + [0.0] * 6
                        elif isinstance(velocity_msg, (list, np.ndarray)):
                            speeds = list(velocity_msg)[:7]
                            right_velocity_data = [float(x) for x in speeds] + [0.0] * (7 - len(speeds))
                    except Exception:
                        right_velocity_data = [0.0] * 7
            
            # è·å–å·¦ä»åŠ¨è‡‚æœ«ç«¯ä½å§¿ (200Hz) - ä½ç½®(x,y,z) + å§¿æ€å››å…ƒæ•°(w,x,y,z)ï¼Œå…±7ç»´
            left_pose_data = [0.0] * 7  # ä½ç½®3ç»´(x,y,z) + å››å…ƒæ•°4ç»´(w,x,y,z)
            if '/left_arm_controller/rm_driver/udp_arm_position' in state_data and i < len(state_data['/left_arm_controller/rm_driver/udp_arm_position']):
                pose_msg = state_data['/left_arm_controller/rm_driver/udp_arm_position'][i]
                if pose_msg is not None:
                    try:
                        # æå–ä½ç½®æ•°æ® - ä¼˜å…ˆä½¿ç”¨poseå±æ€§ï¼ˆJointposeorientationæ¶ˆæ¯ç±»å‹ï¼‰
                        if hasattr(pose_msg, 'pose'):
                            pose = pose_msg.pose
                            # æå–ä½ç½®ï¼ˆx, y, zï¼‰
                            if hasattr(pose, 'position'):
                                pos = pose.position
                                if hasattr(pos, 'x') and hasattr(pos, 'y') and hasattr(pos, 'z'):
                                    left_pose_data[0] = float(pos.x)
                                    left_pose_data[1] = float(pos.y)
                                    left_pose_data[2] = float(pos.z)
                            # æå–å§¿æ€ï¼ˆå››å…ƒæ•° w, x, y, zï¼‰
                            if hasattr(pose, 'orientation'):
                                orient = pose.orientation
                                if hasattr(orient, 'x') and hasattr(orient, 'y') and hasattr(orient, 'z') and hasattr(orient, 'w'):
                                    left_pose_data[3] = float(orient.w)  # w
                                    left_pose_data[4] = float(orient.x)  # x
                                    left_pose_data[5] = float(orient.y)  # y
                                    left_pose_data[6] = float(orient.z)  # z
                        # å…¼å®¹æ—§æ ¼å¼ï¼šç›´æ¥å±æ€§ x, y, z
                        elif hasattr(pose_msg, 'x') and hasattr(pose_msg, 'y') and hasattr(pose_msg, 'z'):
                            left_pose_data[0] = float(pose_msg.x)
                            left_pose_data[1] = float(pose_msg.y)
                            left_pose_data[2] = float(pose_msg.z)
                            if hasattr(pose_msg, 'qw') and hasattr(pose_msg, 'qx') and hasattr(pose_msg, 'qy') and hasattr(pose_msg, 'qz'):
                                left_pose_data[3] = float(pose_msg.qw)  # w
                                left_pose_data[4] = float(pose_msg.qx)  # x
                                left_pose_data[5] = float(pose_msg.qy)  # y
                                left_pose_data[6] = float(pose_msg.qz)  # z
                            elif hasattr(pose_msg, 'roll') and hasattr(pose_msg, 'pitch') and hasattr(pose_msg, 'yaw'):
                                # å¦‚æœåªæœ‰RPYï¼Œè½¬æ¢ä¸ºå››å…ƒæ•°
                                import math
                                roll = float(pose_msg.roll)
                                pitch = float(pose_msg.pitch)
                                yaw = float(pose_msg.yaw)
                                # RPYè½¬å››å…ƒæ•°
                                cy = math.cos(yaw * 0.5)
                                sy = math.sin(yaw * 0.5)
                                cp = math.cos(pitch * 0.5)
                                sp = math.sin(pitch * 0.5)
                                cr = math.cos(roll * 0.5)
                                sr = math.sin(roll * 0.5)
                                left_pose_data[3] = cr * cp * cy + sr * sp * sy  # w
                                left_pose_data[4] = sr * cp * cy - cr * sp * sy  # x
                                left_pose_data[5] = cr * sp * cy + sr * cp * sy  # y
                                left_pose_data[6] = cr * cp * sy - sr * sp * cy  # z
                    except Exception as e:
                        if i == 0:
                            print(f"  âš ï¸  æå–å·¦æœ«ç«¯ä½å§¿å¤±è´¥: {type(pose_msg)}, é”™è¯¯: {e}")
                            if hasattr(pose_msg, 'pose'):
                                print(f"      poseå±æ€§: {pose_msg.pose}")
                                if hasattr(pose_msg.pose, 'position'):
                                    print(f"      pose.position: {pose_msg.pose.position}")
                        left_pose_data = [0.0] * 7
            
            # è·å–å³ä»åŠ¨è‡‚æœ«ç«¯ä½å§¿ (200Hz) - ä½ç½®(x,y,z) + å§¿æ€å››å…ƒæ•°(w,x,y,z)ï¼Œå…±7ç»´
            right_pose_data = [0.0] * 7  # ä½ç½®3ç»´(x,y,z) + å››å…ƒæ•°4ç»´(w,x,y,z)
            if '/right_arm_controller/rm_driver/udp_arm_position' in state_data and i < len(state_data['/right_arm_controller/rm_driver/udp_arm_position']):
                pose_msg = state_data['/right_arm_controller/rm_driver/udp_arm_position'][i]
                if pose_msg is not None:
                    try:
                        # æå–ä½ç½®æ•°æ® - ä¼˜å…ˆä½¿ç”¨poseå±æ€§ï¼ˆJointposeorientationæ¶ˆæ¯ç±»å‹ï¼‰
                        if hasattr(pose_msg, 'pose'):
                            pose = pose_msg.pose
                            # æå–ä½ç½®ï¼ˆx, y, zï¼‰
                            if hasattr(pose, 'position'):
                                pos = pose.position
                                if hasattr(pos, 'x') and hasattr(pos, 'y') and hasattr(pos, 'z'):
                                    right_pose_data[0] = float(pos.x)
                                    right_pose_data[1] = float(pos.y)
                                    right_pose_data[2] = float(pos.z)
                            # æå–å§¿æ€ï¼ˆå››å…ƒæ•° w, x, y, zï¼‰
                            if hasattr(pose, 'orientation'):
                                orient = pose.orientation
                                if hasattr(orient, 'x') and hasattr(orient, 'y') and hasattr(orient, 'z') and hasattr(orient, 'w'):
                                    right_pose_data[3] = float(orient.w)  # w
                                    right_pose_data[4] = float(orient.x)  # x
                                    right_pose_data[5] = float(orient.y)  # y
                                    right_pose_data[6] = float(orient.z)  # z
                        # å…¼å®¹æ—§æ ¼å¼ï¼šç›´æ¥å±æ€§ x, y, z
                        elif hasattr(pose_msg, 'x') and hasattr(pose_msg, 'y') and hasattr(pose_msg, 'z'):
                            right_pose_data[0] = float(pose_msg.x)
                            right_pose_data[1] = float(pose_msg.y)
                            right_pose_data[2] = float(pose_msg.z)
                            if hasattr(pose_msg, 'qw') and hasattr(pose_msg, 'qx') and hasattr(pose_msg, 'qy') and hasattr(pose_msg, 'qz'):
                                right_pose_data[3] = float(pose_msg.qw)  # w
                                right_pose_data[4] = float(pose_msg.qx)  # x
                                right_pose_data[5] = float(pose_msg.qy)  # y
                                right_pose_data[6] = float(pose_msg.qz)  # z
                            elif hasattr(pose_msg, 'roll') and hasattr(pose_msg, 'pitch') and hasattr(pose_msg, 'yaw'):
                                # å¦‚æœåªæœ‰RPYï¼Œè½¬æ¢ä¸ºå››å…ƒæ•°
                                import math
                                roll = float(pose_msg.roll)
                                pitch = float(pose_msg.pitch)
                                yaw = float(pose_msg.yaw)
                                # RPYè½¬å››å…ƒæ•°
                                cy = math.cos(yaw * 0.5)
                                sy = math.sin(yaw * 0.5)
                                cp = math.cos(pitch * 0.5)
                                sp = math.sin(pitch * 0.5)
                                cr = math.cos(roll * 0.5)
                                sr = math.sin(roll * 0.5)
                                right_pose_data[3] = cr * cp * cy + sr * sp * sy  # w
                                right_pose_data[4] = sr * cp * cy - cr * sp * sy  # x
                                right_pose_data[5] = cr * sp * cy + sr * cp * sy  # y
                                right_pose_data[6] = cr * cp * sy - sr * sp * cy  # z
                    except Exception as e:
                        if i == 0:
                            print(f"  âš ï¸  æå–å³æœ«ç«¯ä½å§¿å¤±è´¥: {type(pose_msg)}, é”™è¯¯: {e}")
                            if hasattr(pose_msg, 'pose'):
                                print(f"      poseå±æ€§: {pose_msg.pose}")
                        right_pose_data = [0.0] * 7
            
            # 2. observation.stateï¼ˆä»è‡‚æ•°æ®ï¼šå·¦ä»è‡‚7å…³èŠ‚ + å·¦ä»è‡‚å¤¹çˆª + å³ä»è‡‚7å…³èŠ‚ + å³ä»è‡‚å¤¹çˆª + LiftçŠ¶æ€ + å·¦å…­ç»´åŠ› + å³å…­ç»´åŠ› + å·¦å…³èŠ‚é€Ÿåº¦7 + å³å…³èŠ‚é€Ÿåº¦7 + å·¦æœ«ç«¯ä½å§¿7 + å³æœ«ç«¯ä½å§¿7ï¼Œå…±57ç»´ï¼‰
            state_data_list = (left_follower_arm_data + [left_gripper_value] + 
                             right_follower_arm_data + [right_gripper_value] + 
                             [state_lift_value] + left_force_data + right_force_data +
                             left_velocity_data + right_velocity_data +
                             left_pose_data + right_pose_data)
            row['observation.state'] = np.array(state_data_list, dtype=np.float32)
            
            # 3. å…¶ä»–å­—æ®µ
            row['timestamp'] = image_timestamps[i]
            row['frame_index'] = i
            row['episode_index'] = 0
            row['index'] = i
            row['task_index'] = 0
            
            data_rows.append(row)
        
        # è½¬æ¢ä¸ºDataFrame
        df = pd.DataFrame(data_rows)
        
        # ä¿å­˜Parquetæ–‡ä»¶
        data_chunk_dir = self.data_dir / "chunk-000"
        data_chunk_dir.mkdir(parents=True, exist_ok=True)
        parquet_path = data_chunk_dir / "episode_000000.parquet"
        df.to_parquet(parquet_path, index=False)
        
        print(f"  ä¿å­˜æ•°æ®æ–‡ä»¶: {parquet_path}")
        print(f"  æ•°æ®è¡Œæ•°: {len(df)}")
        print(f"  æ•°æ®åˆ—æ•°: {len(df.columns)}")
        
        return df

    def convert(self):
        """æ‰§è¡Œå®Œæ•´çš„è½¬æ¢æµç¨‹"""
        print("å¼€å§‹LeRobot v2.1æ ‡å‡†æ ¼å¼è½¬æ¢ï¼ˆLinuxæœåŠ¡å™¨ä¼˜åŒ–-ç»Ÿä¸€30Hzï¼‰...")
        start_time = datetime.now()

        try:
            # 1. åŠ è½½topicé…ç½®ï¼ˆç»Ÿä¸€è‡ªåŠ¨ä»MCAPæ¢æµ‹ï¼‰
            self.discover_topic_configs_from_mcap()

            # 2. åˆ†æMCAPæ–‡ä»¶
            file_start_time, file_end_time, duration, topic_counts = self.analyze_mcap_file_for_duration()

            # 3. åˆ†æ‰¹å¤„ç†æ•°æ®
            self.process_mcap_data_in_batches(file_start_time, file_end_time)

            # 4. è¯„ä¼°æ•°æ®è´¨é‡
            self.evaluate_data_quality()

            # 5. ç”ŸæˆLeRobot v2.1æ ‡å‡†æ•°æ®é›†
            df, video_info = self.generate_lerobot_v21_dataset()

            # 6. ç”Ÿæˆå…ƒæ•°æ®æ–‡ä»¶
            self.generate_meta_files(df, video_info)

            # 7. ç»˜åˆ¶æ›²çº¿å›¾
            self.plot_arm_curves()

            end_time = datetime.now()
            processing_time = (end_time - start_time).total_seconds()

            print(f"\nè½¬æ¢å®Œæˆ! è€—æ—¶: {processing_time:.2f} ç§’")
            print(f"è¾“å‡ºç›®å½•: {self.output_dir}")
            print(f"ç”Ÿæˆå¸§æ•°: {len(df)}")
            print(f"æ•´ä½“è´¨é‡è¯„åˆ†: {self.overall_quality_score:.3f}")
            print(f"ç»Ÿä¸€é‡‡æ ·é¢‘ç‡: {TARGET_FREQUENCY} Hz")
            print(f"æ ¼å¼ç‰ˆæœ¬: LeRobot v2.1 æ ‡å‡†")

            return ConversionReport(
                total_topics=len(self.quality_metrics),
                converted_frames=len(df),
                processing_time=processing_time,
                quality_metrics=self.quality_metrics,
                overall_quality_score=self.overall_quality_score,
                conversion_issues=self.conversion_issues,
                timestamp=datetime.now().isoformat()
            )

        except Exception as e:
            print(f"\nè½¬æ¢å¤±è´¥: {e}")
            raise

    def _generate_keys_mapping(self, columns):
        """ç”Ÿæˆkeysæ˜ å°„ - LeRobot v2.1æ ‡å‡†"""
        keys = {}
        for col in columns:
            if col in ['timestamp', 'frame_index', 'episode_index']:
                continue
            elif 'images' in col:
                keys[col] = 'video'
            else:
                keys[col] = 'scalar'
        return keys

    def _generate_features_mapping(self, columns):
        """ç”Ÿæˆfeaturesæ˜ å°„ - LeRobot v2.1æ ‡å‡†"""
        features = {}
        for col in columns:
            if col in ['timestamp', 'frame_index', 'episode_index']:
                continue
            elif 'images' in col:
                # è§†é¢‘ç‰¹å¾
                features[col] = {
                    "dtype": "uint8",
                    "shape": [3, 480, 640],  # å‡è®¾å›¾åƒå°ºå¯¸
                    "video": True
                }
            else:
                # æ ‡é‡ç‰¹å¾
                features[col] = {
                    "dtype": "float32",
                    "shape": []
                }
        return features


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='MCAPåˆ°LeRobot v2.1æ ‡å‡†æ ¼å¼è½¬æ¢å™¨ (LinuxæœåŠ¡å™¨ä¼˜åŒ–ç‰ˆ-ç»Ÿä¸€30Hz)')
    parser.add_argument('--input', required=True, help='è¾“å…¥MCAPæ–‡ä»¶æˆ–ç›®å½•è·¯å¾„')
    parser.add_argument('--output', required=True, help='è¾“å‡ºç›®å½•è·¯å¾„')
    # Excel é…ç½®ç›¸å…³å‚æ•°å·²ç§»é™¤
    parser.add_argument('--max-duration', type=int, default=0, help='æœ€å¤§å¤„ç†æ—¶é•¿ï¼ˆç§’ï¼‰ï¼Œ0è¡¨ç¤ºä¸é™åˆ¶ï¼Œå¤„ç†å®Œæ•´æ–‡ä»¶')
    parser.add_argument('--resolution', choices=['360p', '720p'], default='720p', help='ç›®æ ‡åˆ†è¾¨ç‡ï¼š360p (640x360) æˆ– 720p (1280x720)ï¼Œé»˜è®¤720p')
    parser.add_argument('--no-plot', action='store_true', help='ä¸ç”Ÿæˆè‡‚ä¸å¤¹çˆªæ›²çº¿å›¾')
    parser.add_argument('--threads', type=int, default=None, help='å¤„ç†çº¿ç¨‹æ•°ï¼ˆè¦†ç›–é»˜è®¤çš„åŠæ ¸ç­–ç•¥ï¼‰')
    parser.add_argument('--ffmpeg-threads', type=int, default=None, help='AV1è½¬ç çº¿ç¨‹æ•°ï¼ˆè¦†ç›–è‡ªåŠ¨CPUæ ¸å¿ƒæ•°ï¼‰')
    parser.add_argument('--ffmpeg-cpu-used', type=int, default=None, help='AV1ç¼–ç é€Ÿåº¦å‚æ•°cpu-usedï¼ˆé»˜è®¤8ï¼Œè¶Šå¤§è¶Šå¿«ï¼‰')

    args = parser.parse_args()

    # å½’ä¸€åŒ–è·¯å¾„ï¼šå°†åæ–œæ è½¬ä¸ºæ­£æ–œæ ï¼Œå¹¶è§£æä¸ºç»å¯¹è·¯å¾„
    input_path = Path(str(args.input).replace('\\', '/')).resolve()
    output_dir = Path(str(args.output).replace('\\', '/')).resolve()
    # print(f"[Paths] Input: {input_path.as_posix()}")
    # print(f"[Paths] Output: {output_dir.as_posix()}")

    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not input_path.exists():
        print(f"é”™è¯¯: è¾“å…¥è·¯å¾„ä¸å­˜åœ¨: {input_path.as_posix()}")
        sys.exit(1)

    # è‹¥ä¸ºç›®å½•ï¼Œæœ€å¼€å§‹è¿›è¡Œæ‰«æå¹¶æŠ¥å‘Šmcapæ•°é‡
    mcap_files = None
    if input_path.is_dir():
        mcap_files = [p for p in input_path.rglob("*") if p.is_file() and p.suffix.lower() == ".mcap"]
        print(f"[Scan] åœ¨ç›®å½•ä¸­å‘ç° {len(mcap_files)} ä¸ªMCAPæ–‡ä»¶: {input_path.as_posix()}")
        if not mcap_files:
            print(f"é”™è¯¯: åœ¨ç›®å½•ä¸­æœªæ‰¾åˆ°mcapæ–‡ä»¶: {input_path.as_posix()}")
            sys.exit(1)

    # ä¸å†æ”¯æŒ Excel é…ç½®ï¼Œç»Ÿä¸€ä» MCAP è‡ªåŠ¨æ¢æµ‹

    if args.threads is not None and args.threads > 0:
        os.environ['OPENBLAS_NUM_THREADS'] = str(args.threads)
        os.environ['MKL_NUM_THREADS'] = str(args.threads)
        os.environ['NUMEXPR_NUM_THREADS'] = str(args.threads)
        os.environ['OMP_NUM_THREADS'] = str(args.threads)
        try:
            cv2.setNumThreads(args.threads)
        except Exception:
            pass
        print(f"[Perf] å¤„ç†çº¿ç¨‹æ•°è¦†ç›–ä¸º: {args.threads}")

    if args.ffmpeg_threads is not None and args.ffmpeg_threads > 0:
        os.environ['FFMPEG_THREADS'] = str(args.ffmpeg_threads)
        print(f"[Perf] FFmpegè½¬ç çº¿ç¨‹æ•°è¦†ç›–ä¸º: {args.ffmpeg_threads}")

    if args.ffmpeg_cpu_used is not None and args.ffmpeg_cpu_used >= 0:
        os.environ['FFMPEG_CPU_USED'] = str(args.ffmpeg_cpu_used)
        print(f"[Perf] FFmpeg cpu-used è¦†ç›–ä¸º: {args.ffmpeg_cpu_used}")

    if input_path.is_file():
        converter = MCAPToLeRobotV21StandardConverter(
            str(input_path),
            str(output_dir),
            max_duration=args.max_duration,
            resolution=args.resolution
        )
        converter.no_plot = args.no_plot
        report = converter.convert_separated()
        print("\nè½¬æ¢æ€»ç»“:")
        print(f"- å¤„ç†äº† {len(report.quality_metrics)} ä¸ªtopics")
        converter.validate_training_readiness()
        print(f"- ç”Ÿæˆäº† {report.converted_frames} å¸§æ•°æ®")
        print(f"- æ•´ä½“è´¨é‡è¯„åˆ†: {report.overall_quality_score:.3f}")
        print(f"- ç»Ÿä¸€é‡‡æ ·é¢‘ç‡: {TARGET_FREQUENCY} Hz")
        print(f"- æ ¼å¼ç‰ˆæœ¬: LeRobot v2.1 æ ‡å‡†")
        if report.overall_quality_score >= 0.8:
            print("âœ… æ•°æ®è´¨é‡ä¼˜ç§€ï¼Œå¯ç›´æ¥ç”¨äºè®­ç»ƒ!")
        elif report.overall_quality_score >= 0.6:
            print("âš ï¸  æ•°æ®è´¨é‡è‰¯å¥½ï¼Œå»ºè®®æ£€æŸ¥éƒ¨åˆ†topic")
        else:
            print("âŒ æ•°æ®è´¨é‡è¾ƒå·®ï¼Œå»ºè®®é‡æ–°é‡‡é›†")
    elif input_path.is_dir():
        print(f"å‘ç° {len(mcap_files)} ä¸ªMCAPæ–‡ä»¶")
        for p in tqdm(mcap_files, desc="è½¬æ¢MCAP", unit="file"):
            rel = p.relative_to(input_path)
            sub_output = output_dir / rel.parent / p.stem
            sub_output.mkdir(parents=True, exist_ok=True)
            # print(f"\n[æ‰¹é‡è½¬æ¢] {p.as_posix()} -> {sub_output.as_posix()}")
            converter = MCAPToLeRobotV21StandardConverter(
                str(p),
                str(sub_output),
                max_duration=args.max_duration,
                resolution=args.resolution
            )
            converter.no_plot = args.no_plot
            print("ä½¿ç”¨åˆ†ç¦»å¤„ç†æ¨¡å¼ï¼ˆå…ˆåˆ†åˆ«å¤„ç†è§†é¢‘ã€actionã€stateï¼Œå†æ‹¼æ¥ï¼‰...")
            try:
                report = converter.convert_separated()
                print("\nè½¬æ¢æ€»ç»“:")
                print(f"- å¤„ç†äº† {len(report.quality_metrics)} ä¸ªtopics")
                converter.validate_training_readiness()
                print(f"- ç”Ÿæˆäº† {report.converted_frames} å¸§æ•°æ®")
                print(f"- æ•´ä½“è´¨é‡è¯„åˆ†: {report.overall_quality_score:.3f}")
                print(f"- ç»Ÿä¸€é‡‡æ ·é¢‘ç‡: {TARGET_FREQUENCY} Hz")
                print(f"- æ ¼å¼ç‰ˆæœ¬: LeRobot v2.1 æ ‡å‡†")
                if report.overall_quality_score >= 0.8:
                    print("âœ… æ•°æ®è´¨é‡ä¼˜ç§€ï¼Œå¯ç›´æ¥ç”¨äºè®­ç»ƒ!")
                elif report.overall_quality_score >= 0.6:
                    print("âš ï¸  æ•°æ®è´¨é‡è‰¯å¥½ï¼Œå»ºè®®æ£€æŸ¥éƒ¨åˆ†topic")
                else:
                    print("âŒ æ•°æ®è´¨é‡è¾ƒå·®ï¼Œå»ºè®®é‡æ–°é‡‡é›†")
            except Exception as e:
                print(f"æ–‡ä»¶è½¬æ¢å¤±è´¥: {p} -> {e}")
                continue
        print("\næ‰¹é‡è½¬æ¢å®Œæˆ")
    else:
        print(f"é”™è¯¯: æ— æ•ˆçš„è¾“å…¥è·¯å¾„: {input_path.as_posix()}")
        sys.exit(1)


if __name__ == "__main__":
    main()
